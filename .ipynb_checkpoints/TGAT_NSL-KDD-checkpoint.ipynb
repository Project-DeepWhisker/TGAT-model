{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393be218-97a4-45ba-8c22-cbc7496b4bb9",
   "metadata": {},
   "source": [
    "# preprocess_kdd_large.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85382bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TGAT Preprocessing for Large Datasets\n",
    "This notebook handles the preprocessing of KDD-like datasets, optimized for larger files using Dask.\n",
    "It performs the following steps:\n",
    "1. Loads raw data in chunks.\n",
    "2. Defines column names and types.\n",
    "3. Preprocesses labels (binary and multi-class).\n",
    "4. Preprocesses features:\n",
    "   - One-hot encodes categorical features using Dask's `get_dummies`.\n",
    "   - Scales numerical features using Dask's mean/std.\n",
    "5. Constructs temporal graph components (node features `x`, edge indices `edge_index`, timestamps `ts`).\n",
    "6. Saves the processed data and metadata for the training script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018adbb",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ee8550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (69.0.3)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.35.1)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.8.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.35.1\n",
      "    Uninstalling wheel-0.35.1:\n",
      "      Successfully uninstalled wheel-0.35.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.0.3\n",
      "    Uninstalling setuptools-69.0.3:\n",
      "      Successfully uninstalled setuptools-69.0.3\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient-utils 0.5.0 requires wheel<0.36.0,>=0.35.1, but you have wheel 0.45.1 which is incompatible.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-25.1.1 setuptools-80.8.0 wheel-0.45.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Installing PyG dependencies for Torch 2.1.1 and CUDA cu121...\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.1.1+cu121.html\n",
      "Collecting torch_scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch_sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch_cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_cluster-1.6.3%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch_spline_conv\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (935 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.0/936.0 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.11.2)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (1.26.3)\n",
      "Installing collected packages: torch_spline_conv, torch_scatter, torch_sparse, torch_cluster\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [torch_cluster]0m [torch_sparse]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed torch_cluster-1.6.3+pt21cu121 torch_scatter-2.1.2+pt21cu121 torch_sparse-0.6.18+pt21cu121 torch_spline_conv-1.2.2+pt21cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Installing pyg-lib for Torch 2.1.1 and CUDA cu121...\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.1.1+cu121.html\n",
      "Collecting pyg_lib\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/pyg_lib-0.4.0%2Bpt21cu121-cp311-cp311-linux_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyg_lib\n",
      "Successfully installed pyg_lib-0.4.0+pt21cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch_geometric==2.6.1\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric==2.6.1) (3.9.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric==2.6.1) (2023.6.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric==2.6.1) (3.1.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric==2.6.1) (1.26.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric==2.6.1) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric==2.6.1) (2.4.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric==2.6.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric==2.6.1) (4.66.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric==2.6.1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric==2.6.1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric==2.6.1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric==2.6.1) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric==2.6.1) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->torch_geometric==2.6.1) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric==2.6.1) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric==2.6.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric==2.6.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric==2.6.1) (2020.6.20)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch-geometric-temporal==0.56.0\n",
      "  Downloading torch_geometric_temporal-0.56.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting decorator==4.4.2 (from torch-geometric-temporal==0.56.0)\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal==0.56.0) (2.1.1+cu121)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal==0.56.0) (3.0.2)\n",
      "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal==0.56.0) (0.6.18+pt21cu121)\n",
      "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal==0.56.0) (2.1.2+pt21cu121)\n",
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal==0.56.0) (2.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal==0.56.0) (1.26.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal==0.56.0) (3.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal==0.56.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal==0.56.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal==0.56.0) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal==0.56.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal==0.56.0) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal==0.56.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch-geometric-temporal==0.56.0) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->torch-geometric-temporal==0.56.0) (1.3.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal==0.56.0) (3.9.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal==0.56.0) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric->torch-geometric-temporal==0.56.0) (2.4.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal==0.56.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal==0.56.0) (4.66.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal==0.56.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal==0.56.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal==0.56.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal==0.56.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal==0.56.0) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->torch_geometric->torch-geometric-temporal==0.56.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch-geometric-temporal==0.56.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch-geometric-temporal==0.56.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric->torch-geometric-temporal==0.56.0) (2020.6.20)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse->torch-geometric-temporal==0.56.0) (1.11.2)\n",
      "Downloading torch_geometric_temporal-0.56.0-py3-none-any.whl (98 kB)\n",
      "Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: decorator, torch-geometric-temporal\n",
      "\u001b[2K  Attempting uninstall: decorator\n",
      "\u001b[2K    Found existing installation: decorator 5.1.1\n",
      "\u001b[2K    Uninstalling decorator-5.1.1:\n",
      "\u001b[2K      Successfully uninstalled decorator-5.1.1━━\u001b[0m \u001b[32m0/2\u001b[0m [decorator]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch-geometric-temporal]\n",
      "\u001b[1A\u001b[2KSuccessfully installed decorator-4.4.2 torch-geometric-temporal-0.56.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.31.0)\n",
      "Collecting dask\n",
      "  Downloading dask-2025.5.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.7)\n",
      "Collecting cloudpickle>=3.0.0 (from dask)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2023.6.0)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask) (5.4.1)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib_metadata>=4.13.0 (from dask)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata>=4.13.0->dask)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting locket (from partd>=1.4.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading dask-2025.5.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: zipp, toolz, locket, cloudpickle, partd, importlib_metadata, dask\n",
      "\u001b[2K  Attempting uninstall: zipp\n",
      "\u001b[2K    Found existing installation: zipp 1.0.0\n",
      "\u001b[2K    Uninstalling zipp-1.0.0:\n",
      "\u001b[2K      Successfully uninstalled zipp-1.0.0\n",
      "\u001b[2K  Attempting uninstall: cloudpickle━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K    Found existing installation: cloudpickle 2.2.1━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K    Uninstalling cloudpickle-2.2.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K      Successfully uninstalled cloudpickle-2.2.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K  Attempting uninstall: importlib_metadata━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K    Found existing installation: importlib-metadata 4.6.4━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K    Uninstalling importlib-metadata-4.6.4:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K      Successfully uninstalled importlib-metadata-4.6.4━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [toolz]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [dask][32m6/7\u001b[0m [dask]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cloudpickle-3.1.1 dask-2025.5.1 importlib_metadata-8.7.0 locket-1.0.0 partd-1.4.2 toolz-1.0.0 zipp-3.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Environment Setup ---\n",
    "# Make sure torch is installed first if not already\n",
    "# %pip install torch torchvision torchaudio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "%pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# Install PyTorch Geometric core dependencies (adjust torch version and cuda suffix as needed)\n",
    "TORCH_VERSION = torch.__version__.split('+')[0] # Get base torch version\n",
    "CUDA_VERSION = torch.version.cuda.replace('.', '') if torch.cuda.is_available() else 'cpu'\n",
    "CUDA_SUFFIX = f'cu{CUDA_VERSION}' if CUDA_VERSION != 'cpu' else 'cpu'\n",
    "print(f\"Installing PyG dependencies for Torch {TORCH_VERSION} and CUDA {CUDA_SUFFIX}...\")\n",
    "%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH_VERSION}+{CUDA_SUFFIX}.html\n",
    "\n",
    "# --- Install pyg-lib for potential speedup (addresses warning) ---\n",
    "print(f\"Installing pyg-lib for Torch {TORCH_VERSION} and CUDA {CUDA_SUFFIX}...\")\n",
    "%pip install pyg_lib -f https://data.pyg.org/whl/torch-{TORCH_VERSION}+{CUDA_SUFFIX}.html\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "%pip install \"torch_geometric==2.6.1\"\n",
    "%pip install \"torch-geometric-temporal==0.56.0\"\n",
    "\n",
    "# Other necessary libraries\n",
    "%pip install pandas numpy scikit-learn matplotlib seaborn tqdm requests dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afa06a-fc1d-4f50-9c81-ae6614625cac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cell 2: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebf4cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_DATA_DIR = './data/' # 確保您的 NSL-KDD 文件在此目錄中\n",
    "# 更新為您 NSL-KDD 文件的確切名稱\n",
    "TRAIN_FILE = 'KDDTrain+.txt' # NSL-KDD 訓練文件名 (請確認此文件名與您下載的一致)\n",
    "TEST_FILE = 'KDDTest+.txt'   # NSL-KDD 測試文件名 (或 'KDDTest-21.txt', 請確認)\n",
    "\n",
    "PROCESSED_DATA_DIR = './processed_data_nslkdd/' # 更改以避免覆蓋 KDD99 處理的數據\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "PROCESSED_TRAIN_FILE = os.path.join(PROCESSED_DATA_DIR, 'train_temporal_data_nslkdd.pt')\n",
    "PROCESSED_TEST_FILE = os.path.join(PROCESSED_DATA_DIR, 'test_temporal_data_nslkdd.pt')\n",
    "METADATA_FILE = os.path.join(PROCESSED_DATA_DIR, 'metadata_nslkdd.json')\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "RECENCY_BIAS_FACTOR = 0.9\n",
    "FEATURE_SIMILARITY_COL_NAME = 'service'\n",
    "FEATURE_SIMILARITY_WEIGHT = 0.3\n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "\n",
    "BATCH_SIZE_SEQ_EMBED_GEN = 128\n",
    "SEQUENCE_LENGTH = 15\n",
    "STEP_SIZE = 5\n",
    "SEQ_LABEL_MODE = 'any_attack'\n",
    "BATCH_SIZE_SEQ_MODEL = 64\n",
    "LEARNING_RATE_SEQ_MODEL = 1e-4\n",
    "EPOCHS_SEQ_MODEL = 30\n",
    "SEQ_MODEL_EMBEDDING_DIM = HIDDEN_DIM\n",
    "SEQ_MODEL_HIDDEN_DIM = 128\n",
    "SEQ_MODEL_NUM_LAYERS = 2\n",
    "SEQ_MODEL_RNN_TYPE = 'GRU'\n",
    "SEQ_MODEL_DROPOUT = 0.3\n",
    "\n",
    "COL_NAMES = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "    'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "    'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
    "    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
    "    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
    "    'attack_type', 'difficulty_score'\n",
    "]\n",
    "\n",
    "CATEGORICAL_COLS = ['protocol_type', 'service', 'flag']\n",
    "NUMERICAL_COLS = [col for col in COL_NAMES if col not in CATEGORICAL_COLS + ['attack_type', 'difficulty_score']]\n",
    "LABEL_COL = 'attack_type'\n",
    "NORMAL_TAG = 'normal'\n",
    "\n",
    "ATTACK_MAP_MULTI_CLASS = {\n",
    "    'normal': 0, 'dos': 1, 'probe': 2, 'r2l': 3, 'u2r': 4\n",
    "}\n",
    "KDD_SPECIFIC_TO_GENERAL_ATTACK_MAP = {\n",
    "    'back': 'dos', 'land': 'dos', 'neptune': 'dos', 'pod': 'dos', 'smurf': 'dos', 'teardrop': 'dos',\n",
    "    'mailbomb': 'dos', 'apache2': 'dos', 'processtable': 'dos', 'udpstorm': 'dos',\n",
    "    'ipsweep': 'probe', 'nmap': 'probe', 'portsweep': 'probe', 'satan': 'probe', 'mscan': 'probe', 'saint': 'probe',\n",
    "    'ftp_write': 'r2l', 'guess_passwd': 'r2l', 'imap': 'r2l', 'multihop': 'r2l', 'phf': 'r2l',\n",
    "    'spy': 'r2l', 'warezclient': 'r2l', 'warezmaster': 'r2l', 'sendmail': 'r2l', 'named': 'r2l',\n",
    "    'snmpgetattack': 'r2l', 'snmpguess': 'r2l', 'xlock': 'r2l', 'xsnoop': 'r2l', 'worm': 'r2l',\n",
    "    'buffer_overflow': 'u2r', 'loadmodule': 'u2r', 'perl': 'u2r', 'rootkit': 'u2r',\n",
    "    'httptunnel': 'u2r', 'ps': 'u2r', 'sqlattack': 'u2r', 'xterm': 'u2r'\n",
    "}\n",
    "UNKNOWN_ATTACK_CATEGORY_ID = max(ATTACK_MAP_MULTI_CLASS.values()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5b30e",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171476e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Label Preprocessing Function\n",
    "def preprocess_labels_event_node_dask(df_chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Preprocesses labels for a chunk of data (Pandas DataFrame). \"\"\"\n",
    "    df_chunk['label_binary'] = df_chunk[LABEL_COL].apply(lambda x: 0 if x == NORMAL_TAG else 1)\n",
    "    \n",
    "    def map_to_general_cat_id(attack_name):\n",
    "        if attack_name == NORMAL_TAG:\n",
    "            return ATTACK_MAP_MULTI_CLASS[NORMAL_TAG]\n",
    "        general_category = KDD_SPECIFIC_TO_GENERAL_ATTACK_MAP.get(attack_name)\n",
    "        if general_category:\n",
    "            return ATTACK_MAP_MULTI_CLASS.get(general_category, UNKNOWN_ATTACK_CATEGORY_ID)\n",
    "        return UNKNOWN_ATTACK_CATEGORY_ID\n",
    "\n",
    "    df_chunk['label_multiclass_id'] = df_chunk[LABEL_COL].apply(map_to_general_cat_id)\n",
    "    return df_chunk[['label_binary', 'label_multiclass_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "custom_loader_cell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: Feature Scaler Fitting and Category Info (Minor refinement in comment/consistency)\n",
    "def fit_scalers_and_get_categories_info(ddf: dd.DataFrame, numerical_cols: list, categorical_cols: list):\n",
    "    \"\"\"\n",
    "    Computes means/stds for numerical columns and gets categorical feature names after one-hot encoding.\n",
    "    Assumes ddf[numerical_cols] contains numeric data and ddf[categorical_cols] has been categorized.\n",
    "    \"\"\"\n",
    "    print(\"Fitting scalers and determining categorical feature names...\")\n",
    "    # Numerical part remains the same\n",
    "    computed_means = ddf[numerical_cols].mean().compute()\n",
    "    computed_stds = ddf[numerical_cols].std().compute()\n",
    "    computed_stds = computed_stds.where(computed_stds != 0, 1.0) \n",
    "\n",
    "    # Categorical part: ddf[categorical_cols] should already have 'category' dtype with known categories\n",
    "    # from the .categorize() call in the main preprocessing function.\n",
    "    # So, ddf_cat_casted is essentially ddf[categorical_cols]\n",
    "    ddf_cat_subset = ddf[categorical_cols] \n",
    "    \n",
    "    # Get schema of dummy columns from a small sample for consistency.\n",
    "    # Since categories are known, head(1) or head(2) should be sufficient for schema.\n",
    "    # Compute=True is needed as head() is lazy.\n",
    "    sample_for_schema = ddf_cat_subset.head(max(2, ddf_cat_subset.npartitions if ddf_cat_subset.npartitions > 0 else 2), compute=True) \n",
    "    if sample_for_schema.empty and not ddf_cat_subset.known_divisions: # If dataframe is empty or structure is unknown after categorize\n",
    "        print(\"Warning: Categorical subset for dummy schema is empty or has unknown divisions. Using predefined columns for schema.\")\n",
    "        # Fallback to creating an empty DataFrame with expected columns if sample is problematic\n",
    "        # This might happen if the initial ddf was empty.\n",
    "        # This part might need more robust handling depending on how empty Dask DFs behave with categorize\n",
    "        categorical_feature_names_fitted = [] # Or load from a predefined schema if truly empty\n",
    "        if not ddf[categorical_cols].head(1).empty: # try again just in case.\n",
    "            dummy_ddf_schema = dd.get_dummies(ddf[categorical_cols].head(1), columns=categorical_cols, prefix=categorical_cols, dummy_na=False)\n",
    "            categorical_feature_names_fitted = list(dummy_ddf_schema.columns)\n",
    "\n",
    "    elif not sample_for_schema.empty:\n",
    "         dummy_ddf_schema = pd.get_dummies(sample_for_schema, columns=categorical_cols, prefix=categorical_cols, dummy_na=False)\n",
    "         categorical_feature_names_fitted = list(dummy_ddf_schema.columns)\n",
    "    else: # Fallback if sample_for_schema is empty but ddf_cat_subset was not entirely empty\n",
    "        print(\"Warning: Could not reliably determine dummy schema from sample. Categorical feature names might be incomplete.\")\n",
    "        categorical_feature_names_fitted = [f\"{col}_{cat}\" for col in categorical_cols for cat in ddf_cat_subset[col].cat.categories]\n",
    "\n",
    "\n",
    "    print(f\"Scalers determined. Categorical feature names derived: {len(categorical_feature_names_fitted)} features.\")\n",
    "    return computed_means, computed_stds, categorical_feature_names_fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab3fa3-da28-43c8-88b8-3e7c552b743c",
   "metadata": {},
   "source": [
    "## 6. Main Preprocessing Orchestration (Function Definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562b4365-33ba-4ffc-bb1f-b38cf41c5520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: Main Preprocessing Logic Function (CORRECTED for Warnings)\n",
    "def preprocess_and_save_temporal_data(raw_file_path: str, output_file_path: str,\n",
    "                                      numerical_cols_original_config: list,\n",
    "                                      categorical_cols: list, label_col: str,\n",
    "                                      is_training_set: bool = False,\n",
    "                                      fitted_scalers_and_cats_info: dict = None,\n",
    "                                      recent_window_size: int = 50):\n",
    "    \"\"\" Reads, preprocesses, and saves temporal data components. \"\"\"\n",
    "    print(f\"Starting preprocessing for: {raw_file_path}\")\n",
    "\n",
    "    current_numerical_cols = list(numerical_cols_original_config)\n",
    "    # For NSL-KDD .txt files, often they don't have headers, similar to KDD99\n",
    "    dtype_initial_read = {col: 'object' for col in COL_NAMES}\n",
    "\n",
    "    try:\n",
    "        # NSL-KDD .txt files are usually comma-separated without headers\n",
    "        ddf = dd.read_csv(raw_file_path, header=None, names=COL_NAMES, dtype=dtype_initial_read, blocksize='256MB', usecols=COL_NAMES)\n",
    "    except Exception as e:\n",
    "        print(f\"Dask read_csv error for {raw_file_path}: {e}. Attempting Pandas fallback.\")\n",
    "        df_chunks_list = []\n",
    "        for chunk_pd in pd.read_csv(raw_file_path, header=None, names=COL_NAMES, chunksize=100000, low_memory=False, dtype=str, usecols=COL_NAMES):\n",
    "             df_chunks_list.append(chunk_pd)\n",
    "        if not df_chunks_list:\n",
    "            raise ValueError(f\"Pandas fallback failed: No data read from {raw_file_path}\")\n",
    "        full_df_pandas = pd.concat(df_chunks_list, ignore_index=True)\n",
    "        nparts = max(1, int(np.ceil(len(full_df_pandas) / 500000)))\n",
    "        ddf = dd.from_pandas(full_df_pandas, npartitions=nparts)\n",
    "        print(f\"Pandas fallback to Dask DF successful with {nparts} partitions.\")\n",
    "\n",
    "    print(\"Converting original numerical columns to numeric type...\")\n",
    "    for col in numerical_cols_original_config:\n",
    "        if col in ddf.columns:\n",
    "            ddf[col] = dd.to_numeric(ddf[col], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            print(f\"Warning: Original numerical column '{col}' not found in ddf during initial conversion for {raw_file_path}.\")\n",
    "\n",
    "    if 'temp_event_timestamp' not in ddf.columns:\n",
    "        ddf['temp_event_timestamp'] = ddf.index.astype(np.int64)\n",
    "    else:\n",
    "        ddf['temp_event_timestamp'] = dd.to_numeric(ddf['temp_event_timestamp'], errors='coerce').fillna(0).astype(np.int64)\n",
    "\n",
    "    print(\"Resetting index to prepare for setting a known, sorted index...\")\n",
    "    ddf = ddf.reset_index()\n",
    "    index_col_name_after_reset = 'index'\n",
    "    if index_col_name_after_reset not in ddf.columns and 'level_0' in ddf.columns:\n",
    "        index_col_name_after_reset = 'level_0'\n",
    "    elif index_col_name_after_reset not in ddf.columns and 0 in ddf.columns and ddf.columns[0] == 0 :\n",
    "        ddf = ddf.rename(columns={0: index_col_name_after_reset})\n",
    "    if index_col_name_after_reset not in ddf.columns:\n",
    "        if ddf.columns[0] not in COL_NAMES and ddf.columns[0] not in categorical_cols and ddf.columns[0] not in numerical_cols_original_config:\n",
    "            print(f\"Warning: Reset index column name '{index_col_name_after_reset}' not found. Trying to use first column '{ddf.columns[0]}' as index.\")\n",
    "            index_col_name_after_reset = ddf.columns[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Could not identify the reset index column. Columns are: {ddf.columns}\")\n",
    "    print(f\"Setting '{index_col_name_after_reset}' as new sorted index...\")\n",
    "    ddf = ddf.set_index(index_col_name_after_reset, drop=True, sorted=True)\n",
    "\n",
    "    print(\"Persisting ddf after setting new index...\")\n",
    "    ddf = ddf.persist()\n",
    "    print(f\"After set_index & persist: ddf npartitions: {ddf.npartitions}, divisions known: {ddf.known_divisions}, divisions: {ddf.divisions}\")\n",
    "    if not ddf.known_divisions:\n",
    "         print(\"CRITICAL WARNING: Base ddf still has unknown divisions after set_index(sorted=True) and persist.\")\n",
    "\n",
    "    print(f\"Ensuring specified categorical columns ({categorical_cols}) are 'category' dtype...\")\n",
    "    actual_categorical_cols_in_ddf = []\n",
    "    for col in categorical_cols:\n",
    "        if col in ddf.columns:\n",
    "            if not isinstance(ddf[col].dtype, pd.CategoricalDtype):\n",
    "                 ddf[col] = ddf[col].astype('category')\n",
    "            actual_categorical_cols_in_ddf.append(col)\n",
    "        else:\n",
    "            print(f\"Warning: Categorical column '{col}' not found in ddf for {raw_file_path}.\")\n",
    "\n",
    "    print(f\"Categorizing columns: {actual_categorical_cols_in_ddf}\")\n",
    "    if actual_categorical_cols_in_ddf:\n",
    "        ddf = ddf.categorize(columns=actual_categorical_cols_in_ddf)\n",
    "    else:\n",
    "        print(\"Warning: No specified categorical columns found in ddf to categorize.\")\n",
    "\n",
    "    print(\"Persisting ddf after categorization...\")\n",
    "    ddf = ddf.persist()\n",
    "    print(f\"After categorize & persist: ddf npartitions: {ddf.npartitions}, divisions known: {ddf.known_divisions}, divisions: {ddf.divisions}\")\n",
    "\n",
    "    print(f\"Starting enhanced feature engineering with window size: {recent_window_size}...\")\n",
    "    newly_engineered_feature_cols = []\n",
    "\n",
    "    PROXY_SRC_IP_COL = 'service'\n",
    "    PROXY_DST_IP_COL = 'dst_host_srv_count'\n",
    "    PROXY_DST_PORT_COL = 'service'\n",
    "    TIMESTAMP_COL = 'temp_event_timestamp'\n",
    "    DURATION_COL = 'duration'\n",
    "\n",
    "    if not all(c in ddf.columns for c in [PROXY_SRC_IP_COL, TIMESTAMP_COL]):\n",
    "        print(f\"Skipping 'time_since_last_event...' due to missing required columns: {PROXY_SRC_IP_COL} or {TIMESTAMP_COL}\")\n",
    "    else:\n",
    "        new_feat_time_since_last_src_proxy = f'time_since_last_event_same_{PROXY_SRC_IP_COL}_proxy'\n",
    "        meta_time_since = pd.Series(name=new_feat_time_since_last_src_proxy, dtype=np.int64)\n",
    "        ddf[new_feat_time_since_last_src_proxy] = ddf.groupby(PROXY_SRC_IP_COL, observed=True)[TIMESTAMP_COL].transform(\n",
    "            lambda x: x.diff().fillna(0),\n",
    "            meta=meta_time_since\n",
    "        ).astype(np.float32)\n",
    "        newly_engineered_feature_cols.append(new_feat_time_since_last_src_proxy)\n",
    "        print(f\"Engineered feature: {new_feat_time_since_last_src_proxy}\")\n",
    "\n",
    "    group_cols_for_avg_duration = [PROXY_DST_PORT_COL, PROXY_DST_IP_COL]\n",
    "    if not all(col in ddf.columns for col in group_cols_for_avg_duration) or DURATION_COL not in ddf.columns:\n",
    "        print(f\"Skipping 'avg_duration_recent...' due to missing columns for grouping or value ({group_cols_for_avg_duration}, {DURATION_COL}).\")\n",
    "    else:\n",
    "        new_feat_avg_duration_proxy = f'avg_duration_recent_same_{PROXY_DST_PORT_COL}_{PROXY_DST_IP_COL}_proxy'\n",
    "        if PROXY_DST_IP_COL in ddf.columns and pd.api.types.is_float_dtype(ddf[PROXY_DST_IP_COL].dtype):\n",
    "            ddf[PROXY_DST_IP_COL] = ddf[PROXY_DST_IP_COL].astype(int)\n",
    "\n",
    "        def expanding_avg_duration_partition(df_partition, group_cols_list_internal, val_col_internal, new_col_name_internal):\n",
    "            if not df_partition.empty and all(c in df_partition.columns for c in group_cols_list_internal) and val_col_internal in df_partition.columns:\n",
    "                for group_col_check in group_cols_list_internal:\n",
    "                     if pd.api.types.is_numeric_dtype(df_partition[group_col_check]) and df_partition[group_col_check].nunique() > 200 and len(df_partition) > 1000:\n",
    "                        print(f\"Warning: Grouping by high-cardinality numeric column '{group_col_check}' in expanding_avg_duration_partition within a partition.\")\n",
    "                try:\n",
    "                    df_partition[new_col_name_internal] = df_partition.groupby(group_cols_list_internal, observed=True)[val_col_internal].transform(\n",
    "                        lambda x: x.expanding().mean().fillna(0)\n",
    "                    )\n",
    "                except Exception as e_gb_transform:\n",
    "                    print(f\"Error in expanding_avg_duration_partition's groupby/transform: {e_gb_transform}. Filling with 0.\")\n",
    "                    df_partition[new_col_name_internal] = pd.Series(0, index=df_partition.index, dtype=np.float32)\n",
    "            else:\n",
    "                df_partition[new_col_name_internal] = pd.Series(0, index=df_partition.index, dtype=np.float32)\n",
    "            return df_partition[new_col_name_internal]\n",
    "\n",
    "        meta_avg_duration = pd.Series(name=new_feat_avg_duration_proxy, dtype=np.float32)\n",
    "        ddf[new_feat_avg_duration_proxy] = ddf.map_partitions(\n",
    "            expanding_avg_duration_partition,\n",
    "            group_cols_list_internal=group_cols_for_avg_duration,\n",
    "            val_col_internal=DURATION_COL,\n",
    "            new_col_name_internal=new_feat_avg_duration_proxy,\n",
    "            meta=meta_avg_duration\n",
    "        ).astype(np.float32)\n",
    "        newly_engineered_feature_cols.append(new_feat_avg_duration_proxy)\n",
    "        print(f\"Engineered feature: {new_feat_avg_duration_proxy}\")\n",
    "\n",
    "    if PROXY_SRC_IP_COL not in ddf.columns:\n",
    "        print(f\"Skipping 'count_recent_same_service_cum_proxy' as {PROXY_SRC_IP_COL} is missing.\")\n",
    "    else:\n",
    "        new_feat_count_recent_src_proxy = f'count_recent_same_{PROXY_SRC_IP_COL}_cum_proxy'\n",
    "        ddf[new_feat_count_recent_src_proxy] = ddf.groupby(PROXY_SRC_IP_COL, observed=True).cumcount().astype(np.float32)\n",
    "        newly_engineered_feature_cols.append(new_feat_count_recent_src_proxy)\n",
    "        print(f\"Engineered feature: {new_feat_count_recent_src_proxy}\")\n",
    "\n",
    "    ddf = ddf.persist()\n",
    "    print(f\"Finished enhanced feature engineering. Successfully added columns: {newly_engineered_feature_cols}\")\n",
    "\n",
    "    if label_col in ddf.columns:\n",
    "        meta_labels_df = pd.DataFrame({'label_binary': pd.Series(dtype='int'), 'label_multiclass_id': pd.Series(dtype='int')})\n",
    "        processed_labels_ddf = ddf.map_partitions(preprocess_labels_event_node_dask, meta=meta_labels_df)\n",
    "    else:\n",
    "        print(f\"Warning: Label column '{label_col}' not found in ddf for {raw_file_path}. Creating dummy labels.\")\n",
    "        num_rows_approx = ddf.map_partitions(len).compute().sum() if ddf.npartitions > 0 else 0\n",
    "        dummy_labels_data = {'label_binary': np.zeros(num_rows_approx, dtype=int),\n",
    "                             'label_multiclass_id': np.zeros(num_rows_approx, dtype=int)}\n",
    "        processed_labels_ddf = dd.from_pandas(pd.DataFrame(dummy_labels_data), npartitions=ddf.npartitions if ddf.npartitions > 0 else 1)\n",
    "\n",
    "    print(\"Preprocessing features...\")\n",
    "    current_numerical_cols.extend([col for col in newly_engineered_feature_cols if col in ddf.columns and col not in current_numerical_cols])\n",
    "    print(f\"Final numerical_cols for scaling consideration: {current_numerical_cols}\")\n",
    "\n",
    "    scaler_params_to_return_or_use = {}\n",
    "    cat_feat_names_final = []\n",
    "\n",
    "    if is_training_set:\n",
    "        actual_numerical_cols_for_fit = [col for col in current_numerical_cols if col in ddf.columns]\n",
    "        actual_categorical_cols_for_dummies = [col for col in categorical_cols if col in ddf.columns]\n",
    "        \n",
    "        if not actual_numerical_cols_for_fit:\n",
    "            print(\"Warning: No numerical columns available for fitting scalers during training. Scalers will be empty.\")\n",
    "            fitted_means = pd.Series(dtype=float)\n",
    "            fitted_stds = pd.Series(dtype=float)\n",
    "            cat_feat_names_from_scaler_func = [] # Initialize if no numerical cols\n",
    "        else:\n",
    "            fitted_means, fitted_stds, cat_feat_names_from_scaler_func = fit_scalers_and_get_categories_info(\n",
    "                ddf,\n",
    "                actual_numerical_cols_for_fit,\n",
    "                actual_categorical_cols_in_ddf\n",
    "            )\n",
    "        cat_feat_names_final = cat_feat_names_from_scaler_func\n",
    "\n",
    "        scaler_params_to_return_or_use = {\n",
    "            'means': fitted_means.to_dict(), 'stds': fitted_stds.to_dict(),\n",
    "            'categorical_feature_names': cat_feat_names_final,\n",
    "            'all_numerical_cols_scaled': list(actual_numerical_cols_for_fit)\n",
    "        }\n",
    "    else: # Test set\n",
    "        if fitted_scalers_and_cats_info is None:\n",
    "            raise ValueError(\"fitted_scalers_and_cats_info must be provided for non-training sets.\")\n",
    "        fitted_means = pd.Series(fitted_scalers_and_cats_info['means'])\n",
    "        fitted_stds = pd.Series(fitted_scalers_and_cats_info['stds'])\n",
    "        cat_feat_names_final = fitted_scalers_and_cats_info['categorical_feature_names']\n",
    "        numerical_cols_scaled_during_training = fitted_scalers_and_cats_info.get('all_numerical_cols_scaled', [])\n",
    "        current_numerical_cols = [col for col in numerical_cols_scaled_during_training if col in ddf.columns]\n",
    "        missing_from_test_ddf = [col for col in numerical_cols_scaled_during_training if col not in ddf.columns]\n",
    "        if missing_from_test_ddf:\n",
    "            print(f\"Warning: Numerical columns scaled during training are missing from test ddf: {missing_from_test_ddf}.\")\n",
    "            for col_m in missing_from_test_ddf:\n",
    "                if col_m not in fitted_means: fitted_means[col_m] = 0.0\n",
    "                if col_m not in fitted_stds: fitted_stds[col_m] = 1.0\n",
    "        scaler_params_to_return_or_use = fitted_scalers_and_cats_info\n",
    "\n",
    "    numerical_cols_to_scale_in_ddf = [col for col in current_numerical_cols if col in ddf.columns]\n",
    "    if not numerical_cols_to_scale_in_ddf:\n",
    "        print(\"Warning: No numerical columns to scale found in current ddf. Creating empty scaled_numerical_ddf.\")\n",
    "        num_rows_for_empty_df = ddf.map_partitions(len).compute().sum() if ddf.npartitions > 0 else 0\n",
    "        scaled_numerical_ddf = dd.from_pandas(pd.DataFrame(index=pd.RangeIndex(num_rows_for_empty_df)),\n",
    "                                              npartitions=ddf.npartitions if ddf.npartitions > 0 else 1).persist()\n",
    "    else:\n",
    "        numerical_df_slice = ddf[numerical_cols_to_scale_in_ddf]\n",
    "        print(f\"Persisting numerical_df_slice (cols: {numerical_cols_to_scale_in_ddf}) before map_partitions for scaling...\")\n",
    "        numerical_df_slice = numerical_df_slice.persist()\n",
    "        print(f\"numerical_df_slice persisted: npart={numerical_df_slice.npartitions}, known_div={numerical_df_slice.known_divisions}, div={numerical_df_slice.divisions}\")\n",
    "\n",
    "        def scale_partition_func(partition_pd_df, means_pd_series, stds_pd_series):\n",
    "            cols_to_scale = partition_pd_df.columns\n",
    "            aligned_means = means_pd_series.reindex(cols_to_scale).fillna(0)\n",
    "            aligned_stds = stds_pd_series.reindex(cols_to_scale).fillna(1.0)\n",
    "            aligned_stds[aligned_stds == 0] = 1.0\n",
    "            return ((partition_pd_df - aligned_means) / aligned_stds).fillna(0)\n",
    "\n",
    "        if hasattr(numerical_df_slice, '_meta_nonempty') and not numerical_df_slice._meta_nonempty.empty:\n",
    "            meta_scaled_numerical = numerical_df_slice._meta_nonempty.copy().astype(float)\n",
    "        elif not numerical_df_slice._meta.empty:\n",
    "            meta_scaled_numerical = numerical_df_slice._meta.copy().astype(float)\n",
    "        else:\n",
    "             meta_scaled_numerical = pd.DataFrame(columns=numerical_df_slice.columns, dtype=float)\n",
    "\n",
    "        print(\"Applying scaling via map_partitions...\")\n",
    "        scaled_numerical_ddf = numerical_df_slice.map_partitions(\n",
    "            scale_partition_func,\n",
    "            means_pd_series=fitted_means,\n",
    "            stds_pd_series=fitted_stds,\n",
    "            meta=meta_scaled_numerical\n",
    "        ).persist()\n",
    "\n",
    "    print(f\"scaled_numerical_ddf persisted: npart={scaled_numerical_ddf.npartitions}, known_div={scaled_numerical_ddf.known_divisions}, div={scaled_numerical_ddf.divisions}\")\n",
    "\n",
    "    if not actual_categorical_cols_in_ddf or not cat_feat_names_final:\n",
    "        print(\"Warning: No categorical columns to process or no cat_feat_names defined. Creating empty aligned_processed_features_ddf_cat.\")\n",
    "        num_rows_for_empty_df = ddf.map_partitions(len).compute().sum() if ddf.npartitions > 0 else 0\n",
    "        empty_cat_df_pd = pd.DataFrame(columns=cat_feat_names_final if cat_feat_names_final else [],\n",
    "                                       index=pd.RangeIndex(num_rows_for_empty_df), dtype=np.int8)\n",
    "        aligned_processed_features_ddf_cat = dd.from_pandas(empty_cat_df_pd,\n",
    "                                                            npartitions=ddf.npartitions if ddf.npartitions > 0 else 1)\n",
    "        if ddf.npartitions > 0 and num_rows_for_empty_df > 0 and ddf.index.name is not None:\n",
    "            try:\n",
    "                aligned_processed_features_ddf_cat = aligned_processed_features_ddf_cat.set_index(ddf.index)\n",
    "            except Exception as e_set_index_empty:\n",
    "                 print(f\"Could not set index for empty categorical df: {e_set_index_empty}. Proceeding with default index.\")\n",
    "\n",
    "        aligned_processed_features_ddf_cat = aligned_processed_features_ddf_cat.persist()\n",
    "    else:\n",
    "        processed_features_ddf_cat_raw = dd.get_dummies(ddf[actual_categorical_cols_in_ddf],\n",
    "                                                        prefix=actual_categorical_cols_in_ddf, dummy_na=False)\n",
    "        def align_partition_columns_func(partition_df, target_columns_list):\n",
    "            aligned_partition = pd.DataFrame(0, index=partition_df.index, columns=target_columns_list)\n",
    "            common_cols_in_partition = [col for col in partition_df.columns if col in target_columns_list]\n",
    "            if common_cols_in_partition:\n",
    "                aligned_partition[common_cols_in_partition] = partition_df[common_cols_in_partition]\n",
    "            return aligned_partition.astype(np.int8)\n",
    "\n",
    "        meta_for_aligned_cat = pd.DataFrame(columns=cat_feat_names_final, dtype=np.int8)\n",
    "        aligned_processed_features_ddf_cat = processed_features_ddf_cat_raw.map_partitions(\n",
    "            align_partition_columns_func, target_columns_list=cat_feat_names_final, meta=meta_for_aligned_cat\n",
    "        ).persist()\n",
    "\n",
    "    if ddf.known_divisions:\n",
    "        print(f\"Base ddf has known divisions: {ddf.divisions}. Enforcing these divisions on derived DFs.\")\n",
    "        if scaled_numerical_ddf.divisions != ddf.divisions :\n",
    "             if not (scaled_numerical_ddf.npartitions == 1 and scaled_numerical_ddf.divisions[0] is None and scaled_numerical_ddf.divisions[1] is None):\n",
    "                print(f\"Aligning scaled_numerical_ddf divisions from ({scaled_numerical_ddf.divisions}) to base ddf ({ddf.divisions}).\")\n",
    "                scaled_numerical_ddf = scaled_numerical_ddf.repartition(divisions=ddf.divisions).persist()\n",
    "        if aligned_processed_features_ddf_cat.divisions != ddf.divisions:\n",
    "            if not (aligned_processed_features_ddf_cat.npartitions == 1 and aligned_processed_features_ddf_cat.divisions[0] is None and aligned_processed_features_ddf_cat.divisions[1] is None):\n",
    "                print(f\"Aligning aligned_cat_ddf divisions from ({aligned_processed_features_ddf_cat.divisions}) to base ddf ({ddf.divisions}).\")\n",
    "                aligned_processed_features_ddf_cat = aligned_processed_features_ddf_cat.repartition(divisions=ddf.divisions).persist()\n",
    "    \n",
    "    print(f\"Before concat: Numerical npart={scaled_numerical_ddf.npartitions}, known_div={scaled_numerical_ddf.known_divisions}, div={scaled_numerical_ddf.divisions}, cols={len(scaled_numerical_ddf.columns)}\")\n",
    "    print(f\"Before concat: Categorical npart={aligned_processed_features_ddf_cat.npartitions}, known_div={aligned_processed_features_ddf_cat.known_divisions}, div={aligned_processed_features_ddf_cat.divisions}, cols={len(aligned_processed_features_ddf_cat.columns)}\")\n",
    "\n",
    "    is_numerical_effectively_empty = len(scaled_numerical_ddf.columns) == 0 or (scaled_numerical_ddf.map_partitions(len).compute().sum() == 0)\n",
    "    is_categorical_effectively_empty = len(aligned_processed_features_ddf_cat.columns) == 0 or (aligned_processed_features_ddf_cat.map_partitions(len).compute().sum() == 0)\n",
    "    \n",
    "    num_original_rows = ddf.map_partitions(len).compute().sum() if ddf.npartitions > 0 else 0\n",
    "\n",
    "    if is_numerical_effectively_empty and is_categorical_effectively_empty:\n",
    "        print(\"Both numerical and categorical feature sets are effectively empty. Resulting features will be zero array.\")\n",
    "        x_np = np.zeros((num_original_rows, 1 if num_original_rows > 0 else 0), dtype=np.float32)\n",
    "    elif is_numerical_effectively_empty:\n",
    "        print(\"Numerical features are effectively empty. Using only categorical features.\")\n",
    "        final_features_ddf = aligned_processed_features_ddf_cat.persist()\n",
    "        x_np = final_features_ddf.compute().to_numpy(dtype=np.float32)\n",
    "    elif is_categorical_effectively_empty:\n",
    "        print(\"Categorical features are effectively empty. Using only numerical features.\")\n",
    "        final_features_ddf = scaled_numerical_ddf.persist()\n",
    "        x_np = final_features_ddf.compute().to_numpy(dtype=np.float32)\n",
    "    else:\n",
    "        try:\n",
    "            if not (scaled_numerical_ddf.known_divisions and aligned_processed_features_ddf_cat.known_divisions and scaled_numerical_ddf.divisions == aligned_processed_features_ddf_cat.divisions):\n",
    "                 print(\"Divisions mismatch or unknown before concat, attempting repartition to base ddf divisions or npartitions.\")\n",
    "                 if ddf.known_divisions:\n",
    "                    if scaled_numerical_ddf.divisions != ddf.divisions: scaled_numerical_ddf = scaled_numerical_ddf.repartition(divisions=ddf.divisions).persist()\n",
    "                    if aligned_processed_features_ddf_cat.divisions != ddf.divisions: aligned_processed_features_ddf_cat = aligned_processed_features_ddf_cat.repartition(divisions=ddf.divisions).persist()\n",
    "                 else:\n",
    "                     target_nparts = ddf.npartitions if ddf.npartitions > 0 else 1\n",
    "                     if scaled_numerical_ddf.npartitions != target_nparts: scaled_numerical_ddf = scaled_numerical_ddf.repartition(npartitions=target_nparts).persist()\n",
    "                     if aligned_processed_features_ddf_cat.npartitions != target_nparts: aligned_processed_features_ddf_cat = aligned_processed_features_ddf_cat.repartition(npartitions=target_nparts).persist()\n",
    "\n",
    "            if (scaled_numerical_ddf.known_divisions and\n",
    "                aligned_processed_features_ddf_cat.known_divisions and\n",
    "                scaled_numerical_ddf.divisions == aligned_processed_features_ddf_cat.divisions):\n",
    "                print(\"Attempting dd.concat with known and matching divisions.\")\n",
    "                final_features_ddf = dd.concat([scaled_numerical_ddf, aligned_processed_features_ddf_cat], axis=1, interleave_partitions=False)\n",
    "            else:\n",
    "                print(\"Divisions still unknown or mismatching for concat. Falling back to dd.merge on index after reset.\")\n",
    "                df1_indexed = scaled_numerical_ddf.reset_index()\n",
    "                df2_indexed = aligned_processed_features_ddf_cat.reset_index()\n",
    "                merge_on_col = 'index'\n",
    "                if merge_on_col not in df1_indexed.columns and 'level_0' in df1_indexed.columns: df1_indexed = df1_indexed.rename(columns={'level_0': merge_on_col})\n",
    "                if merge_on_col not in df2_indexed.columns and 'level_0' in df2_indexed.columns: df2_indexed = df2_indexed.rename(columns={'level_0': merge_on_col})\n",
    "                if merge_on_col not in df1_indexed.columns: raise ValueError(f\"Merge column '{merge_on_col}' not found in df1_indexed. Cols: {df1_indexed.columns}\")\n",
    "                if merge_on_col not in df2_indexed.columns: raise ValueError(f\"Merge column '{merge_on_col}' not found in df2_indexed. Cols: {df2_indexed.columns}\")\n",
    "                final_features_ddf = dd.merge(df1_indexed, df2_indexed, on=merge_on_col, how='inner')\n",
    "                if merge_on_col in final_features_ddf.columns: final_features_ddf = final_features_ddf.set_index(merge_on_col, drop=True, sorted=True)\n",
    "                print(\"Used dd.merge with reset_index as a fallback.\")\n",
    "            \n",
    "            final_features_ddf = final_features_ddf.persist()\n",
    "            x_np = final_features_ddf.compute().to_numpy(dtype=np.float32)\n",
    "        except Exception as e_final_combine:\n",
    "            print(f\"Final combination (concat or merge) failed: {e_final_combine}. Creating dummy x_np.\")\n",
    "            num_feat_dim_est = (len(scaled_numerical_ddf.columns) if not is_numerical_effectively_empty else 0) + \\\n",
    "                               (len(aligned_processed_features_ddf_cat.columns) if not is_categorical_effectively_empty else 0)\n",
    "            num_feat_dim_est = num_feat_dim_est if num_feat_dim_est > 0 else 1\n",
    "            x_np = np.zeros((num_original_rows, num_feat_dim_est), dtype=np.float32)\n",
    "\n",
    "    if len(x_np) != num_original_rows:\n",
    "        print(f\"Warning: x_np row count {len(x_np)} mismatch with original data {num_original_rows} after combination.\")\n",
    "        if len(x_np) == 0 and num_original_rows > 0:\n",
    "            print(\"x_np is empty, creating dummy zero features.\")\n",
    "            num_feat_dim_est = (len(scaled_numerical_ddf.columns) if hasattr(scaled_numerical_ddf, 'columns') and scaled_numerical_ddf.columns is not None else 0) + \\\n",
    "                               (len(aligned_processed_features_ddf_cat.columns) if hasattr(aligned_processed_features_ddf_cat, 'columns') and aligned_processed_features_ddf_cat.columns is not None else 0)\n",
    "            num_feat_dim_est = num_feat_dim_est if num_feat_dim_est > 0 else 1\n",
    "            x_np = np.zeros((num_original_rows, num_feat_dim_est), dtype=np.float32)\n",
    "\n",
    "    node_feat_dim = x_np.shape[1] if x_np.ndim == 2 and x_np.shape[0] > 0 else 0\n",
    "    print(f\"Node feature dimension: {node_feat_dim}\")\n",
    "\n",
    "    labels_computed_df = processed_labels_ddf.compute()\n",
    "    if len(labels_computed_df) != num_original_rows:\n",
    "        print(f\"Warning: Label count {len(labels_computed_df)} mismatch with original data {num_original_rows}. Adjusting labels.\")\n",
    "        if num_original_rows == 0:\n",
    "            y_binary_np = np.array([], dtype=np.int64)\n",
    "            y_multiclass_np = np.array([], dtype=np.int64)\n",
    "        elif len(labels_computed_df) == 0 and num_original_rows > 0:\n",
    "            y_binary_np = np.zeros(num_original_rows, dtype=np.int64)\n",
    "            y_multiclass_np = np.zeros(num_original_rows, dtype=np.int64)\n",
    "        elif len(labels_computed_df) > num_original_rows:\n",
    "            y_binary_np = labels_computed_df['label_binary'].to_numpy(dtype=np.int64)[:num_original_rows]\n",
    "            y_multiclass_np = labels_computed_df['label_multiclass_id'].to_numpy(dtype=np.int64)[:num_original_rows]\n",
    "        else:\n",
    "            y_binary_np = np.pad(labels_computed_df['label_binary'].to_numpy(dtype=np.int64), (0, num_original_rows - len(labels_computed_df)), 'edge')\n",
    "            y_multiclass_np = np.pad(labels_computed_df['label_multiclass_id'].to_numpy(dtype=np.int64), (0, num_original_rows - len(labels_computed_df)), 'edge')\n",
    "    else:\n",
    "        y_binary_np = labels_computed_df['label_binary'].to_numpy(dtype=np.int64)\n",
    "        y_multiclass_np = labels_computed_df['label_multiclass_id'].to_numpy(dtype=np.int64)\n",
    "    \n",
    "    print(\"Conversion to NumPy arrays complete.\")\n",
    "\n",
    "    num_nodes = len(x_np)\n",
    "    x_tensor = torch.from_numpy(x_np).float()\n",
    "    edge_index_tensor = torch.empty((2,0)).long()\n",
    "    if num_nodes > 1:\n",
    "        edge_src = torch.arange(0, num_nodes - 1)\n",
    "        edge_dst = torch.arange(1, num_nodes)\n",
    "        edge_index_tensor = torch.stack([edge_src, edge_dst], dim=0).long()\n",
    "\n",
    "    ts_tensor = torch.arange(0, num_nodes).long()\n",
    "    y_binary_tensor = torch.from_numpy(y_binary_np).float().unsqueeze(1) if len(y_binary_np)>0 else torch.empty((0,1)).float()\n",
    "    y_multiclass_tensor = torch.from_numpy(y_multiclass_np).long() if len(y_multiclass_np)>0 else torch.empty(0).long()\n",
    "\n",
    "    data_to_save = {\n",
    "        'x': x_tensor, 'edge_index': edge_index_tensor, 'ts': ts_tensor,\n",
    "        'y_binary': y_binary_tensor, 'y_multiclass': y_multiclass_tensor, 'num_nodes': num_nodes\n",
    "    }\n",
    "    torch.save(data_to_save, output_file_path)\n",
    "    print(f\"Processed data saved to {output_file_path}\")\n",
    "\n",
    "    current_set_metadata = {\n",
    "        'node_feat_dim': node_feat_dim, 'num_nodes': num_nodes,\n",
    "        'labels_binary_unique_count': len(np.unique(y_binary_np)) if len(y_binary_np) > 0 else 0,\n",
    "        'labels_multiclass_unique_count': len(np.unique(y_multiclass_np)) if len(y_multiclass_np) > 0 else 0,\n",
    "        'engineered_feature_cols': newly_engineered_feature_cols\n",
    "    }\n",
    "\n",
    "    if is_training_set:\n",
    "        pos_weight_binary = 1.0\n",
    "        if len(y_binary_np) > 0 :\n",
    "            sum_y_binary = np.sum(y_binary_np)\n",
    "            if sum_y_binary > 0 and sum_y_binary < len(y_binary_np):\n",
    "                neg_count = len(y_binary_np) - sum_y_binary\n",
    "                pos_count = sum_y_binary\n",
    "                pos_weight_binary = neg_count / (pos_count + 1e-7)\n",
    "            elif sum_y_binary == len(y_binary_np):\n",
    "                 print(\"Warning: All labels are positive in training set for pos_weight calculation.\")\n",
    "                 pos_weight_binary = 0.01\n",
    "            else:\n",
    "                 print(\"Warning: No positive labels in training set for pos_weight calculation.\")\n",
    "                 pos_weight_binary = 1.0\n",
    "        current_set_metadata['pos_weight_binary'] = pos_weight_binary\n",
    "    \n",
    "    return current_set_metadata, scaler_params_to_return_or_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49628f-5666-47cd-bc29-0d3933214633",
   "metadata": {},
   "source": [
    "## 7. Execute Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25eb960d-6302-4bb5-afda-e09ce17923a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Preprocessing Pipeline for NSL-KDD ---\n",
      "✅ Found NSL-KDD training file: ./data/KDDTrain+.txt\n",
      "✅ Found NSL-KDD test file: ./data/KDDTest-21.txt\n",
      "Proceeding with preprocessing...\n",
      "Starting preprocessing for: ./data/KDDTrain+.txt\n",
      "Converting original numerical columns to numeric type...\n",
      "Resetting index to prepare for setting a known, sorted index...\n",
      "Setting 'index' as new sorted index...\n",
      "Persisting ddf after setting new index...\n",
      "After set_index & persist: ddf npartitions: 1, divisions known: True, divisions: (0, 125972)\n",
      "Ensuring specified categorical columns (['protocol_type', 'service', 'flag']) are 'category' dtype...\n",
      "Categorizing columns: ['protocol_type', 'service', 'flag']\n",
      "Persisting ddf after categorization...\n",
      "After categorize & persist: ddf npartitions: 1, divisions known: True, divisions: (0, 125972)\n",
      "Starting enhanced feature engineering with window size: 50...\n",
      "Engineered feature: time_since_last_event_same_service_proxy\n",
      "Engineered feature: avg_duration_recent_same_service_dst_host_srv_count_proxy\n",
      "Engineered feature: count_recent_same_service_cum_proxy\n",
      "Warning: Grouping by high-cardinality numeric column 'dst_host_srv_count' in expanding_avg_duration_partition within a partition.\n",
      "Finished enhanced feature engineering. Successfully added columns: ['time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']\n",
      "Preprocessing features...\n",
      "Final numerical_cols for scaling consideration: ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']\n",
      "Fitting scalers and determining categorical feature names...\n",
      "Scalers determined. Categorical feature names derived: 84 features.\n",
      "Persisting numerical_df_slice (cols: ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']) before map_partitions for scaling...\n",
      "numerical_df_slice persisted: npart=1, known_div=True, div=(0, 125972)\n",
      "Applying scaling via map_partitions...\n",
      "scaled_numerical_ddf persisted: npart=1, known_div=True, div=(0, 125972)\n",
      "Base ddf has known divisions: (0, 125972). Enforcing these divisions on derived DFs.\n",
      "Before concat: Numerical npart=1, known_div=True, div=(0, 125972), cols=41\n",
      "Before concat: Categorical npart=1, known_div=True, div=(0, 125972), cols=84\n",
      "Attempting dd.concat with known and matching divisions.\n",
      "Node feature dimension: 125\n",
      "Conversion to NumPy arrays complete.\n",
      "Processed data saved to ./processed_data_nslkdd/train_temporal_data_nslkdd.pt\n",
      "\n",
      "--- Training Data Preprocessing Summary (NSL-KDD) ---\n",
      "node_feat_dim: 125\n",
      "num_nodes: 125973\n",
      "labels_binary_unique_count: 2\n",
      "labels_multiclass_unique_count: 5\n",
      "engineered_feature_cols: ['time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']\n",
      "pos_weight_binary: 1.148609926656748\n",
      "Starting preprocessing for: ./data/KDDTest-21.txt\n",
      "Converting original numerical columns to numeric type...\n",
      "Resetting index to prepare for setting a known, sorted index...\n",
      "Setting 'index' as new sorted index...\n",
      "Persisting ddf after setting new index...\n",
      "After set_index & persist: ddf npartitions: 1, divisions known: True, divisions: (0, 11849)\n",
      "Ensuring specified categorical columns (['protocol_type', 'service', 'flag']) are 'category' dtype...\n",
      "Categorizing columns: ['protocol_type', 'service', 'flag']\n",
      "Persisting ddf after categorization...\n",
      "After categorize & persist: ddf npartitions: 1, divisions known: True, divisions: (0, 11849)\n",
      "Starting enhanced feature engineering with window size: 50...\n",
      "Engineered feature: time_since_last_event_same_service_proxy\n",
      "Engineered feature: avg_duration_recent_same_service_dst_host_srv_count_proxy\n",
      "Engineered feature: count_recent_same_service_cum_proxy\n",
      "Warning: Grouping by high-cardinality numeric column 'dst_host_srv_count' in expanding_avg_duration_partition within a partition.\n",
      "Finished enhanced feature engineering. Successfully added columns: ['time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']\n",
      "Preprocessing features...\n",
      "Final numerical_cols for scaling consideration: ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']\n",
      "Persisting numerical_df_slice (cols: ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']) before map_partitions for scaling...\n",
      "numerical_df_slice persisted: npart=1, known_div=True, div=(0, 11849)\n",
      "Applying scaling via map_partitions...\n",
      "scaled_numerical_ddf persisted: npart=1, known_div=True, div=(0, 11849)\n",
      "Base ddf has known divisions: (0, 11849). Enforcing these divisions on derived DFs.\n",
      "Before concat: Numerical npart=1, known_div=True, div=(0, 11849), cols=41\n",
      "Before concat: Categorical npart=1, known_div=True, div=(0, 11849), cols=84\n",
      "Attempting dd.concat with known and matching divisions.\n",
      "Node feature dimension: 125\n",
      "Conversion to NumPy arrays complete.\n",
      "Processed data saved to ./processed_data_nslkdd/test_temporal_data_nslkdd.pt\n",
      "\n",
      "--- Test Data Preprocessing Summary (NSL-KDD) ---\n",
      "node_feat_dim: 125\n",
      "num_nodes: 11850\n",
      "labels_binary_unique_count: 2\n",
      "labels_multiclass_unique_count: 5\n",
      "engineered_feature_cols: ['time_since_last_event_same_service_proxy', 'avg_duration_recent_same_service_dst_host_srv_count_proxy', 'count_recent_same_service_cum_proxy']\n",
      "\n",
      "Global metadata saved to ./processed_data_nslkdd/metadata_nslkdd.json\n",
      "--- Data Preprocessing Pipeline Finished ---\n"
     ]
    }
   ],
   "source": [
    "# Cell: Main execution block\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Starting Data Preprocessing Pipeline for NSL-KDD ---\")\n",
    "\n",
    "    # 確保 RAW_DATA_DIR 已在全局配置中定義 (例如 './data/')\n",
    "    # 確保 TRAIN_FILE 和 TEST_FILE 已在全局配置中定義為您 NSL-KDD 數據集的文件名\n",
    "\n",
    "    # 創建原始數據目錄 (如果不存在)\n",
    "    if not os.path.exists(RAW_DATA_DIR):\n",
    "        os.makedirs(RAW_DATA_DIR)\n",
    "        print(f\"Created directory: {RAW_DATA_DIR}\")\n",
    "        print(f\"This directory is for your raw NSL-KDD dataset files.\")\n",
    "\n",
    "    # 定義預期的 NSL-KDD 文件路徑\n",
    "    train_raw_path = os.path.join(RAW_DATA_DIR, TRAIN_FILE)\n",
    "    test_raw_path = os.path.join(RAW_DATA_DIR, TEST_FILE)\n",
    "\n",
    "    # 檢查數據集文件是否存在\n",
    "    files_missing = False\n",
    "    if not os.path.exists(train_raw_path):\n",
    "        print(f\"❌ ERROR: Training file '{TRAIN_FILE}' not found in '{os.path.abspath(RAW_DATA_DIR)}'.\")\n",
    "        files_missing = True\n",
    "    else:\n",
    "        print(f\"✅ Found NSL-KDD training file: {train_raw_path}\")\n",
    "\n",
    "    if not os.path.exists(test_raw_path):\n",
    "        print(f\"❌ ERROR: Test file '{TEST_FILE}' not found in '{os.path.abspath(RAW_DATA_DIR)}'.\")\n",
    "        files_missing = True\n",
    "    else:\n",
    "        print(f\"✅ Found NSL-KDD test file: {test_raw_path}\")\n",
    "\n",
    "    if files_missing:\n",
    "        print(\"\\n‼️ ACTION REQUIRED: Please download the NSL-KDD dataset files.\")\n",
    "        print(\"   You can download them from sources like:\")\n",
    "        print(\"     1. The GitHub repository you provided: https://github.com/HoaNP/NSL-KDD-DataSet\")\n",
    "        print(\"        - On that page, you can click 'Code' (green button) -> 'Download ZIP'.\")\n",
    "        print(\"        - After downloading and unzipping, copy the required .txt files (e.g., KDDTrain+.txt, KDDTest+.txt) into the\")\n",
    "        print(f\"          '{os.path.abspath(RAW_DATA_DIR)}' directory.\")\n",
    "        print(\"     2. University of New Brunswick (UNB) CIC Datasets (usually requires agreeing to terms):\")\n",
    "        print(\"        https://www.unb.ca/cic/datasets/nsl.html\")\n",
    "        print(\"\\n   Ensure the filenames in your notebook's configuration cell match the downloaded files.\")\n",
    "        # 停止進一步執行，因為數據文件缺失\n",
    "        raise FileNotFoundError(f\"NSL-KDD files ('{TRAIN_FILE}', '{TEST_FILE}') not found in '{RAW_DATA_DIR}'. Please download them.\")\n",
    "    else:\n",
    "        print(\"All required NSL-KDD dataset files found. Proceeding with preprocessing...\")\n",
    "\n",
    "        # 只有在文件存在時才執行預處理\n",
    "        train_set_metadata, fitted_scalers_and_categories = preprocess_and_save_temporal_data(\n",
    "            raw_file_path=train_raw_path, output_file_path=PROCESSED_TRAIN_FILE,\n",
    "            numerical_cols_original_config=NUMERICAL_COLS,\n",
    "            categorical_cols=CATEGORICAL_COLS, label_col=LABEL_COL,\n",
    "            is_training_set=True\n",
    "        )\n",
    "\n",
    "        if train_set_metadata and fitted_scalers_and_categories:\n",
    "            print(\"\\n--- Training Data Preprocessing Summary (NSL-KDD) ---\")\n",
    "            for key, val in train_set_metadata.items(): print(f\"{key}: {val}\")\n",
    "\n",
    "            test_set_metadata, _ = preprocess_and_save_temporal_data(\n",
    "                raw_file_path=test_raw_path, output_file_path=PROCESSED_TEST_FILE,\n",
    "                numerical_cols_original_config=NUMERICAL_COLS,\n",
    "                categorical_cols=CATEGORICAL_COLS, label_col=LABEL_COL,\n",
    "                is_training_set=False, fitted_scalers_and_cats_info=fitted_scalers_and_categories\n",
    "            )\n",
    "\n",
    "            if test_set_metadata:\n",
    "                print(\"\\n--- Test Data Preprocessing Summary (NSL-KDD) ---\")\n",
    "                for key, val in test_set_metadata.items(): print(f\"{key}: {val}\")\n",
    "\n",
    "                global_metadata_to_save = {\n",
    "                    'NODE_FEAT_DIM': train_set_metadata['node_feat_dim'],\n",
    "                    'NUM_CLASSES_BINARY': 2,\n",
    "                    'NUM_CLASSES_MULTI': len(ATTACK_MAP_MULTI_CLASS) + (1 if UNKNOWN_ATTACK_CATEGORY_ID > max(ATTACK_MAP_MULTI_CLASS.values()) else 0) ,\n",
    "                    'POS_WEIGHT_BINARY': train_set_metadata.get('pos_weight_binary', 1.0),\n",
    "                    'train_num_nodes': train_set_metadata['num_nodes'],\n",
    "                    'test_num_nodes': test_set_metadata['num_nodes'],\n",
    "                    'categorical_feature_names': fitted_scalers_and_categories.get('categorical_feature_names', []),\n",
    "                    'numerical_cols_list': fitted_scalers_and_categories.get('all_numerical_cols_scaled', NUMERICAL_COLS),\n",
    "                    'engineered_feature_cols_list': train_set_metadata.get('engineered_feature_cols', [])\n",
    "                }\n",
    "                \n",
    "                with open(METADATA_FILE, 'w') as f:\n",
    "                    json.dump(global_metadata_to_save, f, indent=4)\n",
    "                print(f\"\\nGlobal metadata saved to {METADATA_FILE}\")\n",
    "        else:\n",
    "            print(\"Training data processing failed or was skipped due to missing files. Aborting subsequent steps.\")\n",
    "        print(\"--- Data Preprocessing Pipeline Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21455837-2372-429b-9682-953c9e6cfc6b",
   "metadata": {},
   "source": [
    "## 8. Notes on Preprocessing\n",
    "- **Memory for `.compute()`**: The step `final_features_ddf.compute().to_numpy(dtype=np.float32)` will load all processed features into memory. For extremely large datasets where even the processed feature matrix doesn't fit, this part needs to be re-written to save the Dask DataFrame to a format like Parquet and then load it in chunks in the training script, or process Dask Arrays partition by partition into tensors.\n",
    "- **Dask `get_dummies` Consistency**: Ensuring `dd.get_dummies` on the test set produces columns consistent with the training set is critical. The `aligned_processed_features_ddf_cat` logic attempts to handle this by reindexing partitions based on `cat_feat_names` derived from the training set.\n",
    "- **Timestamps (`ts`)**: Currently, event indices are used as timestamps. If your data has actual timestamps, they should be used and appropriately scaled/normalized if necessary for the time encoder in TGAT.\n",
    "- **Error Handling & Robustness**: More error handling can be added, especially around file I/O and Dask computations. The KDD fallback is one example.\n",
    "- **Dask Performance**: `blocksize` in `dd.read_csv` and `npartitions` in `dd.from_pandas` can significantly affect Dask performance. These may need tuning based on your specific dataset size and system resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd438c-e086-44fb-817c-947e7a5936dc",
   "metadata": {},
   "source": [
    "# train_tgat_from_processed.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747aa7d6-50d9-4365-bb99-77626ea95781",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TGAT Model Training from Preprocessed Data\n",
    "This notebook loads preprocessed temporal graph data and trains the TGAT model for network intrusion detection.\n",
    "**Prerequisites**:\n",
    "- Run `preprocess_kdd_large.ipynb` first to generate the `processed_data_large` directory with training/testing data and metadata.\n",
    "**Steps**:\n",
    "1. Configure paths and hyperparameters.\n",
    "2. Define utility functions, TGAT model, and TemporalNeighborLoader.\n",
    "3. Implement data loading function for preprocessed files.\n",
    "4. Implement training and evaluation functions.\n",
    "5. Orchestrate the training process.\n",
    "6. Plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12fa2200-b4b2-455e-b18f-e2133bb9dc90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import TemporalData\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
    "                             confusion_matrix, roc_auc_score, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# --- Configuration ---\n",
    "RAW_DATA_DIR = './data/' # 確保與預處理配置一致\n",
    "TRAIN_FILE = 'KDDTrain+.txt' # 確保與預處理配置一致\n",
    "TEST_FILE = 'KDDTest+.txt'      # 確保與預處理配置一致 (或 'KDDTest-21.txt')\n",
    "\n",
    "PROCESSED_DATA_DIR = './processed_data_nslkdd/' # 使用新的 NSL-KDD 處理目錄\n",
    "PROCESSED_TRAIN_FILE = os.path.join(PROCESSED_DATA_DIR, 'train_temporal_data_nslkdd.pt')\n",
    "PROCESSED_TEST_FILE = os.path.join(PROCESSED_DATA_DIR, 'test_temporal_data_nslkdd.pt')\n",
    "METADATA_FILE = os.path.join(PROCESSED_DATA_DIR, 'metadata_nslkdd.json')\n",
    "\n",
    "MODEL_SAVE_DIR = './saved_models_nslkdd/' # 更改以避免覆蓋\n",
    "BEST_MODEL_NAME = 'best_tgat_model_nslkdd.pth'\n",
    "\n",
    "DEFAULT_DEVICE_ID = 0\n",
    "DEVICE = torch.device(f'cuda:{DEFAULT_DEVICE_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- TGAT Model Hyperparameters ---\n",
    "EPOCHS = 3 # 與您的日誌輸出一致\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0005\n",
    "HIDDEN_DIM = 256\n",
    "TIME_DIM = 64\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "NUM_NEIGHBORS = [10, 5]\n",
    "CLIP_GRAD_NORM = 1.0\n",
    "WEIGHT_DECAY = 1e-5\n",
    "USE_FOCAL_LOSS = True\n",
    "CLASSIFICATION_MODE = 'binary'\n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "\n",
    "# --- Parameters for Smarter Sampling in TemporalNeighborLoader ---\n",
    "RECENCY_BIAS_FACTOR = 0.9\n",
    "FEATURE_SIMILARITY_COL_NAME = 'service'\n",
    "FEATURE_SIMILARITY_WEIGHT = 0.3\n",
    "\n",
    "# --- Parameters for Sequence Modeling ---\n",
    "BATCH_SIZE_SEQ_EMBED_GEN = BATCH_SIZE\n",
    "SEQUENCE_LENGTH = 10\n",
    "STEP_SIZE = 5\n",
    "SEQ_LABEL_MODE = 'any_attack'\n",
    "BATCH_SIZE_SEQ_MODEL = 64\n",
    "LEARNING_RATE_SEQ_MODEL = 1e-4\n",
    "EPOCHS_SEQ_MODEL = 20 # 與您的日誌輸出一致\n",
    "SEQ_MODEL_EMBEDDING_DIM_ACTUAL = HIDDEN_DIM\n",
    "SEQ_MODEL_HIDDEN_DIM = 128\n",
    "SEQ_MODEL_NUM_LAYERS = 1\n",
    "SEQ_MODEL_RNN_TYPE = 'GRU'\n",
    "SEQ_MODEL_DROPOUT = 0.2\n",
    "\n",
    "# --- NSL-KDD Dataset Specific Column Names ---\n",
    "COL_NAMES = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "    'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "    'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
    "    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
    "    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
    "    'attack_type', 'difficulty_score'\n",
    "]\n",
    "ATTACK_MAP_MULTI_CLASS = {\n",
    "    'normal': 0, 'dos': 1, 'probe': 2, 'r2l': 3, 'u2r': 4\n",
    "}\n",
    "KDD_SPECIFIC_TO_GENERAL_ATTACK_MAP = {\n",
    "    'back': 'dos', 'land': 'dos', 'neptune': 'dos', 'pod': 'dos', 'smurf': 'dos', 'teardrop': 'dos',\n",
    "    'mailbomb': 'dos', 'apache2': 'dos', 'processtable': 'dos', 'udpstorm': 'dos',\n",
    "    'ipsweep': 'probe', 'nmap': 'probe', 'portsweep': 'probe', 'satan': 'probe', 'mscan': 'probe', 'saint': 'probe',\n",
    "    'ftp_write': 'r2l', 'guess_passwd': 'r2l', 'imap': 'r2l', 'multihop': 'r2l', 'phf': 'r2l',\n",
    "    'spy': 'r2l', 'warezclient': 'r2l', 'warezmaster': 'r2l', 'sendmail': 'r2l', 'named': 'r2l',\n",
    "    'snmpgetattack': 'r2l', 'snmpguess': 'r2l', 'xlock': 'r2l', 'xsnoop': 'r2l', 'worm': 'r2l',\n",
    "    'buffer_overflow': 'u2r', 'loadmodule': 'u2r', 'perl': 'u2r', 'rootkit': 'u2r',\n",
    "    'httptunnel': 'u2r', 'ps': 'u2r', 'sqlattack': 'u2r', 'xterm': 'u2r'\n",
    "}\n",
    "UNKNOWN_ATTACK_CATEGORY_ID = max(ATTACK_MAP_MULTI_CLASS.values()) + 1\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "def get_device():\n",
    "    return DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038487d2-dd90-435a-8a7f-c40d56a9158e",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19be5024-d6af-4536-8e83-b6b141e46d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell：Utility Functions & Custom Loss\n",
    "\n",
    "def get_device():\n",
    "    return DEVICE\n",
    "\n",
    "def plot_confusion_matrix_custom(y_true, y_pred, class_names, title='Confusion Matrix'):\n",
    "    if not y_true or not y_pred or len(y_true) != len(y_pred) or len(y_true) == 0:\n",
    "        print(f\"Cannot plot confusion matrix for {title}: y_true or y_pred is empty or mismatched.\")\n",
    "        return\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title); plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.show()\n",
    "\n",
    "def print_metrics(epoch_str, loss, accuracy, precision, recall, f1, auc=None, phase='Train', class_report=None):\n",
    "    loss_str = f\"{loss:.4f}\" if loss is not None and not np.isnan(loss) else \"N/A\"\n",
    "    acc_str = f\"{accuracy:.4f}\" if accuracy is not None and not np.isnan(accuracy) else \"N/A\"\n",
    "    prec_str = f\"{precision:.4f}\" if precision is not None and not np.isnan(precision) else \"N/A\"\n",
    "    rec_str = f\"{recall:.4f}\" if recall is not None and not np.isnan(recall) else \"N/A\"\n",
    "    f1_str = f\"{f1:.4f}\" if f1 is not None and not np.isnan(f1) else \"N/A\"\n",
    "    \n",
    "    print(f\"{epoch_str} | {phase} Loss: {loss_str} | Acc: {acc_str} | Prec: {prec_str} | Rec: {rec_str} | F1: {f1_str}\", end=\"\")\n",
    "    if auc is not None and not np.isnan(auc):\n",
    "        print(f\" | AUC: {auc:.4f}\", end=\"\")\n",
    "    print() # Newline\n",
    "    if class_report:\n",
    "        if isinstance(class_report, str): # If it's already a formatted string\n",
    "            print(class_report)\n",
    "        elif isinstance(class_report, dict): # If it's a dict from classification_report\n",
    "            print(\"Classification Report (Dict):\\n\", json.dumps(class_report, indent=2))\n",
    "\n",
    "class FocalLoss(nn.Module): # Keep for future use\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', pos_weight_for_bce=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight_for_bce = pos_weight_for_bce\n",
    "\n",
    "    def forward(self, inputs, targets): \n",
    "        if self.pos_weight_for_bce is not None:\n",
    "            bce_loss = F.binary_cross_entropy_with_logits(inputs, targets.float(), reduction='none', pos_weight=self.pos_weight_for_bce)\n",
    "        else:\n",
    "            bce_loss = F.binary_cross_entropy_with_logits(inputs, targets.float(), reduction='none')\n",
    "        \n",
    "        pt = torch.exp(-bce_loss) \n",
    "        \n",
    "        alpha_t = self.alpha\n",
    "        if self.alpha is not None: \n",
    "            if isinstance(self.alpha, (float, int)): \n",
    "                alpha_tensor = torch.tensor([self.alpha], device=inputs.device, dtype=inputs.dtype)\n",
    "                alpha_t = torch.where(targets == 1, alpha_tensor, 1.0 - alpha_tensor)\n",
    "            elif isinstance(self.alpha, torch.Tensor) and self.alpha.ndim == 0: \n",
    "                 alpha_tensor = self.alpha.to(inputs.device, dtype=inputs.dtype)\n",
    "                 alpha_t = torch.where(targets == 1, alpha_tensor, 1.0 - alpha_tensor)\n",
    "            if alpha_t.ndim == 1 and targets.ndim > 1 and alpha_t.shape[0] == targets.shape[0] and targets.shape[1] == 1: # Ensure broadcasting for [B,1] targets\n",
    "                alpha_t = alpha_t.unsqueeze(1)\n",
    "            \n",
    "        if alpha_t is None: \n",
    "            focal_loss_unreduced = (1 - pt)**self.gamma * bce_loss\n",
    "        else:\n",
    "            focal_loss_unreduced = alpha_t * (1 - pt)**self.gamma * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss_unreduced)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss_unreduced)\n",
    "        else: \n",
    "            return focal_loss_unreduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f4492-75ff-4946-adf0-e9db1cbcc6ad",
   "metadata": {},
   "source": [
    "## 4. Model Definition (TGAT & TemporalGraphAttentionLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be45cea8-e5db-468c-8489-7ddb6581e434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: Model Definition (TGAT class forward method MODIFIED for sliced data)\n",
    "class FunctionalTimeEncoder(nn.Module):\n",
    "    def __init__(self, D_in_emb, D_time_emb, D_out_emb):\n",
    "        super(FunctionalTimeEncoder, self).__init__()\n",
    "        self.time_emb_layer = nn.Linear(1, D_time_emb)\n",
    "        self.output_layer = nn.Linear(D_in_emb + D_time_emb, D_out_emb)\n",
    "\n",
    "    def forward(self, x_feat, delta_t):\n",
    "        if delta_t.ndim == 1: delta_t = delta_t.unsqueeze(-1)\n",
    "        time_embedding_input = torch.tanh(self.time_emb_layer(delta_t.float())) \n",
    "        time_emb = torch.cos(time_embedding_input) \n",
    "        output_concat = torch.cat([x_feat, time_emb], dim=-1)\n",
    "        return self.output_layer(output_concat)\n",
    "\n",
    "class TemporalGraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, n_feat_dim_input, n_time_emb_dim, n_out_dim_layer, n_head=2, dropout=0.1):\n",
    "        super(TemporalGraphAttentionLayer, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_out_dim_head = n_out_dim_layer // n_head\n",
    "        if self.n_out_dim_head == 0: \n",
    "            raise ValueError(f\"Output dimension per head is 0. n_out_dim_layer ({n_out_dim_layer}) must be >= n_head ({n_head}).\")\n",
    "        self.time_encoder = FunctionalTimeEncoder(n_feat_dim_input, n_time_emb_dim, n_feat_dim_input) \n",
    "        self.W_q = nn.Linear(n_feat_dim_input, self.n_out_dim_head * n_head) \n",
    "        self.W_k = nn.Linear(n_feat_dim_input, self.n_out_dim_head * n_head)\n",
    "        self.W_v = nn.Linear(n_feat_dim_input, self.n_out_dim_head * n_head)\n",
    "        self.W_out = nn.Linear(self.n_out_dim_head * n_head, n_out_dim_layer)\n",
    "        self.dropout_m = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(n_out_dim_layer)\n",
    "\n",
    "    def forward(self, target_node_feat_input, target_node_ts, neighbor_feats_input_list, neighbor_ts_list, neighbor_masks):\n",
    "        B, N_max_neighbors, D_feat_input = neighbor_feats_input_list.shape\n",
    "        if target_node_feat_input.shape[-1] != D_feat_input:\n",
    "            raise ValueError(f\"Mismatched feature dimensions for target ({target_node_feat_input.shape[-1]}) and_neighbors ({D_feat_input}) in TemporalGraphAttentionLayer\")\n",
    "\n",
    "        Q = self.W_q(target_node_feat_input).view(B, self.n_head, self.n_out_dim_head)\n",
    "        neighbor_feats_flat = neighbor_feats_input_list.reshape(-1, D_feat_input)\n",
    "        neighbor_ts_flat = neighbor_ts_list.reshape(-1)\n",
    "        target_node_ts_expanded = target_node_ts.unsqueeze(1).expand(-1, N_max_neighbors).reshape(-1)\n",
    "        delta_t_neighbors = target_node_ts_expanded - neighbor_ts_flat\n",
    "        \n",
    "        time_aware_neighbor_feats_flat = self.time_encoder(neighbor_feats_flat, delta_t_neighbors)\n",
    "        K = self.W_k(time_aware_neighbor_feats_flat).view(B, N_max_neighbors, self.n_head, self.n_out_dim_head)\n",
    "        V = self.W_v(time_aware_neighbor_feats_flat).view(B, N_max_neighbors, self.n_head, self.n_out_dim_head)\n",
    "        K_t = K.permute(0, 2, 3, 1) \n",
    "        \n",
    "        attn_scores = torch.matmul(Q.unsqueeze(2), K_t) / np.sqrt(self.n_out_dim_head + 1e-9) \n",
    "        attn_scores = attn_scores.squeeze(2) \n",
    "        attn_scores = torch.clamp(attn_scores, min=-10.0, max=10.0) \n",
    "        attn_scores = attn_scores.masked_fill(~neighbor_masks.unsqueeze(1), float('-inf'))\n",
    "        all_masked = torch.all(attn_scores == float('-inf'), dim=-1, keepdim=True)\n",
    "        attn_scores_safe = torch.where(all_masked, torch.zeros_like(attn_scores), attn_scores)\n",
    "        attn_probs = torch.softmax(attn_scores_safe, dim=-1) \n",
    "        attn_probs = torch.where(all_masked, torch.zeros_like(attn_probs), attn_probs)\n",
    "        attn_probs = self.dropout_m(attn_probs) \n",
    "        output = torch.matmul(attn_probs.unsqueeze(2), V.permute(0, 2, 1, 3)) \n",
    "        output = output.squeeze(2).reshape(B, self.n_head * self.n_out_dim_head)\n",
    "        output = self.W_out(output)\n",
    "        output = self.dropout_m(output)\n",
    "        output = self.layer_norm(output)\n",
    "        return output\n",
    "\n",
    "class TGAT(nn.Module):\n",
    "    def __init__(self, node_feat_dim, time_emb_dim, n_head, n_layers, hidden_dim_per_layer, num_classes, dropout=0.1):\n",
    "        super(TGAT, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.node_feat_dim = node_feat_dim \n",
    "        self.attn_layers = nn.ModuleList()\n",
    "        self.neighbor_feat_projectors = nn.ModuleList()\n",
    "\n",
    "        current_dim_of_h = node_feat_dim\n",
    "        for i in range(n_layers):\n",
    "            self.attn_layers.append(\n",
    "                TemporalGraphAttentionLayer(\n",
    "                    n_feat_dim_input=current_dim_of_h, \n",
    "                    n_time_emb_dim=time_emb_dim,      \n",
    "                    n_out_dim_layer=hidden_dim_per_layer, \n",
    "                    n_head=n_head, \n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "            if i > 0: \n",
    "                self.neighbor_feat_projectors.append(\n",
    "                    nn.Linear(self.node_feat_dim, current_dim_of_h) # Project from original dim\n",
    "                )\n",
    "            else: \n",
    "                self.neighbor_feat_projectors.append(None) \n",
    "            current_dim_of_h = hidden_dim_per_layer\n",
    "\n",
    "        mlp_input_dim = current_dim_of_h\n",
    "        mlp_hidden_dim = mlp_input_dim // 2 if mlp_input_dim // 2 > 0 else 1\n",
    "        if mlp_hidden_dim == 0 : mlp_hidden_dim = 1\n",
    "\n",
    "        self.output_mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_input_dim, mlp_hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, num_classes)\n",
    "        )\n",
    "        self.activation = nn.ReLU() \n",
    "\n",
    "    # MODIFIED: forward now takes batch_node_features and batch_node_timestamps (sliced data)\n",
    "    # target_node_indices and neighbor_node_indices are now batch-local\n",
    "    def forward(self, target_node_indices_local, \n",
    "                batch_node_features, batch_node_timestamps, \n",
    "                neighbor_info_batches_all_layers_local, return_embedding=False):\n",
    "        \n",
    "        h = batch_node_features[target_node_indices_local] \n",
    "        target_ts_for_attn = batch_node_timestamps[target_node_indices_local]\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            layer_input_h_target = h \n",
    "            if i >= len(neighbor_info_batches_all_layers_local):\n",
    "                print(f\"Warning: Not enough neighbor_info_batches for layer {i}.\")\n",
    "                break \n",
    "            \n",
    "            # These are batch-local indices now\n",
    "            neighbor_node_indices_padded_local, neighbor_ts_padded, neighbor_masks = neighbor_info_batches_all_layers_local[i]\n",
    "            \n",
    "            # Fetch neighbor features using batch-local indices from batch_node_features\n",
    "            # Important: The dimension of batch_node_features is self.node_feat_dim (original)\n",
    "            original_neighbor_features = batch_node_features[neighbor_node_indices_padded_local.reshape(-1)].reshape(\n",
    "                neighbor_node_indices_padded_local.shape[0], \n",
    "                neighbor_node_indices_padded_local.shape[1], \n",
    "                self.node_feat_dim # Neighbors are always fetched with original feature dim\n",
    "            )\n",
    "            \n",
    "            projector = self.neighbor_feat_projectors[i]\n",
    "            if projector is not None:\n",
    "                # Project original neighbor features to match current_dim_of_h (which is layer_input_h_target.shape[-1])\n",
    "                B_Nmax_shape = original_neighbor_features.shape[:2]\n",
    "                flat_original_neighbor_features = original_neighbor_features.reshape(-1, self.node_feat_dim)\n",
    "                projected_flat_neighbor_features = projector(flat_original_neighbor_features)\n",
    "                input_neighbor_features_for_attn = projected_flat_neighbor_features.reshape(*B_Nmax_shape, -1)\n",
    "            else: \n",
    "                # First layer: layer_input_h_target is original_node_feat_dim, so original_neighbor_features match\n",
    "                input_neighbor_features_for_attn = original_neighbor_features\n",
    "            \n",
    "            input_neighbor_features_for_attn[~neighbor_masks] = 0 \n",
    "\n",
    "            h = self.attn_layers[i](\n",
    "                layer_input_h_target, \n",
    "                target_ts_for_attn, # These are absolute timestamps, but fetched for batch nodes\n",
    "                input_neighbor_features_for_attn, \n",
    "                neighbor_ts_padded, # These are absolute timestamps for neighbors\n",
    "                neighbor_masks\n",
    "            )\n",
    "            h = self.activation(h) \n",
    "        \n",
    "        output_logits = self.output_mlp(h)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return output_logits, h \n",
    "        else:\n",
    "            return output_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15103a6-15ef-437a-ba84-705d2d960555",
   "metadata": {},
   "source": [
    "## 5. Custom TemporalNeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8936df-f7a4-4c4d-92a8-742d3b72819c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: TemporalNeighborLoader Class (MODIFIED for Smarter Neighbor Sampling)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm # Ensure tqdm is imported if used within the class\n",
    "\n",
    "class TemporalNeighborLoader:\n",
    "    def __init__(self, temporal_data_cpu, batch_size, num_neighbors_per_layer_list, device,\n",
    "                 shuffle=True, \n",
    "                 recency_bias_factor=0.8, \n",
    "                 feature_similarity_col_name='service', \n",
    "                 feature_similarity_weight=0.5, \n",
    "                 raw_data_file_path_for_ids=None, # Default is None\n",
    "                 col_names_list=None # Parameter to pass COL_NAMES for reading raw file\n",
    "                ):\n",
    "        self.temporal_data_cpu = temporal_data_cpu\n",
    "        self.x_cpu = temporal_data_cpu.x\n",
    "        self.ts_cpu = temporal_data_cpu.ts\n",
    "        self.y_cpu = temporal_data_cpu.y\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neighbors_per_layer_list = num_neighbors_per_layer_list\n",
    "        self.shuffle = shuffle\n",
    "        self.device = device\n",
    "        self.N = temporal_data_cpu.num_nodes if temporal_data_cpu.num_nodes is not None else 0\n",
    "        self.node_indices_global = torch.arange(self.N) if self.N > 0 else torch.empty(0, dtype=torch.long)\n",
    "\n",
    "        self.adj = [[] for _ in range(self.N)]\n",
    "        if self.N > 0 and hasattr(temporal_data_cpu, 'edge_index') and temporal_data_cpu.edge_index is not None:\n",
    "            edge_index_cpu = temporal_data_cpu.edge_index.cpu() \n",
    "            src, dst = edge_index_cpu\n",
    "            \n",
    "            for i in range(len(src)):\n",
    "                s, d = src[i].item(), dst[i].item()\n",
    "                if s < self.N and d < self.N and self.ts_cpu[s] < self.ts_cpu[d]: # Bounds check\n",
    "                    self.adj[d].append(s)\n",
    "            \n",
    "            for i in range(self.N):\n",
    "                self.adj[i].sort(key=lambda pred_idx: self.ts_cpu[pred_idx], reverse=True)\n",
    "\n",
    "        self.recency_bias_factor = recency_bias_factor\n",
    "        self.feature_similarity_col_name = feature_similarity_col_name\n",
    "        self.feature_similarity_weight = feature_similarity_weight\n",
    "        self.raw_data_file_path_for_ids = raw_data_file_path_for_ids \n",
    "        self.original_feature_for_similarity_cpu = None\n",
    "        self._col_names_for_raw_read = col_names_list \n",
    "\n",
    "        if self.feature_similarity_col_name and self.raw_data_file_path_for_ids:\n",
    "            if not os.path.exists(self.raw_data_file_path_for_ids):\n",
    "                print(f\"Warning: 'raw_data_file_path_for_ids' ('{self.raw_data_file_path_for_ids}') provided but file does not exist. Disabling feature similarity sampling.\")\n",
    "                self.original_feature_for_similarity_cpu = None\n",
    "            elif self._col_names_for_raw_read is None or not isinstance(self._col_names_for_raw_read, list) or len(self._col_names_for_raw_read) == 0:\n",
    "                print(f\"Warning: 'col_names_list' not provided or invalid to TemporalNeighborLoader. Disabling feature similarity sampling for '{self.feature_similarity_col_name}'.\")\n",
    "                self.original_feature_for_similarity_cpu = None\n",
    "            elif self.feature_similarity_col_name not in self._col_names_for_raw_read:\n",
    "                print(f\"Warning: feature_similarity_col_name '{self.feature_similarity_col_name}' not found in provided col_names_list. Disabling feature similarity sampling.\")\n",
    "                self.original_feature_for_similarity_cpu = None\n",
    "            else:\n",
    "                try:\n",
    "                    print(f\"Loading '{self.feature_similarity_col_name}' from {self.raw_data_file_path_for_ids} for similarity sampling...\")\n",
    "                    df_ids = pd.read_csv(self.raw_data_file_path_for_ids, header=None, names=self._col_names_for_raw_read, usecols=[self.feature_similarity_col_name], low_memory=False)\n",
    "                    self.original_feature_for_similarity_cpu = df_ids[self.feature_similarity_col_name].values\n",
    "                    print(f\"Successfully loaded '{self.feature_similarity_col_name}' for {len(self.original_feature_for_similarity_cpu)} nodes.\")\n",
    "                    if self.N > 0 and len(self.original_feature_for_similarity_cpu) != self.N:\n",
    "                        print(f\"Warning: Length mismatch for similarity feature. Expected {self.N}, got {len(self.original_feature_for_similarity_cpu)}. Disabling feature similarity.\")\n",
    "                        self.original_feature_for_similarity_cpu = None\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load feature '{self.feature_similarity_col_name}' for similarity sampling from '{self.raw_data_file_path_for_ids}': {e}. Disabling.\")\n",
    "                    self.original_feature_for_similarity_cpu = None\n",
    "        elif self.feature_similarity_col_name: \n",
    "             print(f\"Warning: 'feature_similarity_col_name' ('{self.feature_similarity_col_name}') provided, but 'raw_data_file_path_for_ids' is None or empty. Disabling feature similarity sampling.\")\n",
    "             self.original_feature_for_similarity_cpu = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.N == 0 : \n",
    "            self.node_indices_permuted_global = torch.empty(0, dtype=torch.long)\n",
    "        elif self.shuffle:\n",
    "            self.node_indices_permuted_global = self.node_indices_global[torch.randperm(self.N)]\n",
    "        else:\n",
    "            self.node_indices_permuted_global = self.node_indices_global\n",
    "        self.current_idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_idx >= self.N or self.N == 0: \n",
    "            raise StopIteration\n",
    "        \n",
    "        end_idx = min(self.current_idx + self.batch_size, self.N)\n",
    "        target_node_indices_batch_global_cpu = self.node_indices_permuted_global[self.current_idx:end_idx]\n",
    "        self.current_idx = end_idx\n",
    "        \n",
    "        unique_global_indices_for_batch_set = set(target_node_indices_batch_global_cpu.tolist())\n",
    "        neighbor_info_for_model_layers_global_cpu = []\n",
    "        \n",
    "        for k_neighbors_this_layer in self.num_neighbors_per_layer_list:\n",
    "            batch_neigh_idx_padded_global_cpu, batch_neigh_ts_padded_cpu, batch_neigh_masks_cpu = [], [], []\n",
    "            for node_idx_val_global in target_node_indices_batch_global_cpu.tolist():\n",
    "                if node_idx_val_global >= self.N: \n",
    "                    preds_global = []\n",
    "                else:\n",
    "                    preds_global = self.adj[node_idx_val_global]\n",
    "                actual_k_candidates = len(preds_global)\n",
    "                \n",
    "                sampled_pred_indices_global_np = np.array([], dtype=np.int64)\n",
    "                if actual_k_candidates > 0:\n",
    "                    weights = np.ones(actual_k_candidates, dtype=float) \n",
    "                    \n",
    "                    if self.recency_bias_factor > 0 and self.recency_bias_factor < 1 and actual_k_candidates > 1:\n",
    "                        recency_weights = np.array([self.recency_bias_factor**i for i in range(actual_k_candidates)], dtype=float)\n",
    "                        weights *= recency_weights\n",
    "                    \n",
    "                    if self.original_feature_for_similarity_cpu is not None and \\\n",
    "                       self.feature_similarity_weight > 0 and \\\n",
    "                       node_idx_val_global < len(self.original_feature_for_similarity_cpu): \n",
    "                        \n",
    "                        target_feature_value = self.original_feature_for_similarity_cpu[node_idx_val_global]\n",
    "                        valid_preds_for_sim_indices = [p for p in preds_global if p < len(self.original_feature_for_similarity_cpu)]\n",
    "                        \n",
    "                        if valid_preds_for_sim_indices:\n",
    "                            pred_to_valid_idx_map = {pred_val: i for i, pred_val in enumerate(valid_preds_for_sim_indices)}\n",
    "                            neighbor_feature_values = self.original_feature_for_similarity_cpu[valid_preds_for_sim_indices]\n",
    "                            similarity_scores_for_valid_preds = np.array([1.0 if nf == target_feature_value else (1.0 - self.feature_similarity_weight) for nf in neighbor_feature_values], dtype=float)\n",
    "                            \n",
    "                            for i, pred_original_idx in enumerate(preds_global):\n",
    "                                if pred_original_idx in pred_to_valid_idx_map:\n",
    "                                    weights[i] *= similarity_scores_for_valid_preds[pred_to_valid_idx_map[pred_original_idx]]\n",
    "\n",
    "                    sum_weights = np.sum(weights)\n",
    "                    p_dist = None\n",
    "                    if sum_weights > 1e-9: \n",
    "                        p_dist = weights / sum_weights\n",
    "                    elif actual_k_candidates > 0 : \n",
    "                        pass # p=None in np.random.choice means uniform\n",
    "\n",
    "                    actual_k_to_sample = min(actual_k_candidates, k_neighbors_this_layer)\n",
    "                    \n",
    "                    try:\n",
    "                        if actual_k_candidates > 0:\n",
    "                            sampled_pred_indices_global_np = np.random.choice(\n",
    "                                preds_global, size=actual_k_to_sample, replace=False, p=p_dist\n",
    "                            )\n",
    "                    except ValueError as e_choice: \n",
    "                        if actual_k_candidates > 0: # Ensure preds_global is not empty before trying uniform sampling\n",
    "                             # print(f\"Warning: np.random.choice ValueError ({e_choice}). Sum_w: {np.sum(p_dist) if p_dist is not None else 'None (uniform)'}. N_cand: {actual_k_candidates}, k_sample: {actual_k_to_sample}. Uniform sampling for node {node_idx_val_global}.\")\n",
    "                             sampled_pred_indices_global_np = np.random.choice(\n",
    "                                preds_global, size=actual_k_to_sample, replace=False\n",
    "                            )\n",
    "                actual_k = len(sampled_pred_indices_global_np)\n",
    "                sampled_pred_indices_global_torch = torch.from_numpy(sampled_pred_indices_global_np).long()\n",
    "                \n",
    "                sampled_pred_ts_cpu = torch.empty(0, dtype=torch.long)\n",
    "                if actual_k > 0:\n",
    "                    # Ensure sampled indices are valid before fetching from ts_cpu\n",
    "                    valid_ts_indices = sampled_pred_indices_global_torch[(sampled_pred_indices_global_torch >= 0) & (sampled_pred_indices_global_torch < self.N)]\n",
    "                    if len(valid_ts_indices) > 0: \n",
    "                        sampled_pred_ts_cpu = self.ts_cpu[valid_ts_indices]\n",
    "                    \n",
    "                    # Pad if some indices became invalid (should be rare if preds_global are valid)\n",
    "                    if len(valid_ts_indices) != actual_k:\n",
    "                         # print(f\"Warning: Timestamp fetch mismatch for node {node_idx_val_global}. Expected {actual_k}, got {len(valid_ts_indices)}. Padded with zeros.\")\n",
    "                         sampled_pred_ts_cpu = torch.cat([sampled_pred_ts_cpu, torch.zeros(actual_k - len(valid_ts_indices), dtype=torch.long)])\n",
    "\n",
    "                unique_global_indices_for_batch_set.update(sampled_pred_indices_global_torch.tolist())\n",
    "                \n",
    "                padding_needed = k_neighbors_this_layer - actual_k\n",
    "                mask_cpu = torch.ones(actual_k, dtype=torch.bool)\n",
    "                \n",
    "                if padding_needed > 0:\n",
    "                    pad_idx_val = 0 \n",
    "                    if self.N > 0 and pad_idx_val not in unique_global_indices_for_batch_set: \n",
    "                         unique_global_indices_for_batch_set.add(pad_idx_val)\n",
    "                    \n",
    "                    sampled_pred_indices_global_torch = torch.cat([sampled_pred_indices_global_torch, torch.full((padding_needed,), pad_idx_val, dtype=torch.long)])\n",
    "                    pad_ts_val = self.ts_cpu[pad_idx_val].item() if self.N > 0 and pad_idx_val < self.N else 0\n",
    "                    sampled_pred_ts_cpu = torch.cat([sampled_pred_ts_cpu, torch.full((padding_needed,), pad_ts_val, dtype=torch.long)])\n",
    "                    mask_cpu = torch.cat([mask_cpu, torch.zeros(padding_needed, dtype=torch.bool)])\n",
    "                \n",
    "                batch_neigh_idx_padded_global_cpu.append(sampled_pred_indices_global_torch)\n",
    "                batch_neigh_ts_padded_cpu.append(sampled_pred_ts_cpu)\n",
    "                batch_neigh_masks_cpu.append(mask_cpu)\n",
    "            \n",
    "            if not batch_neigh_idx_padded_global_cpu: # If target_node_indices_batch_global_cpu was empty or led to no neighbors\n",
    "                 dummy_shape_neighbors = (0, k_neighbors_this_layer)\n",
    "                 neighbor_info_for_model_layers_global_cpu.append((\n",
    "                    torch.empty(dummy_shape_neighbors, dtype=torch.long),\n",
    "                    torch.empty(dummy_shape_neighbors, dtype=torch.long),\n",
    "                    torch.empty(dummy_shape_neighbors, dtype=torch.bool)))\n",
    "            else:\n",
    "                neighbor_info_for_model_layers_global_cpu.append((\n",
    "                    torch.stack(batch_neigh_idx_padded_global_cpu),\n",
    "                    torch.stack(batch_neigh_ts_padded_cpu),\n",
    "                    torch.stack(batch_neigh_masks_cpu)))\n",
    "\n",
    "        unique_global_indices_list_cpu = sorted(list(unique_global_indices_for_batch_set))\n",
    "        if not unique_global_indices_list_cpu : # If, after all processing, the set is empty\n",
    "            if self.N > 0 : # If dataset has nodes, but this batch yielded no unique indices (e.g. only padded 0s and 0 was already there)\n",
    "                unique_global_indices_list_cpu = [0] # Add 0 to fetch its features at least\n",
    "                # print(\"Warning: unique_global_indices_list_cpu was empty after processing neighbors, defaulting to [0].\")\n",
    "            else: # Dataset itself is empty\n",
    "                 # print(\"Error: Dataset has no nodes (self.N=0). Returning empty batch from loader.\")\n",
    "                 x_dim = self.x_cpu.shape[1] if self.x_cpu.ndim > 1 and self.x_cpu.shape[0] > 0 else 1\n",
    "                 y_shape_rest = self.y_cpu.shape[1:] if self.y_cpu.ndim > 1 and self.y_cpu.shape[0] > 0 else ()\n",
    "                 return (torch.empty(0,dtype=torch.long).to(self.device), \n",
    "                         torch.empty(0, x_dim).to(self.device), \n",
    "                         torch.empty(0,dtype=torch.long).to(self.device), [], \n",
    "                         torch.empty(0, *y_shape_rest, dtype=self.y_cpu.dtype).to(self.device))\n",
    "\n",
    "\n",
    "        global_to_batch_local_idx_map = {global_idx: local_idx for local_idx, global_idx in enumerate(unique_global_indices_list_cpu)}\n",
    "        \n",
    "        valid_fetch_indices_tensor = torch.tensor(unique_global_indices_list_cpu, dtype=torch.long)\n",
    "        # Filter out-of-bounds indices, though ideally, unique_global_indices_list_cpu should only contain valid global indices < self.N\n",
    "        # (or index 0 if used for padding and N > 0)\n",
    "        valid_fetch_indices_tensor = valid_fetch_indices_tensor[valid_fetch_indices_tensor < self.N] \n",
    "        \n",
    "        if valid_fetch_indices_tensor.numel() == 0 : # If no valid indices after filtering (e.g., unique_global_indices_list_cpu was empty or only contained invalid indices)\n",
    "             if self.N > 0: # If dataset has nodes, but no valid indices were collected\n",
    "                # print(\"Warning: No valid indices to fetch features/timestamps after filtering. Using node 0 if N > 0.\")\n",
    "                valid_fetch_indices_tensor = torch.tensor([0], dtype=torch.long) # Default to fetching node 0\n",
    "                if 0 not in global_to_batch_local_idx_map: # Ensure mapping exists for node 0 if we defaulted to it\n",
    "                    global_to_batch_local_idx_map[0] = len(global_to_batch_local_idx_map) \n",
    "            \n",
    "        if valid_fetch_indices_tensor.numel() == 0: # Still no valid indices (e.g. self.N=0 or above fallback failed)\n",
    "             x_dim = self.x_cpu.shape[1] if self.x_cpu.ndim > 1 and self.x_cpu.shape[0] > 0 else 1\n",
    "             y_shape_rest = self.y_cpu.shape[1:] if self.y_cpu.ndim > 1 and self.y_cpu.shape[0] > 0 else ()\n",
    "             return (torch.empty(0,dtype=torch.long).to(self.device), \n",
    "                     torch.empty(0, x_dim).to(self.device), \n",
    "                     torch.empty(0,dtype=torch.long).to(self.device), [], \n",
    "                     torch.empty(0, *y_shape_rest, dtype=self.y_cpu.dtype).to(self.device))\n",
    "\n",
    "        batch_node_features_dev = self.x_cpu[valid_fetch_indices_tensor].to(self.device)\n",
    "        batch_node_timestamps_dev = self.ts_cpu[valid_fetch_indices_tensor].to(self.device)\n",
    "\n",
    "        target_node_indices_batch_local_dev_list = []\n",
    "        for global_idx_tensor in target_node_indices_batch_global_cpu:\n",
    "            global_idx = global_idx_tensor.item()\n",
    "            local_idx = global_to_batch_local_idx_map.get(global_idx)\n",
    "            if local_idx is None: # Should not happen if unique_global_indices_for_batch_set was built from targets\n",
    "                # print(f\"Critical Warning: Target node global index {global_idx} not in local map! Using local index of global 0 as fallback.\")\n",
    "                local_idx = global_to_batch_local_idx_map.get(0,0) # Default to local index of global 0 if something went wrong\n",
    "            target_node_indices_batch_local_dev_list.append(local_idx)\n",
    "        \n",
    "        target_node_indices_batch_local_dev = torch.tensor(\n",
    "            target_node_indices_batch_local_dev_list, dtype=torch.long\n",
    "        ).to(self.device)\n",
    "\n",
    "        neighbor_info_for_model_layers_local_dev = []\n",
    "        for global_indices_layer, global_ts_layer, masks_layer in neighbor_info_for_model_layers_global_cpu:\n",
    "            if global_indices_layer.numel() > 0: \n",
    "                remapped_indices_list = []\n",
    "                for row_idx in range(global_indices_layer.shape[0]):\n",
    "                    row = global_indices_layer[row_idx]\n",
    "                    remapped_row = [global_to_batch_local_idx_map.get(x.item(), global_to_batch_local_idx_map.get(0,0)) for x in row]\n",
    "                    remapped_indices_list.append(remapped_row)\n",
    "                \n",
    "                if not remapped_indices_list: \n",
    "                     batch_local_indices_layer = torch.empty_like(global_indices_layer) \n",
    "                else:\n",
    "                     batch_local_indices_layer = torch.tensor(remapped_indices_list, dtype=torch.long)\n",
    "\n",
    "                neighbor_info_for_model_layers_local_dev.append((\n",
    "                    batch_local_indices_layer.to(self.device),\n",
    "                    global_ts_layer.to(self.device),\n",
    "                    masks_layer.to(self.device)\n",
    "                ))\n",
    "            else: \n",
    "                 k_this_layer = global_indices_layer.shape[1] if global_indices_layer.ndim ==2 and global_indices_layer.shape[1] > 0 else (self.num_neighbors_per_layer_list[0] if self.num_neighbors_per_layer_list and len(self.num_neighbors_per_layer_list)>0 else 0)\n",
    "                 dummy_shape_layer_neighbors = (len(target_node_indices_batch_global_cpu), k_this_layer) # Ensure batch dim matches target nodes\n",
    "                 if len(target_node_indices_batch_global_cpu) == 0: dummy_shape_layer_neighbors = (0, k_this_layer)\n",
    "\n",
    "                 neighbor_info_for_model_layers_local_dev.append((\n",
    "                    torch.empty(dummy_shape_layer_neighbors, dtype=torch.long).to(self.device),\n",
    "                    torch.empty(dummy_shape_layer_neighbors, dtype=torch.long).to(self.device),\n",
    "                    torch.empty(dummy_shape_layer_neighbors, dtype=torch.bool).to(self.device)\n",
    "                 ))\n",
    "        \n",
    "        if target_node_indices_batch_global_cpu.numel() > 0:\n",
    "             # Ensure indices are valid for y_cpu before fetching\n",
    "             valid_y_indices = target_node_indices_batch_global_cpu[target_node_indices_batch_global_cpu < self.N]\n",
    "             if valid_y_indices.numel() > 0:\n",
    "                 batch_labels_dev = self.y_cpu[valid_y_indices].to(self.device)\n",
    "                 if len(batch_labels_dev) != len(target_node_indices_batch_local_dev): # If filtering changed size\n",
    "                     print(f\"Warning: Label batch size mismatch after filtering valid y_indices. This might cause issues.\")\n",
    "                     # Fallback: create dummy labels matching target_node_indices_batch_local_dev size\n",
    "                     y_shape_rest = self.y_cpu.shape[1:] if self.y_cpu.ndim > 1 and self.y_cpu.shape[0] > 0 else ()\n",
    "                     batch_labels_dev = torch.empty(len(target_node_indices_batch_local_dev), *y_shape_rest, dtype=self.y_cpu.dtype).to(self.device)\n",
    "\n",
    "             else: # No valid indices left for y_cpu\n",
    "                y_shape_rest = self.y_cpu.shape[1:] if self.y_cpu.ndim > 1 and self.y_cpu.shape[0] > 0 else ()\n",
    "                batch_labels_dev = torch.empty(0, *y_shape_rest, dtype=self.y_cpu.dtype).to(self.device)\n",
    "\n",
    "        else: # target_node_indices_batch_global_cpu was empty\n",
    "             y_shape_rest = self.y_cpu.shape[1:] if self.y_cpu.ndim > 1 and self.y_cpu.shape[0] > 0 else ()\n",
    "             batch_labels_dev = torch.empty(0, *y_shape_rest, dtype=self.y_cpu.dtype).to(self.device)\n",
    "\n",
    "        return (target_node_indices_batch_local_dev,\n",
    "                batch_node_features_dev,\n",
    "                batch_node_timestamps_dev,\n",
    "                neighbor_info_for_model_layers_local_dev,\n",
    "                batch_labels_dev)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.N == 0: return 0\n",
    "        return (self.N + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292f589-fce4-4496-a455-a3620769bf62",
   "metadata": {},
   "source": [
    "## 6. Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbbae281-216c-46ee-a595-681fbe6d6065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: load_processed_data function\n",
    "def load_processed_data(data_file_path: str, metadata_file_path: str, classification_mode: str = 'binary'):\n",
    "    print(f\"Loading data from: {data_file_path}\")\n",
    "    data_dict = torch.load(data_file_path, map_location='cpu') \n",
    "    \n",
    "    print(f\"Loading metadata from: {metadata_file_path}\")\n",
    "    with open(metadata_file_path, 'r') as f: metadata = json.load(f)\n",
    "\n",
    "    y_labels = data_dict['y_binary'] if classification_mode == 'binary' else data_dict['y_multiclass'].squeeze() # Ensure y_multiclass is 1D\n",
    "    num_classes = metadata['NUM_CLASSES_BINARY'] if classification_mode == 'binary' else metadata['NUM_CLASSES_MULTI']\n",
    "    \n",
    "    pos_weight_tensor = None\n",
    "    if classification_mode == 'binary':\n",
    "        pos_weight_val = metadata.get('POS_WEIGHT_BINARY', 1.0)\n",
    "        pos_weight_tensor = torch.tensor([pos_weight_val], dtype=torch.float32) \n",
    "\n",
    "    temporal_data_obj = TemporalData(x=data_dict['x'], edge_index=data_dict['edge_index'], \n",
    "                                     ts=data_dict['ts'], y=y_labels)\n",
    "    \n",
    "    print(f\"Data loaded: Nodes={temporal_data_obj.num_nodes}, Edges={temporal_data_obj.num_edges}\")\n",
    "    print(f\"Metadata: NodeFeatDim={metadata['NODE_FEAT_DIM']}, NumClasses({classification_mode})={num_classes}, PosWeight={pos_weight_tensor.item() if pos_weight_tensor is not None and classification_mode == 'binary' else 'N/A_or_Multiclass'}\")\n",
    "    return temporal_data_obj, metadata, num_classes, pos_weight_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549ab9b-0cd9-4513-9fac-2498fb7b1299",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac9b2775-77b8-489a-8486-26dfcfc5d3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: train_epoch and evaluate_model functions (MODIFIED for new loader output)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, clip_grad_val=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds_list, all_true_list = [], []\n",
    "    valid_batches_for_loss = 0\n",
    "    pbar = tqdm(loader, desc=\"Train Epoch\")\n",
    "    # MODIFIED: Unpack new items from loader\n",
    "    for batch_idx, (target_indices_local_dev, batch_features_dev, batch_ts_dev, \n",
    "                     neighbor_batches_local_dev, batch_labels_dev) in enumerate(pbar):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            if len(neighbor_batches_local_dev) < model.n_layers:\n",
    "                print(f\"Warning: Batch {batch_idx} has insufficient neighbor_info_batches for N_LAYERS. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # MODIFIED: Pass sliced data to model\n",
    "            output_logits = model(target_indices_local_dev, batch_features_dev, batch_ts_dev, neighbor_batches_local_dev)\n",
    "            \n",
    "            if output_logits.isnan().any() or output_logits.isinf().any():\n",
    "                print(f\"Batch {batch_idx}: NaNs/Infs in model output_logits! Skipping.\")\n",
    "                continue \n",
    "            \n",
    "            # batch_labels_dev are already on device and correspond to target_indices_local_dev\n",
    "            true_labels_dev = batch_labels_dev \n",
    "            if CLASSIFICATION_MODE == 'binary' and true_labels_dev.ndim == 1:\n",
    "                true_labels_dev = true_labels_dev.unsqueeze(1)\n",
    "            elif CLASSIFICATION_MODE == 'multiclass' and true_labels_dev.ndim > 1 and true_labels_dev.size(1) == 1:\n",
    "                true_labels_dev = true_labels_dev.squeeze(1).long()\n",
    "            \n",
    "            loss = criterion(output_logits, true_labels_dev.float() if CLASSIFICATION_MODE == 'binary' else true_labels_dev)\n",
    "            \n",
    "            if loss.isnan() or loss.isinf():\n",
    "                print(f\"Batch {batch_idx}: Loss is NaN or Inf! Skipping backward. Logits: {output_logits.flatten()[:3]}, Labels: {true_labels_dev.flatten()[:3]}\")\n",
    "                continue \n",
    "            \n",
    "            loss.backward()\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (param.grad.isnan().any() or param.grad.isinf().any()):\n",
    "                    print(f\"Batch {batch_idx}: NaNs/Infs in gradients of {name}! Zeroing grad.\")\n",
    "                    param.grad = torch.zeros_like(param.grad) \n",
    "            if clip_grad_val:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_val)\n",
    "            optimizer.step()\n",
    "            \n",
    "            for name, param in model.named_parameters():\n",
    "                if param.isnan().any() or param.isinf().any():\n",
    "                    print(f\"CRITICAL: Batch {batch_idx}: NaNs/Infs in param {name} AFTER step!\")\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            valid_batches_for_loss += 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                preds = (torch.sigmoid(output_logits.detach()) > 0.5).cpu().numpy() if CLASSIFICATION_MODE == 'binary' else torch.argmax(output_logits.detach(), dim=1).cpu().numpy()\n",
    "            all_preds_list.extend(preds.flatten().tolist())\n",
    "            all_true_list.extend(true_labels_dev.cpu().numpy().flatten().tolist()) # Use true_labels_dev which is already processed\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        except RuntimeError as e:\n",
    "            if \"NaN\" in str(e) or \"Inf\" in str(e) or \"nan\" in str(e):\n",
    "                print(f\"RuntimeError involving NaN/Inf in train batch {batch_idx}: {e}. Skipping.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Unexpected RuntimeError in train batch {batch_idx}: {e}\")\n",
    "                raise e\n",
    "                \n",
    "    if valid_batches_for_loss == 0: return float('nan'),0,0,0,0,None\n",
    "    avg_loss = total_loss / valid_batches_for_loss\n",
    "    if not all_true_list or not all_preds_list or len(all_true_list) != len(all_preds_list): return avg_loss,0,0,0,0,None\n",
    "    \n",
    "    accuracy = accuracy_score(all_true_list, all_preds_list)\n",
    "    avg_mode = 'binary' if CLASSIFICATION_MODE == 'binary' else 'weighted'\n",
    "    pos_label_val = 1 if CLASSIFICATION_MODE == 'binary' else None\n",
    "    if CLASSIFICATION_MODE == 'binary':\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_true_list, all_preds_list, average=avg_mode, pos_label=pos_label_val, zero_division=0)\n",
    "    else:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_true_list, all_preds_list, average=avg_mode, zero_division=0)\n",
    "    \n",
    "    auc = None\n",
    "    if CLASSIFICATION_MODE == 'binary' and len(np.unique(all_true_list)) >= 2:\n",
    "        # For AUC with 0/1 preds, it's equivalent to accuracy if preds are hard labels.\n",
    "        # If output_logits were probabilities, this would be more meaningful.\n",
    "        # The current `all_preds_list` are hard 0/1 predictions.\n",
    "        try:\n",
    "            auc = roc_auc_score(all_true_list, all_preds_list) \n",
    "        except ValueError as e:\n",
    "            print(f\"Could not compute train AUC (0/1 preds): {e}. True unique: {np.unique(all_true_list)}\")\n",
    "            \n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, phase='Validation', return_embeddings_and_ids=False, entity_id_col_name=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds_list, all_true_list, all_probs_binary_list = [], [], []\n",
    "    \n",
    "    all_node_embeddings_list = []\n",
    "    all_target_node_indices_global_list = [] # Store GLOBAL indices if needed for post-hoc\n",
    "    all_entity_ids_list = []\n",
    "\n",
    "    valid_batches_for_loss = 0\n",
    "    class_report_dict = None\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"{phase} Phase\")\n",
    "    with torch.no_grad():\n",
    "        # MODIFIED: Unpack new items from loader\n",
    "        for batch_idx, (target_indices_local_dev, batch_features_dev, batch_ts_dev, \n",
    "                         neighbor_batches_local_dev, batch_labels_dev) in enumerate(pbar):\n",
    "            try:\n",
    "                if len(neighbor_batches_local_dev) < model.n_layers:\n",
    "                    print(f\"Warning: Batch {batch_idx} in {phase} has insufficient neighbor_info_batches. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                if return_embeddings_and_ids:\n",
    "                    # MODIFIED: Pass sliced data to model\n",
    "                    output_logits, node_embeddings = model(\n",
    "                        target_indices_local_dev, batch_features_dev, batch_ts_dev, \n",
    "                        neighbor_batches_local_dev, return_embedding=True\n",
    "                    )\n",
    "                    all_node_embeddings_list.append(node_embeddings.cpu())\n",
    "                    # To get global indices for post-hoc, we need the loader to also provide the original global indices for the batch\n",
    "                    # The current modified loader does not explicitly return target_node_indices_batch_global_cpu in the tuple.\n",
    "                    # For now, if we need global indices, this part needs adjustment in loader output.\n",
    "                    # Assuming target_indices_local_dev can be mapped back if necessary, or loader provides mapping.\n",
    "                    # For simplicity, let's say we store local indices for now, or acknowledge this limitation for post-hoc.\n",
    "                    # If loader.current_idx and batch_size are used, we can reconstruct global indices if not shuffled.\n",
    "                    # Placeholder:\n",
    "                    # all_target_node_indices_global_list.append(target_indices_local_dev.cpu()) # This is NOT global yet\n",
    "                    \n",
    "                else:\n",
    "                    output_logits = model(\n",
    "                        target_indices_local_dev, batch_features_dev, batch_ts_dev, \n",
    "                        neighbor_batches_local_dev, return_embedding=False\n",
    "                    )\n",
    "\n",
    "                if output_logits.isnan().any() or output_logits.isinf().any():\n",
    "                    print(f\"Batch {batch_idx} in {phase}: NaNs or Infs in model output_logits! Skipping metrics for this batch.\")\n",
    "                    continue\n",
    "                \n",
    "                true_labels_dev = batch_labels_dev # Already on device\n",
    "                if CLASSIFICATION_MODE == 'binary' and true_labels_dev.ndim == 1:\n",
    "                    true_labels_dev = true_labels_dev.unsqueeze(1)\n",
    "                elif CLASSIFICATION_MODE == 'multiclass' and true_labels_dev.ndim > 1 and true_labels_dev.size(1) == 1:\n",
    "                    true_labels_dev = true_labels_dev.squeeze(1).long()\n",
    "                \n",
    "                loss = criterion(output_logits, true_labels_dev.float() if CLASSIFICATION_MODE == 'binary' else true_labels_dev)\n",
    "                if not (loss.isnan() or loss.isinf()):\n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches_for_loss +=1\n",
    "                else:\n",
    "                    print(f\"Batch {batch_idx} in {phase}: Loss is NaN or Inf! Logits: {output_logits.flatten()[:5]}\")\n",
    "                \n",
    "                current_batch_true_labels = true_labels_dev.cpu().numpy().flatten().tolist()\n",
    "                current_batch_preds_np, current_batch_probs_np = np.array([]), np.array([])\n",
    "                \n",
    "                if CLASSIFICATION_MODE == 'binary':\n",
    "                    probs = torch.sigmoid(output_logits)\n",
    "                    if probs.isnan().any() or probs.isinf().any():\n",
    "                        print(f\"Batch {batch_idx} in {phase}: Sigmoid probs NaN/Inf.\")\n",
    "                        current_batch_preds_np = np.zeros(len(current_batch_true_labels), dtype=int) \n",
    "                        current_batch_probs_np = np.full(len(current_batch_true_labels), 0.5, dtype=float)\n",
    "                    else:\n",
    "                        current_batch_probs_np = probs.cpu().numpy().flatten()\n",
    "                        current_batch_preds_np = (probs > 0.5).cpu().numpy().flatten()\n",
    "                    all_probs_binary_list.extend(current_batch_probs_np.tolist())\n",
    "                else:\n",
    "                    current_batch_preds_np = torch.argmax(output_logits, dim=1).cpu().numpy().flatten()\n",
    "                \n",
    "                all_preds_list.extend(current_batch_preds_np.tolist())\n",
    "                all_true_list.extend(current_batch_true_labels) \n",
    "                pbar.set_postfix({'loss': loss.item() if not (loss.isnan() or loss.isinf()) else float('nan')})\n",
    "            except RuntimeError as e:\n",
    "                if \"NaN\" in str(e) or \"Inf\" in str(e) or \"nan\" in str(e):\n",
    "                    print(f\"RuntimeError involving NaN/Inf in {phase} batch {batch_idx}: {e}. Skipping.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Unexpected RuntimeError in {phase} batch {batch_idx}: {e}\")\n",
    "                    raise e\n",
    "\n",
    "    if valid_batches_for_loss == 0: \n",
    "        empty_return = (float('nan'),0,0,0,0, None, [], [], None)\n",
    "        if return_embeddings_and_ids: empty_return += (torch.empty(0), torch.empty(0), [])\n",
    "        return empty_return\n",
    "\n",
    "    avg_loss = total_loss / valid_batches_for_loss\n",
    "    if not all_true_list or not all_preds_list or len(all_true_list) != len(all_preds_list):\n",
    "        print(f\"Warning: Mismatch/empty metric lists in {phase}. True: {len(all_true_list)}, Pred: {len(all_preds_list)}\")\n",
    "        empty_return = (avg_loss,0,0,0,0,None,all_true_list,all_preds_list, None)\n",
    "        if return_embeddings_and_ids: empty_return += (torch.empty(0), torch.empty(0), [])\n",
    "        return empty_return\n",
    "    \n",
    "    accuracy = accuracy_score(all_true_list, all_preds_list)\n",
    "    avg_mode_overall = 'binary' if CLASSIFICATION_MODE == 'binary' else 'weighted'\n",
    "    pos_label_overall = 1 if CLASSIFICATION_MODE == 'binary' else None\n",
    "    \n",
    "    if CLASSIFICATION_MODE == 'binary':\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_true_list, all_preds_list, average=avg_mode_overall, pos_label=pos_label_overall, zero_division=0)\n",
    "        if len(np.unique(all_true_list)) >=2:\n",
    "            target_names = ['Normal (0)', 'Attack (1)']\n",
    "            try: \n",
    "                class_report_dict = classification_report(all_true_list, all_preds_list, target_names=target_names, zero_division=0, output_dict=True)\n",
    "            except ValueError as e_report: print(f\"Could not generate classification report dict in {phase}: {e_report}\")\n",
    "    else: \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_true_list, all_preds_list, average=avg_mode_overall, zero_division=0)\n",
    "        if len(np.unique(all_true_list)) >=2:\n",
    "            try: class_report_dict = classification_report(all_true_list, all_preds_list, zero_division=0, output_dict=True)\n",
    "            except ValueError as e_report: print(f\"Could not generate classification report dict in {phase}: {e_report}\")\n",
    "    \n",
    "    auc = None\n",
    "    if CLASSIFICATION_MODE == 'binary' and len(all_true_list) > 0 :\n",
    "        valid_pairs = [(t, p) for t, p in zip(all_true_list, all_probs_binary_list) if not (isinstance(p, (float, np.floating)) and (np.isnan(p) or np.isinf(p)))]\n",
    "        if len(valid_pairs) > 1:\n",
    "            vt, vp = [p[0] for p in valid_pairs], [p[1] for p in valid_pairs]\n",
    "            if len(np.unique(vt)) >= 2:\n",
    "                try: auc = roc_auc_score(vt, vp)\n",
    "                except ValueError as e: print(f\"Could not compute AUC in {phase} (filtered): {e}. Unique true: {np.unique(vt)}\")\n",
    "            else: print(f\"Not enough unique classes in valid_true_for_auc ({np.unique(vt)}) for AUC in {phase}.\")\n",
    "        else: print(f\"Not enough valid (non-NaN prob) points ({len(valid_pairs)}) for AUC in {phase}.\")\n",
    "    \n",
    "    if return_embeddings_and_ids:\n",
    "        final_embeddings = torch.cat(all_node_embeddings_list, dim=0) if all_node_embeddings_list else torch.empty(0)\n",
    "        # final_indices below would be local if all_target_node_indices_global_list is not populated with global indices\n",
    "        final_indices = torch.cat(all_target_node_indices_global_list, dim=0) if all_target_node_indices_global_list else torch.empty(0) \n",
    "        final_entity_ids = [] # Placeholder, not populated in this version\n",
    "        return avg_loss, accuracy, precision, recall, f1, auc, all_true_list, all_preds_list, class_report_dict, final_embeddings, final_indices, final_entity_ids\n",
    "    else:\n",
    "        return avg_loss, accuracy, precision, recall, f1, auc, all_true_list, all_preds_list, class_report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae711d39-282f-4cda-bb68-276e8cd4306c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: NEW/MODIFIED - Prepare Sequences from Event Embeddings\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils # 確保 rnn_utils 已導入，雖然此函數不直接用它，但它在序列部分被使用\n",
    "\n",
    "def get_all_event_embeddings(model_tgat, data_cpu, batch_size_for_gen, num_neighbors_tgat, device_tgat,\n",
    "                             raw_file_path_for_loader_ids=None, # 新增參數\n",
    "                             col_names_for_loader_ids=None):    # 新增參數\n",
    "    \"\"\"\n",
    "    Gets TGAT embeddings for all events in data_cpu.\n",
    "    Assumes model_tgat is already trained and on the correct device.\n",
    "    \"\"\"\n",
    "    model_tgat.eval()\n",
    "    # 使用修改後的 TemporalNeighborLoader，它可以處理原始文件路徑以進行特徵相似性取樣\n",
    "    loader = TemporalNeighborLoader(\n",
    "        data_cpu, \n",
    "        batch_size_for_gen, \n",
    "        num_neighbors_tgat, \n",
    "        device_tgat, \n",
    "        shuffle=False,\n",
    "        raw_data_file_path_for_ids=raw_file_path_for_loader_ids, # 傳遞參數\n",
    "        col_names_list=col_names_for_loader_ids                    # 傳遞參數\n",
    "    )\n",
    "    \n",
    "    all_embeddings_list = []\n",
    "    all_global_indices_list = [] \n",
    "    all_original_labels_list = [] \n",
    "    \n",
    "    print(\"Generating TGAT embeddings for all events...\")\n",
    "    processed_global_idx_count = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(loader, desc=\"Generating Embeddings\"):\n",
    "            target_indices_local_dev, batch_features_dev, batch_ts_dev, \\\n",
    "            neighbor_batches_local_dev, batch_labels_dev = batch_data\n",
    "\n",
    "            batch_size_actual = target_indices_local_dev.size(0)\n",
    "            # 假設非隨機 loader 按順序提供全局索引\n",
    "            global_indices_for_this_batch = torch.arange(\n",
    "                processed_global_idx_count, \n",
    "                processed_global_idx_count + batch_size_actual\n",
    "            ).long()\n",
    "            processed_global_idx_count += batch_size_actual\n",
    "\n",
    "            _, node_embeddings = model_tgat( # 假設 TGAT forward 返回 (logits, embeddings)\n",
    "                target_indices_local_dev, batch_features_dev, batch_ts_dev, \n",
    "                neighbor_batches_local_dev, return_embedding=True\n",
    "            )\n",
    "            all_embeddings_list.append(node_embeddings.cpu())\n",
    "            all_global_indices_list.append(global_indices_for_this_batch.cpu())\n",
    "            all_original_labels_list.append(batch_labels_dev.cpu())\n",
    "    \n",
    "    if not all_embeddings_list:\n",
    "        # 如果模型有 attn_layers 屬性且不為空，則獲取輸出維度，否則使用預設值\n",
    "        emb_dim_placeholder = model_tgat.attn_layers[-1].n_out_dim_layer \\\n",
    "            if hasattr(model_tgat, 'attn_layers') and model_tgat.attn_layers \\\n",
    "            else (model_tgat.output_mlp[0].in_features if hasattr(model_tgat, 'output_mlp') and model_tgat.output_mlp else 128) # 備用回退\n",
    "        \n",
    "        # 返回兩個值以匹配解包\n",
    "        return torch.empty(0, emb_dim_placeholder), torch.empty(0, dtype=torch.long) \n",
    "\n",
    "    full_embeddings = torch.cat(all_embeddings_list, dim=0)\n",
    "    full_global_indices = torch.cat(all_global_indices_list, dim=0)\n",
    "    full_original_labels = torch.cat(all_original_labels_list, dim=0)\n",
    "\n",
    "    sorted_idx_map = torch.argsort(full_global_indices)\n",
    "    sorted_embeddings = full_embeddings[sorted_idx_map]\n",
    "    sorted_labels = full_original_labels[sorted_idx_map]\n",
    "    \n",
    "    print(f\"Generated {sorted_embeddings.shape[0]} event embeddings of dimension {sorted_embeddings.shape[1]}\")\n",
    "    return sorted_embeddings, sorted_labels\n",
    "\n",
    "def create_embedding_sequences(event_embeddings, event_labels, sequence_length, step_size,\n",
    "                               label_mode='any_attack', # 'any_attack', 'all_attack', 'majority_attack'\n",
    "                               classification_mode='binary'): # For KDD, labels are binary or multiclass event-wise\n",
    "    \"\"\"\n",
    "    Creates sequences of embeddings and corresponding sequence labels.\n",
    "    event_labels should be for binary classification (0 for normal, 1 for attack event).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    sequence_labels = []\n",
    "    num_events = event_embeddings.shape[0]\n",
    "\n",
    "    for i in range(0, num_events - sequence_length + 1, step_size):\n",
    "        seq = event_embeddings[i : i + sequence_length]\n",
    "        seq_event_labels = event_labels[i : i + sequence_length] # These are event-level labels\n",
    "\n",
    "        if classification_mode == 'binary':\n",
    "            # Determine sequence label based on event labels within the sequence\n",
    "            if label_mode == 'any_attack':\n",
    "                # If any event in the sequence is an attack, the sequence is an attack\n",
    "                label = 1 if torch.any(seq_event_labels.float() == 1.0) else 0\n",
    "            elif label_mode == 'all_attack':\n",
    "                # Only if all events in sequence are attacks\n",
    "                label = 1 if torch.all(seq_event_labels.float() == 1.0) else 0\n",
    "            elif label_mode == 'majority_attack':\n",
    "                # If majority of events are attacks\n",
    "                label = 1 if torch.sum(seq_event_labels.float() == 1.0) > sequence_length / 2 else 0\n",
    "            else: # Default to 'any_attack'\n",
    "                label = 1 if torch.any(seq_event_labels.float() == 1.0) else 0\n",
    "        else: # Multiclass - how to define sequence label? For now, let's assume binary sequence target.\n",
    "            # Or, one could try to predict the dominant attack type in the sequence, or a multi-label output.\n",
    "            # This part needs more sophisticated handling for multiclass sequence labeling.\n",
    "            # For simplicity, we'll stick to binary sequence classification (attack vs. normal sequence).\n",
    "            print(\"Warning: Sequence labeling for 'multiclass' event labels is not deeply implemented. Defaulting to binary 'any_attack'.\")\n",
    "            label = 1 if torch.any(seq_event_labels.float() > 0) else 0 # Assuming 0 is normal in multiclass too\n",
    "\n",
    "        sequences.append(seq)\n",
    "        sequence_labels.append(label)\n",
    "    \n",
    "    if not sequences:\n",
    "        # Return empty tensors with expected structure if no sequences are generated\n",
    "        embedding_dim = event_embeddings.shape[1] if event_embeddings.numel() > 0 else 1\n",
    "        return [], [], torch.empty(0, sequence_length, embedding_dim), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "\n",
    "    # For PackedSequence, we need lengths of sequences before padding\n",
    "    # In this sliding window approach, all sequences have the same length `sequence_length`\n",
    "    lengths = [sequence_length] * len(sequences)\n",
    "    \n",
    "    # Pad sequences to the max length (which is sequence_length here)\n",
    "    # `sequences` is a list of Tensors [ (seq_len, embed_dim), ... ]\n",
    "    # Need to stack them and then pad if lengths were variable. Here they are fixed.\n",
    "    padded_sequences = torch.stack(sequences) # (num_sequences, sequence_length, embedding_dim)\n",
    "    \n",
    "    return sequences, sequence_labels, padded_sequences, torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "\n",
    "class EmbeddingSequenceDataset(Dataset):\n",
    "    def __init__(self, padded_sequences, sequence_labels, sequence_lengths):\n",
    "        self.padded_sequences = padded_sequences\n",
    "        self.sequence_labels = torch.tensor(sequence_labels, dtype=torch.float32) # For BCEWithLogitsLoss\n",
    "        self.sequence_lengths = sequence_lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.padded_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return sequence, its actual length, and its label\n",
    "        return self.padded_sequences[idx], self.sequence_lengths[idx], self.sequence_labels[idx]\n",
    "\n",
    "def collate_fn_packed(batch):\n",
    "    sequences, lengths, labels = zip(*batch)\n",
    "    # sequences are already padded to the same length `sequence_length`\n",
    "    padded_sequences = torch.stack(sequences) # (batch_size, sequence_length, embedding_dim)\n",
    "    \n",
    "    # Create PackedSequence\n",
    "    # Sort by lengths in descending order for pack_padded_sequence\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    sorted_lengths, sorted_idx = lengths.sort(descending=True)\n",
    "    sorted_sequences = padded_sequences[sorted_idx]\n",
    "    \n",
    "    packed_sequences = rnn_utils.pack_padded_sequence(sorted_sequences, sorted_lengths.cpu(), batch_first=True)\n",
    "    \n",
    "    # Also sort labels according to sorted_idx\n",
    "    labels = torch.stack(labels)[sorted_idx]\n",
    "    \n",
    "    return packed_sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d9075d7-1512-48d5-bfa9-2f543572affa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: NEW - Training and Evaluation for Sequence Aggregator Model\n",
    "\n",
    "def train_sequence_epoch(seq_model, loader, optimizer, criterion_seq, device):\n",
    "    seq_model.train()\n",
    "    total_loss = 0.0\n",
    "    all_seq_preds, all_seq_true = [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Train Sequence Epoch\")\n",
    "    for packed_sequences_batch, labels_batch in pbar:\n",
    "        packed_sequences_batch = packed_sequences_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device).unsqueeze(1) # For BCEWithLogitsLoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_logits_seq = seq_model(packed_sequences_batch)\n",
    "        \n",
    "        loss = criterion_seq(output_logits_seq, labels_batch)\n",
    "        if loss.isnan() or loss.isinf():\n",
    "            print(f\"Sequence Train: Loss NaN/Inf. Skipping batch.\")\n",
    "            continue\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(output_logits_seq.detach()) > 0.5).cpu().numpy()\n",
    "        all_seq_preds.extend(preds.flatten().tolist())\n",
    "        all_seq_true.extend(labels_batch.cpu().numpy().flatten().tolist())\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    avg_loss = total_loss / len(loader) if len(loader) > 0 else float('nan')\n",
    "    accuracy = accuracy_score(all_seq_true, all_seq_preds) if all_seq_true else 0.0\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_seq_true, all_seq_preds, average='binary', zero_division=0) if all_seq_true else (0,0,0,None)\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "def evaluate_sequence_model(seq_model, loader, criterion_seq, device, phase=\"Val Seq\"):\n",
    "    seq_model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_seq_preds, all_seq_true, all_seq_probs = [], [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"{phase} Phase\")\n",
    "    with torch.no_grad():\n",
    "        for packed_sequences_batch, labels_batch in pbar:\n",
    "            packed_sequences_batch = packed_sequences_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device).unsqueeze(1)\n",
    "\n",
    "            output_logits_seq = seq_model(packed_sequences_batch)\n",
    "            loss = criterion_seq(output_logits_seq, labels_batch)\n",
    "            if loss.isnan() or loss.isinf():\n",
    "                print(f\"Sequence Eval {phase}: Loss NaN/Inf. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(output_logits_seq.detach())\n",
    "            preds = (probs > 0.5).cpu().numpy()\n",
    "            \n",
    "            all_seq_preds.extend(preds.flatten().tolist())\n",
    "            all_seq_true.extend(labels_batch.cpu().numpy().flatten().tolist())\n",
    "            all_seq_probs.extend(probs.cpu().numpy().flatten().tolist())\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(loader) if len(loader) > 0 else float('nan')\n",
    "    accuracy = accuracy_score(all_seq_true, all_seq_preds) if all_seq_true else 0.0\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_seq_true, all_seq_preds, average='binary', zero_division=0) if all_seq_true else (0,0,0,None)\n",
    "    \n",
    "    auc = None\n",
    "    if all_seq_true and all_seq_probs and len(np.unique(all_seq_true)) >= 2:\n",
    "        try:\n",
    "            auc = roc_auc_score(all_seq_true, all_seq_probs)\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not compute AUC for sequences in {phase}: {e}\")\n",
    "            \n",
    "    print_metrics(phase, avg_loss, accuracy, precision, recall, f1, auc, phase=f\"{phase} Results\")\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc, all_seq_true, all_seq_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb9261f6-ce2d-418d-a83c-c16c952b719c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: Sequence Modeling Additions and Modified Pipeline (Conceptual)\n",
    "# Ensure all necessary imports from the original notebook are present\n",
    "# e.g., torch, nn, optim, tqdm, sklearn.metrics, plt, np, os, json, time, Dataset, DataLoader, F\n",
    "# from torch.utils.data import Dataset, DataLoader # Add if not already imported globally\n",
    "\n",
    "\n",
    "# 1. EventSequenceAggregator Model Definition\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "class EventSequenceAggregator(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_classes, dropout=0.1, rnn_type='GRU'):\n",
    "        super(EventSequenceAggregator, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "\n",
    "        if self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        elif self.rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type. Choose 'GRU' or 'LSTM'.\")\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout_layer = nn.Dropout(dropout) # Renamed to avoid conflict\n",
    "\n",
    "    def forward(self, packed_event_embeddings):\n",
    "        # packed_event_embeddings is a PackedSequence object\n",
    "        if self.rnn_type == 'GRU':\n",
    "            # output is (batch, seq_len, hidden_dim) for packed sequence\n",
    "            # hidden is (num_layers, batch_size, hidden_dim)\n",
    "            _, hidden = self.rnn(packed_event_embeddings) \n",
    "            last_hidden = hidden[-1, :, :] \n",
    "        elif self.rnn_type == 'LSTM':\n",
    "            # output is (batch, seq_len, hidden_dim)\n",
    "            # h_n is (num_layers, batch_size, hidden_dim)\n",
    "            # c_n is (num_layers, batch_size, hidden_dim)\n",
    "            _, (h_n, _) = self.rnn(packed_event_embeddings) \n",
    "            last_hidden = h_n[-1, :, :]\n",
    "        \n",
    "        out = self.dropout_layer(last_hidden)\n",
    "        out = self.fc(out) \n",
    "        return out\n",
    "\n",
    "# 2. Functions to Prepare Sequences from Event Embeddings\n",
    "def get_all_event_embeddings(model_tgat, data_cpu, batch_size_for_gen, num_neighbors_tgat, device_tgat):\n",
    "    model_tgat.eval()\n",
    "    # Use the modified TemporalNeighborLoader that handles sliced data loading\n",
    "    loader = TemporalNeighborLoader(data_cpu, batch_size_for_gen, num_neighbors_tgat, device_tgat, shuffle=False)\n",
    "    \n",
    "    all_embeddings_list = []\n",
    "    all_global_indices_list = [] \n",
    "    all_original_labels_list = [] \n",
    "    \n",
    "    print(\"Generating TGAT embeddings for all events...\")\n",
    "    processed_global_idx_count = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(loader, desc=\"Generating Embeddings\"):\n",
    "            target_indices_local_dev, batch_features_dev, batch_ts_dev, \\\n",
    "            neighbor_batches_local_dev, batch_labels_dev = batch_data\n",
    "\n",
    "            batch_size_actual = target_indices_local_dev.size(0)\n",
    "            # Assuming non-shuffled loader gives sequential global indices\n",
    "            global_indices_for_this_batch = torch.arange(\n",
    "                processed_global_idx_count, \n",
    "                processed_global_idx_count + batch_size_actual\n",
    "            ).long()\n",
    "            processed_global_idx_count += batch_size_actual\n",
    "\n",
    "            _, node_embeddings = model_tgat( # Assuming TGAT forward returns (logits, embeddings)\n",
    "                target_indices_local_dev, batch_features_dev, batch_ts_dev, \n",
    "                neighbor_batches_local_dev, return_embedding=True\n",
    "            )\n",
    "            all_embeddings_list.append(node_embeddings.cpu())\n",
    "            all_global_indices_list.append(global_indices_for_this_batch.cpu())\n",
    "            all_original_labels_list.append(batch_labels_dev.cpu())\n",
    "\n",
    "    if not all_embeddings_list:\n",
    "        # Determine embedding_dim from model or data if possible, else use a placeholder\n",
    "        emb_dim_placeholder = model_tgat.attn_layers[-1].n_out_dim_layer if hasattr(model_tgat, 'attn_layers') and model_tgat.attn_layers else 128 # Fallback\n",
    "        return torch.empty(0, emb_dim_placeholder), torch.empty(0, dtype=torch.long), torch.empty(0)\n",
    "\n",
    "\n",
    "    full_embeddings = torch.cat(all_embeddings_list, dim=0)\n",
    "    full_global_indices = torch.cat(all_global_indices_list, dim=0)\n",
    "    full_original_labels = torch.cat(all_original_labels_list, dim=0)\n",
    "\n",
    "    sorted_idx_map = torch.argsort(full_global_indices)\n",
    "    sorted_embeddings = full_embeddings[sorted_idx_map]\n",
    "    sorted_labels = full_original_labels[sorted_idx_map]\n",
    "    \n",
    "    print(f\"Generated {sorted_embeddings.shape[0]} event embeddings of dimension {sorted_embeddings.shape[1]}\")\n",
    "    return sorted_embeddings, sorted_labels\n",
    "\n",
    "\n",
    "def create_embedding_sequences(event_embeddings, event_labels, sequence_length, step_size,\n",
    "                               label_mode='any_attack', classification_mode='binary'):\n",
    "    sequences_as_tensors = [] # Store list of tensors directly\n",
    "    sequence_labels_list = []\n",
    "    num_events = event_embeddings.shape[0]\n",
    "\n",
    "    for i in range(0, num_events - sequence_length + 1, step_size):\n",
    "        seq = event_embeddings[i : i + sequence_length] # (sequence_length, embedding_dim)\n",
    "        seq_event_labels = event_labels[i : i + sequence_length]\n",
    "\n",
    "        label = 0 # Default to normal\n",
    "        if classification_mode == 'binary': # Ensure event_labels are binary (0 or 1)\n",
    "            if label_mode == 'any_attack':\n",
    "                label = 1 if torch.any(seq_event_labels.float() == 1.0) else 0\n",
    "            elif label_mode == 'all_attack':\n",
    "                label = 1 if torch.all(seq_event_labels.float() == 1.0) else 0\n",
    "            elif label_mode == 'majority_attack':\n",
    "                label = 1 if torch.sum(seq_event_labels.float() == 1.0) > sequence_length / 2 else 0\n",
    "            else: \n",
    "                label = 1 if torch.any(seq_event_labels.float() == 1.0) else 0\n",
    "        else: # For multiclass event labels, defining sequence label needs care\n",
    "            print(\"Warning: Sequence labeling for 'multiclass' event labels is complex. Defaulting to binary 'any_attack' (event_label > 0 is attack).\")\n",
    "            label = 1 if torch.any(seq_event_labels.float() > 0) else 0 # Assuming 0 is normal\n",
    "\n",
    "        sequences_as_tensors.append(seq)\n",
    "        sequence_labels_list.append(label)\n",
    "    \n",
    "    if not sequences_as_tensors:\n",
    "        embedding_dim = event_embeddings.shape[1] if event_embeddings.numel() > 0 else 1\n",
    "        return [], torch.empty(0, sequence_length, embedding_dim), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    padded_sequences = torch.stack(sequences_as_tensors) # (num_sequences, sequence_length, embedding_dim)\n",
    "    lengths = torch.full((len(sequences_as_tensors),), sequence_length, dtype=torch.long) # All sequences have same length\n",
    "    \n",
    "    return sequence_labels_list, padded_sequences, lengths\n",
    "\n",
    "\n",
    "class EmbeddingSequenceDataset(Dataset):\n",
    "    def __init__(self, padded_sequences, sequence_labels, sequence_lengths):\n",
    "        self.padded_sequences = padded_sequences\n",
    "        # Ensure sequence_labels is a tensor\n",
    "        if isinstance(sequence_labels, list):\n",
    "            self.sequence_labels = torch.tensor(sequence_labels, dtype=torch.float32)\n",
    "        elif isinstance(sequence_labels, torch.Tensor):\n",
    "            self.sequence_labels = sequence_labels.float()\n",
    "        else:\n",
    "            raise TypeError(f\"sequence_labels must be a list or Tensor, got {type(sequence_labels)}\")\n",
    "\n",
    "        self.sequence_lengths = sequence_lengths # This is a tensor of lengths for each sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.padded_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.padded_sequences[idx], self.sequence_lengths[idx], self.sequence_labels[idx]\n",
    "\n",
    "def collate_fn_packed_fixed_length(batch): # Simplified for fixed length sequences from sliding window\n",
    "    sequences, lengths, labels = zip(*batch)\n",
    "    # sequences are already Tensors of shape (sequence_length, embedding_dim)\n",
    "    # Stack them to create (batch_size, sequence_length, embedding_dim)\n",
    "    stacked_sequences = torch.stack(sequences)\n",
    "    \n",
    "    # lengths are all the same (SEQUENCE_LENGTH), but pack_padded_sequence still needs them\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.long) \n",
    "    \n",
    "    # Sort by lengths in descending order (though here all are same)\n",
    "    sorted_lengths, sorted_idx = lengths_tensor.sort(descending=True)\n",
    "    sorted_sequences = stacked_sequences[sorted_idx]\n",
    "    \n",
    "    packed_sequences = rnn_utils.pack_padded_sequence(sorted_sequences, sorted_lengths.cpu(), batch_first=True)\n",
    "    \n",
    "    # Sort labels\n",
    "    labels_tensor = torch.stack([l if isinstance(l, torch.Tensor) else torch.tensor(l) for l in labels])\n",
    "    sorted_labels = labels_tensor[sorted_idx]\n",
    "    \n",
    "    return packed_sequences, sorted_labels\n",
    "\n",
    "\n",
    "# 3. Training and Evaluation Functions for Sequence Model\n",
    "def train_sequence_epoch(seq_model, loader, optimizer_seq, criterion_seq, device_seq):\n",
    "    seq_model.train()\n",
    "    total_loss_seq = 0.0\n",
    "    all_seq_preds_epoch, all_seq_true_epoch = [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Train Sequence Epoch\")\n",
    "    for packed_sequences_batch, labels_batch_seq in pbar:\n",
    "        packed_sequences_batch = packed_sequences_batch.to(device_seq)\n",
    "        labels_batch_seq = labels_batch_seq.to(device_seq).unsqueeze(1) # For BCEWithLogitsLoss\n",
    "\n",
    "        optimizer_seq.zero_grad()\n",
    "        output_logits_seq = seq_model(packed_sequences_batch)\n",
    "        \n",
    "        loss_seq = criterion_seq(output_logits_seq, labels_batch_seq)\n",
    "        if loss_seq.isnan() or loss_seq.isinf():\n",
    "            print(f\"Sequence Train: Loss NaN/Inf. Logits: {output_logits_seq.flatten()[:3]}, Labels: {labels_batch_seq.flatten()[:3]}. Skipping batch.\")\n",
    "            continue\n",
    "            \n",
    "        loss_seq.backward()\n",
    "        optimizer_seq.step()\n",
    "        \n",
    "        total_loss_seq += loss_seq.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds_seq = (torch.sigmoid(output_logits_seq.detach()) > 0.5).cpu().numpy()\n",
    "        all_seq_preds_epoch.extend(preds_seq.flatten().tolist())\n",
    "        all_seq_true_epoch.extend(labels_batch_seq.cpu().numpy().flatten().tolist())\n",
    "        pbar.set_postfix({'loss': loss_seq.item()})\n",
    "        \n",
    "    avg_loss_seq = total_loss_seq / len(loader) if len(loader) > 0 else float('nan')\n",
    "    # Ensure all_seq_true_epoch is not empty before calculating metrics\n",
    "    if not all_seq_true_epoch:\n",
    "        print(\"Warning: No true labels collected in sequence training epoch.\")\n",
    "        return avg_loss_seq, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    accuracy_seq = accuracy_score(all_seq_true_epoch, all_seq_preds_epoch)\n",
    "    precision_seq, recall_seq, f1_seq, _ = precision_recall_fscore_support(all_seq_true_epoch, all_seq_preds_epoch, average='binary', zero_division=0)\n",
    "    return avg_loss_seq, accuracy_seq, precision_seq, recall_seq, f1_seq\n",
    "\n",
    "def evaluate_sequence_model(seq_model, loader, criterion_seq, device_seq, phase=\"Val Seq\"):\n",
    "    seq_model.eval()\n",
    "    total_loss_seq = 0.0\n",
    "    all_seq_preds_eval, all_seq_true_eval, all_seq_probs_eval = [], [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"{phase} Phase\")\n",
    "    with torch.no_grad():\n",
    "        for packed_sequences_batch, labels_batch_seq in pbar:\n",
    "            packed_sequences_batch = packed_sequences_batch.to(device_seq)\n",
    "            labels_batch_seq = labels_batch_seq.to(device_seq).unsqueeze(1)\n",
    "\n",
    "            output_logits_seq = seq_model(packed_sequences_batch)\n",
    "            loss_seq = criterion_seq(output_logits_seq, labels_batch_seq)\n",
    "            if loss_seq.isnan() or loss_seq.isinf():\n",
    "                print(f\"Sequence Eval {phase}: Loss NaN/Inf. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            total_loss_seq += loss_seq.item()\n",
    "            \n",
    "            probs_seq = torch.sigmoid(output_logits_seq.detach())\n",
    "            preds_seq = (probs_seq > 0.5).cpu().numpy()\n",
    "            \n",
    "            all_seq_preds_eval.extend(preds_seq.flatten().tolist())\n",
    "            all_seq_true_eval.extend(labels_batch_seq.cpu().numpy().flatten().tolist())\n",
    "            all_seq_probs_eval.extend(probs_seq.cpu().numpy().flatten().tolist())\n",
    "            pbar.set_postfix({'loss': loss_seq.item()})\n",
    "\n",
    "    if not all_seq_true_eval: # Check if any data was processed\n",
    "        print(f\"Warning: No true labels collected in sequence evaluation phase {phase}.\")\n",
    "        return float('nan'), 0.0, 0.0, 0.0, 0.0, None, [], []\n",
    "\n",
    "\n",
    "    avg_loss_seq = total_loss_seq / len(loader) if len(loader) > 0 else float('nan')\n",
    "    accuracy_seq = accuracy_score(all_seq_true_eval, all_seq_preds_eval)\n",
    "    precision_seq, recall_seq, f1_seq, _ = precision_recall_fscore_support(all_seq_true_eval, all_seq_preds_eval, average='binary', zero_division=0)\n",
    "    \n",
    "    auc_seq = None\n",
    "    if all_seq_probs_eval and len(np.unique(all_seq_true_eval)) >= 2:\n",
    "        try:\n",
    "            auc_seq = roc_auc_score(all_seq_true_eval, all_seq_probs_eval)\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not compute AUC for sequences in {phase}: {e}\")\n",
    "            \n",
    "    print_metrics(f\"{phase} Seq Aggregator\", avg_loss_seq, accuracy_seq, precision_seq, recall_seq, f1_seq, auc_seq, phase=f\"{phase} Seq Results\")\n",
    "    return avg_loss_seq, accuracy_seq, precision_seq, recall_seq, f1_seq, auc_seq, all_seq_true_eval, all_seq_preds_eval\n",
    "\n",
    "\n",
    "# 4. Modified train_pipeline (Conceptual - integrate stage 2)\n",
    "# This is a high-level sketch. The original train_pipeline needs to be carefully refactored.\n",
    "# For now, I will keep the original train_pipeline as is, and the sequence training\n",
    "# will be a separate set of calls in the __main__ block after TGAT training.\n",
    "\n",
    "# --- Keeping original train_pipeline for TGAT ---\n",
    "# The original train_pipeline in cell \"d6727e62-27d4-4ac9-8ffa-2563c1be7743\"\n",
    "# will train and save the best TGAT model. We will use that saved model.\n",
    "\n",
    "# --- Add these to your global configuration (e.g., Cell 2 of the notebook) ---\n",
    "# Make sure these are defined if you haven't already\n",
    "BATCH_SIZE_SEQ_EMBED_GEN = BATCH_SIZE # Use TGAT's batch size for consistency or define separately\n",
    "SEQUENCE_LENGTH = 10 \n",
    "STEP_SIZE = 5      \n",
    "SEQ_LABEL_MODE = 'any_attack'\n",
    "BATCH_SIZE_SEQ_MODEL = 64  \n",
    "LEARNING_RATE_SEQ_MODEL = 1e-4\n",
    "EPOCHS_SEQ_MODEL = 20 # Fewer epochs for the sequence model might be sufficient\n",
    "SEQ_MODEL_EMBEDDING_DIM_ACTUAL = HIDDEN_DIM # This is the output dim of TGAT's attention layers, used as input to its MLP\n",
    "                                          # and thus the dimension of embeddings `h` returned by TGAT\n",
    "SEQ_MODEL_HIDDEN_DIM = 128   \n",
    "SEQ_MODEL_NUM_LAYERS = 1 # GRU/LSTM layers   \n",
    "SEQ_MODEL_RNN_TYPE = 'GRU'        \n",
    "SEQ_MODEL_DROPOUT = 0.2\n",
    "\n",
    "# Ensure DEVICE is defined globally\n",
    "# DEVICE = get_device() \n",
    "\n",
    "# It's better to run sequence model training as a separate step after TGAT training is complete.\n",
    "# So, the `train_pipeline` function itself will not be massively changed to include stage 2.\n",
    "# Instead, the __main__ block will call TGAT training, then sequence model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebd06a-8c64-4583-a08b-08be53e7298e",
   "metadata": {},
   "source": [
    "## 8. Main Training Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6727e62-27d4-4ac9-8ffa-2563c1be7743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell: train_pipeline function (MODIFIED Loader Instantiation)\n",
    "def train_pipeline():\n",
    "    # --- STAGE 1: Train TGAT Event-Level Model ---\n",
    "    print(\"--- Stage 1: TGAT Event-Level Model Training ---\")\n",
    "    stage1_start_time = time.time()\n",
    "    \n",
    "    processed_train_file_path = globals().get('PROCESSED_TRAIN_FILE', './processed_data_nslkdd/train_temporal_data_nslkdd.pt')\n",
    "    processed_test_file_path = globals().get('PROCESSED_TEST_FILE', './processed_data_nslkdd/test_temporal_data_nslkdd.pt')\n",
    "    metadata_file_path = globals().get('METADATA_FILE', './processed_data_nslkdd/metadata_nslkdd.json')\n",
    "    classification_mode_val = globals().get('CLASSIFICATION_MODE', 'binary')\n",
    "    \n",
    "    if not all(k in globals() for k in ['PROCESSED_TRAIN_FILE', 'METADATA_FILE', 'CLASSIFICATION_MODE', 'PROCESSED_TEST_FILE']):\n",
    "        print(\"Warning: Some essential global configuration variables for data loading might be missing. Using defaults.\")\n",
    "\n",
    "    train_data_cpu_tgat, metadata_tgat, num_classes_meta_tgat, pos_weight_cpu_tgat = load_processed_data(\n",
    "        processed_train_file_path, metadata_file_path, classification_mode_val\n",
    "    )\n",
    "    test_data_cpu_tgat, _, _, _ = load_processed_data( \n",
    "        processed_test_file_path, metadata_file_path, classification_mode_val\n",
    "    )\n",
    "\n",
    "    NODE_FEAT_DIM_RUNTIME_TGAT = metadata_tgat['NODE_FEAT_DIM']\n",
    "    \n",
    "    device_val = globals().get('DEVICE', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    use_focal_loss_val = globals().get('USE_FOCAL_LOSS', True)\n",
    "    if 'USE_FOCAL_LOSS' not in globals(): print(f\"Warning: Global 'USE_FOCAL_LOSS' not set, defaulted to {use_focal_loss_val}.\")\n",
    "\n",
    "    if classification_mode_val == 'binary':\n",
    "        ACTUAL_NUM_OUTPUT_CLASSES_TGAT = 1 \n",
    "        if use_focal_loss_val: \n",
    "            criterion_tgat = FocalLoss(gamma=2.0, \n",
    "                                       pos_weight_for_bce=pos_weight_cpu_tgat.to(device_val) if pos_weight_cpu_tgat is not None else None, \n",
    "                                       alpha=None) # 提示: Normal 類別召回率低時可考慮調整 alpha\n",
    "        else: \n",
    "            criterion_tgat = nn.BCEWithLogitsLoss(pos_weight=pos_weight_cpu_tgat.to(device_val) if pos_weight_cpu_tgat is not None else None)\n",
    "    else: \n",
    "        ACTUAL_NUM_OUTPUT_CLASSES_TGAT = metadata_tgat.get('NUM_CLASSES_MULTI', num_classes_meta_tgat)\n",
    "        criterion_tgat = nn.CrossEntropyLoss()\n",
    "\n",
    "    epochs_val = globals().get('EPOCHS', 3)\n",
    "    batch_size_val = globals().get('BATCH_SIZE', 256)\n",
    "    learning_rate_val = globals().get('LEARNING_RATE', 0.0005)\n",
    "    hidden_dim_val = globals().get('HIDDEN_DIM', 256)\n",
    "    time_dim_val = globals().get('TIME_DIM', 64)\n",
    "    n_layers_val = globals().get('N_LAYERS', 2)\n",
    "    n_heads_val = globals().get('N_HEADS', 4)\n",
    "    dropout_val = globals().get('DROPOUT', 0.3)\n",
    "    weight_decay_val = globals().get('WEIGHT_DECAY', 1e-5)\n",
    "\n",
    "    print(f\"\\n--- TGAT Training Configuration ---\")\n",
    "    print(f\"Device: {device_val}, Epochs: {epochs_val}, Batch Size: {batch_size_val}, LR: {learning_rate_val}\")\n",
    "    print(f\"Node Feat Dim: {NODE_FEAT_DIM_RUNTIME_TGAT}, Hidden Dim: {hidden_dim_val}, Time Emb: {time_dim_val}\")\n",
    "    print(f\"TGAT Layers: {n_layers_val}, Heads: {n_heads_val}, Dropout: {dropout_val}\")\n",
    "    print(f\"TGAT Output Dim: {ACTUAL_NUM_OUTPUT_CLASSES_TGAT}, Mode: {classification_mode_val}\")\n",
    "    print(f\"TGAT Loss: {type(criterion_tgat).__name__} (FocalLoss used: {use_focal_loss_val})\")\n",
    "    print(f\"----------------------------\\n\")\n",
    "\n",
    "    tgat_model = TGAT(\n",
    "        node_feat_dim=NODE_FEAT_DIM_RUNTIME_TGAT, time_emb_dim=time_dim_val, n_head=n_heads_val, \n",
    "        n_layers=n_layers_val, hidden_dim_per_layer=hidden_dim_val, \n",
    "        num_classes=ACTUAL_NUM_OUTPUT_CLASSES_TGAT, dropout=dropout_val\n",
    "    ).to(device_val)\n",
    "    \n",
    "    optimizer_tgat = optim.Adam(tgat_model.parameters(), lr=learning_rate_val, weight_decay=weight_decay_val)\n",
    "    lr_scheduler_tgat = optim.lr_scheduler.ReduceLROnPlateau(optimizer_tgat, mode='max', factor=0.5, patience=7, verbose=True, min_lr=1e-7)\n",
    "\n",
    "    num_neighbors_val = globals().get('NUM_NEIGHBORS', [10,5])\n",
    "    recency_bias_factor_val = globals().get('RECENCY_BIAS_FACTOR', 0.9)\n",
    "    feature_similarity_col_name_val = globals().get('FEATURE_SIMILARITY_COL_NAME', 'service')\n",
    "    feature_similarity_weight_val = globals().get('FEATURE_SIMILARITY_WEIGHT', 0.3)\n",
    "    col_names_val = globals().get('COL_NAMES')\n",
    "    raw_data_dir_val = globals().get('RAW_DATA_DIR', './data/')\n",
    "    train_file_val = globals().get('TRAIN_FILE', 'KDDTrain+.txt')\n",
    "    test_file_val = globals().get('TEST_FILE', 'KDDTest+.txt')\n",
    "\n",
    "    if col_names_val is None:\n",
    "        print(\"Critical Warning: Global variable COL_NAMES is not defined. Feature similarity sampling in TemporalNeighborLoader will be disabled.\")\n",
    "        \n",
    "    train_raw_csv_path_to_check = None\n",
    "    if raw_data_dir_val and train_file_val and os.path.exists(os.path.join(str(raw_data_dir_val), str(train_file_val))):\n",
    "        train_raw_csv_path_to_check = os.path.join(str(raw_data_dir_val), str(train_file_val))\n",
    "        print(f\"Train raw data file for loader's feature similarity FOUND at: {os.path.abspath(train_raw_csv_path_to_check)}\")\n",
    "    else:\n",
    "        print(f\"Warning: Train raw data file for loader's feature similarity NOT FOUND at: {os.path.abspath(os.path.join(str(raw_data_dir_val), str(train_file_val)))}. Feature similarity sampling for train loader will be disabled if {feature_similarity_col_name_val} is set.\")\n",
    "\n",
    "    test_raw_csv_path_to_check = None\n",
    "    if raw_data_dir_val and test_file_val and os.path.exists(os.path.join(str(raw_data_dir_val), str(test_file_val))):\n",
    "        test_raw_csv_path_to_check = os.path.join(str(raw_data_dir_val), str(test_file_val))\n",
    "        print(f\"Test raw data file for loader's feature similarity FOUND at: {os.path.abspath(test_raw_csv_path_to_check)}\")\n",
    "    else:\n",
    "        print(f\"Warning: Test raw data file for loader's feature similarity NOT FOUND at: {os.path.abspath(os.path.join(str(raw_data_dir_val), str(test_file_val)))}. Feature similarity sampling for test loader will be disabled if {feature_similarity_col_name_val} is set.\")\n",
    "    \n",
    "    train_loader_tgat = TemporalNeighborLoader(\n",
    "        train_data_cpu_tgat, batch_size_val, num_neighbors_val, device_val, shuffle=True,\n",
    "        recency_bias_factor=recency_bias_factor_val, \n",
    "        feature_similarity_col_name=feature_similarity_col_name_val, \n",
    "        feature_similarity_weight=feature_similarity_weight_val,\n",
    "        raw_data_file_path_for_ids=train_raw_csv_path_to_check,\n",
    "        col_names_list=col_names_val \n",
    "    )\n",
    "    test_loader_tgat = TemporalNeighborLoader(\n",
    "        test_data_cpu_tgat, batch_size_val, num_neighbors_val, device_val, shuffle=False,\n",
    "        recency_bias_factor=recency_bias_factor_val,\n",
    "        feature_similarity_col_name=feature_similarity_col_name_val,\n",
    "        feature_similarity_weight=feature_similarity_weight_val,\n",
    "        raw_data_file_path_for_ids=test_raw_csv_path_to_check,\n",
    "        col_names_list=col_names_val \n",
    "    )\n",
    "    \n",
    "    best_val_f1_attack_tgat = -1.0\n",
    "    history_tgat = {k: [] for k in ['train_loss', 'train_acc', 'train_f1', 'train_auc', \n",
    "                                   'val_loss', 'val_acc', 'val_f1', 'val_f1_attack', 'val_auc']}\n",
    "    epochs_without_improvement_tgat = 0\n",
    "    \n",
    "    early_stopping_patience_val = globals().get('EARLY_STOPPING_PATIENCE', 20)\n",
    "    clip_grad_norm_val = globals().get('CLIP_GRAD_NORM', 1.0)\n",
    "    model_save_dir_val = globals().get('MODEL_SAVE_DIR', './saved_models_nslkdd/')\n",
    "    best_model_name_val = globals().get('BEST_MODEL_NAME', 'best_tgat_model_nslkdd.pth')\n",
    "\n",
    "    for epoch_tgat in range(epochs_val):\n",
    "        epoch_str_display_tgat = f\"TGAT Epoch {epoch_tgat+1}/{epochs_val}\"\n",
    "        print(f\"--- {epoch_str_display_tgat} ---\")\n",
    "        train_loss, train_acc, train_prec, train_rec, train_f1, train_auc = train_epoch(\n",
    "            tgat_model, train_loader_tgat, optimizer_tgat, criterion_tgat, clip_grad_norm_val\n",
    "        )\n",
    "        print_metrics(epoch_str_display_tgat, train_loss, train_acc, train_prec, train_rec, train_f1, train_auc, phase='Train TGAT')\n",
    "        for k,v_val_hist in zip(['loss', 'acc', 'f1', 'auc'],[train_loss, train_acc, train_f1, train_auc]): \n",
    "            history_tgat[f'train_{k}'].append(v_val_hist if v_val_hist is not None and not (isinstance(v_val_hist, float) and np.isnan(v_val_hist)) else np.nan)\n",
    "\n",
    "        eval_output_tgat = evaluate_model(tgat_model, test_loader_tgat, criterion_tgat, phase='Validation TGAT', return_embeddings_and_ids=False)\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, _, _, val_class_report_dict_tgat = eval_output_tgat\n",
    "        \n",
    "        val_f1_attack_current_tgat = np.nan\n",
    "        if classification_mode_val == 'binary' and val_class_report_dict_tgat and isinstance(val_class_report_dict_tgat, dict):\n",
    "            attack_label_str = 'Attack (1)' if 'Attack (1)' in val_class_report_dict_tgat else ('1' if '1' in val_class_report_dict_tgat else None)\n",
    "            if attack_label_str and attack_label_str in val_class_report_dict_tgat: \n",
    "                f1_score_val_metric = val_class_report_dict_tgat[attack_label_str].get('f1-score')\n",
    "                if f1_score_val_metric is not None: \n",
    "                    val_f1_attack_current_tgat = f1_score_val_metric\n",
    "        elif val_f1 is not None and not np.isnan(val_f1): \n",
    "             val_f1_attack_current_tgat = val_f1 \n",
    "        \n",
    "        print_metrics(epoch_str_display_tgat, val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, phase='Validation TGAT', class_report=str(val_class_report_dict_tgat if val_class_report_dict_tgat else \"N/A\"))\n",
    "        history_tgat[f'val_f1_attack'].append(val_f1_attack_current_tgat if not np.isnan(val_f1_attack_current_tgat) else np.nan)\n",
    "        for k,v_val_hist in zip(['loss', 'acc', 'f1', 'auc'],[val_loss, val_acc, val_f1, val_auc]): \n",
    "            history_tgat[f'val_{k}'].append(v_val_hist if v_val_hist is not None and not (isinstance(v_val_hist, float) and np.isnan(v_val_hist)) else np.nan)\n",
    "        \n",
    "        current_lr_tgat = optimizer_tgat.param_groups[0]['lr']\n",
    "        print(f\"TGAT Current LR: {current_lr_tgat}\")\n",
    "        \n",
    "        scheduler_metric = np.nan\n",
    "        if not np.isnan(val_f1_attack_current_tgat):\n",
    "            scheduler_metric = val_f1_attack_current_tgat\n",
    "        elif val_loss is not None and not np.isnan(val_loss) : \n",
    "            scheduler_metric = -val_loss \n",
    "        \n",
    "        if not np.isnan(scheduler_metric):\n",
    "            lr_scheduler_tgat.step(scheduler_metric)\n",
    "        else:\n",
    "            print(f\"{epoch_str_display_tgat}: Both Val F1 (Attack) and Val Loss are NaN, scheduler not stepped.\")\n",
    "\n",
    "        if not np.isnan(val_f1_attack_current_tgat) and val_f1_attack_current_tgat > best_val_f1_attack_tgat:\n",
    "            best_val_f1_attack_tgat = val_f1_attack_current_tgat\n",
    "            if not os.path.exists(model_save_dir_val): os.makedirs(model_save_dir_val)\n",
    "            torch.save(tgat_model.state_dict(), os.path.join(model_save_dir_val, best_model_name_val))\n",
    "            print(f\"{epoch_str_display_tgat}: New best TGAT model saved! Val F1 (Attack): {best_val_f1_attack_tgat:.4f}\")\n",
    "            epochs_without_improvement_tgat = 0\n",
    "        else: \n",
    "            epochs_without_improvement_tgat += 1\n",
    "            current_f1_display = f\"{val_f1_attack_current_tgat:.4f}\" if not np.isnan(val_f1_attack_current_tgat) else \"NaN\"\n",
    "            best_f1_display = f\"{best_val_f1_attack_tgat:.4f}\" if best_val_f1_attack_tgat != -1.0 and not np.isnan(best_val_f1_attack_tgat) else \"N/A\"\n",
    "            print(f\"{epoch_str_display_tgat}: No improvement count: {epochs_without_improvement_tgat} (Current F1 Attack: {current_f1_display}, Best F1 Attack: {best_f1_display}).\")\n",
    "        \n",
    "        if epochs_without_improvement_tgat >= early_stopping_patience_val:\n",
    "            print(f\"TGAT Early stopping triggered after {early_stopping_patience_val} epochs without improvement.\")\n",
    "            break\n",
    "        print(f\"---------------------------------\\n\")\n",
    "\n",
    "    stage1_time = time.time() - stage1_start_time\n",
    "    best_val_f1_attack_tgat_str = f\"{best_val_f1_attack_tgat:.4f}\" if best_val_f1_attack_tgat != -1.0 and not np.isnan(best_val_f1_attack_tgat) else \"N/A\"\n",
    "    print(f\"--- TGAT Training (Stage 1) Finished in {stage1_time:.2f}s. Best Val F1 (Attack): {best_val_f1_attack_tgat_str} ---\")\n",
    "\n",
    "    # --- STAGE 2: Train Sequence Aggregator Model ---\n",
    "    print(\"\\n\\n--- Stage 2: Sequence Aggregator Model Training ---\")\n",
    "    stage2_start_time = time.time()\n",
    "\n",
    "    best_tgat_model_path = os.path.join(model_save_dir_val, best_model_name_val)\n",
    "    if not os.path.exists(best_tgat_model_path):\n",
    "        print(f\"Error: Best TGAT model not found at {best_tgat_model_path} from Stage 1. Cannot proceed with Stage 2.\")\n",
    "        return history_tgat, None, None, None, None, None, None # MODIFIED RETURN\n",
    "\n",
    "    tgat_model_for_embeddings = TGAT(\n",
    "        node_feat_dim=NODE_FEAT_DIM_RUNTIME_TGAT, time_emb_dim=time_dim_val, n_head=n_heads_val,\n",
    "        n_layers=n_layers_val, hidden_dim_per_layer=hidden_dim_val, \n",
    "        num_classes=ACTUAL_NUM_OUTPUT_CLASSES_TGAT, dropout=dropout_val\n",
    "    ).to(device_val)\n",
    "    tgat_model_for_embeddings.load_state_dict(torch.load(best_tgat_model_path, map_location=device_val))\n",
    "    tgat_model_for_embeddings.eval()\n",
    "\n",
    "    batch_size_seq_embed_gen_val = globals().get('BATCH_SIZE_SEQ_EMBED_GEN', batch_size_val)\n",
    "    \n",
    "    # 確保將原始文件路徑和列名傳遞給 get_all_event_embeddings 以便 TemporalNeighborLoader 使用\n",
    "    train_raw_path_for_embed_loader = train_raw_csv_path_to_check if FEATURE_SIMILARITY_COL_NAME else None\n",
    "    test_raw_path_for_embed_loader = test_raw_csv_path_to_check if FEATURE_SIMILARITY_COL_NAME else None\n",
    "\n",
    "    train_event_embeddings, train_event_labels_tgat = get_all_event_embeddings(\n",
    "        tgat_model_for_embeddings, train_data_cpu_tgat, batch_size_seq_embed_gen_val, num_neighbors_val, device_val,\n",
    "        raw_file_path_for_loader_ids=train_raw_path_for_embed_loader, \n",
    "        col_names_for_loader_ids=col_names_val \n",
    "    )\n",
    "    # Initialize test_event_embeddings and test_event_labels_tgat before the if block\n",
    "    # to ensure they are defined for the return statement, even if train_event_embeddings is empty.\n",
    "    test_event_embeddings = torch.empty(0) \n",
    "    test_event_labels_tgat = torch.empty(0)\n",
    "    if train_event_embeddings.numel() > 0: # Only get test embeddings if train embeddings were successful\n",
    "        test_event_embeddings, test_event_labels_tgat = get_all_event_embeddings(\n",
    "            tgat_model_for_embeddings, test_data_cpu_tgat, batch_size_seq_embed_gen_val, num_neighbors_val, device_val,\n",
    "            raw_file_path_for_loader_ids=test_raw_path_for_embed_loader, \n",
    "            col_names_for_loader_ids=col_names_val \n",
    "        )\n",
    "    else: # train_event_embeddings.numel() == 0\n",
    "        print(\"Error: Failed to generate train event embeddings for Stage 2. Aborting Stage 2.\")\n",
    "        return history_tgat, None, None, None, None, None, None \n",
    "\n",
    "    train_event_labels_binary_squeezed = None\n",
    "    test_event_labels_binary_squeezed = None\n",
    "\n",
    "    if train_event_labels_tgat.numel() > 0:\n",
    "        train_event_labels_binary_squeezed = (train_event_labels_tgat.squeeze() > 0).long() if classification_mode_val == 'multiclass' else train_event_labels_tgat.squeeze().long()\n",
    "    else:\n",
    "        train_event_labels_binary_squeezed = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    if test_event_labels_tgat.numel() > 0:\n",
    "        test_event_labels_binary_squeezed = (test_event_labels_tgat.squeeze() > 0).long() if classification_mode_val == 'multiclass' else test_event_labels_tgat.squeeze().long()\n",
    "    else:\n",
    "        test_event_labels_binary_squeezed = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    sequence_length_val = globals().get('SEQUENCE_LENGTH', 10)\n",
    "    step_size_val = globals().get('STEP_SIZE', 5)\n",
    "    seq_label_mode_val = globals().get('SEQ_LABEL_MODE', 'any_attack')\n",
    "\n",
    "    # Initialize sequence variables for returning\n",
    "    returned_train_seq_labels_list = []\n",
    "    returned_train_padded_sequences = torch.empty(0)\n",
    "    returned_train_seq_lengths = torch.empty(0, dtype=torch.long)\n",
    "    returned_test_seq_labels_list = []\n",
    "    returned_test_padded_sequences = torch.empty(0)\n",
    "    returned_test_seq_lengths = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "\n",
    "    if train_event_embeddings.numel() > 0 and train_event_labels_binary_squeezed.numel() > 0:\n",
    "        print(f\"Creating training sequences (Length: {sequence_length_val}, Step: {step_size_val}, Label Mode: {seq_label_mode_val})...\")\n",
    "        _, returned_train_seq_labels_list, returned_train_padded_sequences, returned_train_seq_lengths = create_embedding_sequences( # Corrected unpacking\n",
    "            train_event_embeddings, train_event_labels_binary_squeezed, \n",
    "            sequence_length_val, step_size_val, seq_label_mode_val, 'binary'\n",
    "        )\n",
    "        if returned_train_padded_sequences.numel() == 0:\n",
    "            print(\"No training sequences generated. Aborting Stage 2 training.\")\n",
    "            return history_tgat, None, test_event_embeddings, test_event_labels_binary_squeezed, None, None, None\n",
    "        \n",
    "        train_sequence_dataset = EmbeddingSequenceDataset(returned_train_padded_sequences, returned_train_seq_labels_list, returned_train_seq_lengths)\n",
    "        batch_size_seq_model_val = globals().get('BATCH_SIZE_SEQ_MODEL', 64)\n",
    "        train_sequence_loader = DataLoader(train_sequence_dataset, batch_size=batch_size_seq_model_val, shuffle=True, collate_fn=collate_fn_packed_fixed_length)\n",
    "        print(f\"Created {len(train_sequence_dataset)} training sequences.\")\n",
    "    else:\n",
    "        print(\"No train event embeddings or labels to create train sequences. Aborting Stage 2 training.\")\n",
    "        return history_tgat, None, test_event_embeddings, test_event_labels_binary_squeezed, None, None, None\n",
    "\n",
    "\n",
    "    test_sequence_loader = None\n",
    "    if test_event_embeddings.numel() > 0 and test_event_labels_binary_squeezed.numel() > 0 :\n",
    "        print(f\"Creating test sequences (Length: {sequence_length_val}, Step: {step_size_val}, Label Mode: {seq_label_mode_val})...\")\n",
    "        _, returned_test_seq_labels_list, returned_test_padded_sequences, returned_test_seq_lengths = create_embedding_sequences( # Corrected unpacking\n",
    "            test_event_embeddings, test_event_labels_binary_squeezed, \n",
    "            sequence_length_val, step_size_val, seq_label_mode_val, 'binary'\n",
    "        )\n",
    "        if returned_test_padded_sequences.numel() > 0:\n",
    "            test_sequence_dataset = EmbeddingSequenceDataset(returned_test_padded_sequences, returned_test_seq_labels_list, returned_test_seq_lengths)\n",
    "            test_sequence_loader = DataLoader(test_sequence_dataset, batch_size=batch_size_seq_model_val, shuffle=False, collate_fn=collate_fn_packed_fixed_length)\n",
    "            print(f\"Created {len(test_sequence_dataset)} test sequences.\")\n",
    "        else:\n",
    "            print(\"No test sequences generated for sequence model evaluation.\")\n",
    "    else:\n",
    "        print(\"No test event embeddings or labels available for creating test sequences.\")\n",
    "    \n",
    "    seq_model_input_dim = train_event_embeddings.shape[1] if train_event_embeddings.numel() > 0 else hidden_dim_val\n",
    "    \n",
    "    seq_model_hidden_dim_val = globals().get('SEQ_MODEL_HIDDEN_DIM', 128)\n",
    "    seq_model_num_layers_val = globals().get('SEQ_MODEL_NUM_LAYERS', 1)\n",
    "    seq_model_dropout_val = globals().get('SEQ_MODEL_DROPOUT', 0.2)\n",
    "    seq_model_rnn_type_val = globals().get('SEQ_MODEL_RNN_TYPE', 'GRU')\n",
    "\n",
    "    sequence_model = EventSequenceAggregator(\n",
    "        embedding_dim=seq_model_input_dim,\n",
    "        hidden_dim=seq_model_hidden_dim_val,\n",
    "        num_layers=seq_model_num_layers_val,\n",
    "        num_classes=1, \n",
    "        dropout=seq_model_dropout_val,\n",
    "        rnn_type=seq_model_rnn_type_val\n",
    "    ).to(device_val)\n",
    "\n",
    "    lr_seq_model_val = globals().get('LEARNING_RATE_SEQ_MODEL', 1e-4)\n",
    "    criterion_seq = nn.BCEWithLogitsLoss()\n",
    "    optimizer_seq = optim.Adam(sequence_model.parameters(), lr=lr_seq_model_val)\n",
    "    \n",
    "    epochs_seq_model_val = globals().get('EPOCHS_SEQ_MODEL', 20)\n",
    "    print(f\"\\n--- Training Sequence Aggregator Model ({seq_model_rnn_type_val}) ---\")\n",
    "    print(f\"Input Embedding Dim: {seq_model_input_dim}, Sequence Length: {sequence_length_val}\")\n",
    "    print(f\"Seq Model: Hidden={seq_model_hidden_dim_val}, Layers={seq_model_num_layers_val}, Dropout={seq_model_dropout_val}\")\n",
    "    print(f\"Optimizer: Adam, LR={lr_seq_model_val}. Epochs: {epochs_seq_model_val}\")\n",
    "\n",
    "    history_sequence_model = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                              'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': []}\n",
    "\n",
    "    for epoch_seq in range(epochs_seq_model_val):\n",
    "        print(f\"Sequence Model Epoch {epoch_seq+1}/{epochs_seq_model_val}\")\n",
    "        if train_sequence_loader and len(train_sequence_loader) > 0:\n",
    "            train_loss_s, train_acc_s, _, _, train_f1_s = train_sequence_epoch(\n",
    "                sequence_model, train_sequence_loader, optimizer_seq, criterion_seq, device_val\n",
    "            )\n",
    "            history_sequence_model['train_loss'].append(train_loss_s)\n",
    "            history_sequence_model['train_acc'].append(train_acc_s)\n",
    "            history_sequence_model['train_f1'].append(train_f1_s)\n",
    "            \n",
    "            train_loss_s_str = f\"{train_loss_s:.4f}\" if train_loss_s is not None and not np.isnan(train_loss_s) else \"N/A\"\n",
    "            train_acc_s_str = f\"{train_acc_s:.4f}\" if train_acc_s is not None and not np.isnan(train_acc_s) else \"N/A\"\n",
    "            train_f1_s_str = f\"{train_f1_s:.4f}\" if train_f1_s is not None and not np.isnan(train_f1_s) else \"N/A\"\n",
    "            print(f\"  Seq Train: Loss={train_loss_s_str}, Acc={train_acc_s_str}, F1={train_f1_s_str}\")\n",
    "        else:\n",
    "            print(\"  Skipping sequence training epoch as train_sequence_loader is empty or not created.\")\n",
    "            for k_train_s in ['train_loss', 'train_acc', 'train_f1']: history_sequence_model[k_train_s].append(float('nan'))\n",
    "\n",
    "\n",
    "        if test_sequence_loader and len(test_sequence_loader) > 0:\n",
    "            val_loss_s, val_acc_s, _, _, val_f1_s, val_auc_s, _, _ = evaluate_sequence_model(\n",
    "                sequence_model, test_sequence_loader, criterion_seq, device_val, phase=\"Val Seq Agg\"\n",
    "            )\n",
    "            history_sequence_model['val_loss'].append(val_loss_s)\n",
    "            history_sequence_model['val_acc'].append(val_acc_s)\n",
    "            history_sequence_model['val_f1'].append(val_f1_s)\n",
    "            history_sequence_model['val_auc'].append(val_auc_s if val_auc_s is not None and not np.isnan(val_auc_s) else np.nan)\n",
    "            \n",
    "            val_loss_s_str = f\"{val_loss_s:.4f}\" if val_loss_s is not None and not np.isnan(val_loss_s) else \"N/A\"\n",
    "            val_acc_s_str = f\"{val_acc_s:.4f}\" if val_acc_s is not None and not np.isnan(val_acc_s) else \"N/A\"\n",
    "            val_f1_s_str = f\"{val_f1_s:.4f}\" if val_f1_s is not None and not np.isnan(val_f1_s) else \"N/A\"\n",
    "            val_auc_s_str = f\"{val_auc_s:.4f}\" if val_auc_s is not None and not np.isnan(val_auc_s) else \"N/A\"\n",
    "            print(f\"  Seq Val:   Loss={val_loss_s_str}, Acc={val_acc_s_str}, F1={val_f1_s_str}, AUC={val_auc_s_str}\")\n",
    "        else:\n",
    "            print(\"  No test data for sequence model validation this epoch.\")\n",
    "            for k_val_s in ['val_loss', 'val_acc', 'val_f1', 'val_auc']: history_sequence_model[k_val_s].append(float('nan'))\n",
    "    \n",
    "    stage2_time = time.time() - stage2_start_time\n",
    "    print(f\"--- Sequence Aggregator Training (Stage 2) Finished in {stage2_time:.2f}s ---\")\n",
    "    \n",
    "    seq_model_save_path = os.path.join(model_save_dir_val, \"sequence_aggregator_model_nslkdd.pth\")\n",
    "    if not os.path.exists(model_save_dir_val): os.makedirs(model_save_dir_val)\n",
    "    torch.save(sequence_model.state_dict(), seq_model_save_path)\n",
    "    print(f\"Sequence aggregator model saved to {seq_model_save_path}\")\n",
    "\n",
    "    combined_history = {\n",
    "        \"tgat\": history_tgat,\n",
    "        \"sequence_aggregator\": history_sequence_model\n",
    "    }\n",
    "    \n",
    "    # 返回階段二測試集相關數據，以便主執行區塊使用\n",
    "    return (combined_history, \n",
    "            sequence_model,\n",
    "            test_event_embeddings, # 可能為 None 或空 Tensor\n",
    "            test_event_labels_binary_squeezed, # 可能為 None 或空 Tensor\n",
    "            returned_test_seq_labels_list, # 從 create_embedding_sequences 返回的\n",
    "            returned_test_padded_sequences, # 從 create_embedding_sequences 返回的\n",
    "            returned_test_seq_lengths # 從 create_embedding_sequences 返回的\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407f186-ed1d-43f2-951e-4b7994642995",
   "metadata": {},
   "source": [
    "## 9. Execute Training and Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec9bf76e-e447-4a64-8797-e172ce67c977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Combined Training Pipeline (TGAT and Sequence Model) for NSL-KDD ---\n",
      "--- Stage 1: TGAT Event-Level Model Training ---\n",
      "Loading data from: ./processed_data_nslkdd/train_temporal_data_nslkdd.pt\n",
      "Loading metadata from: ./processed_data_nslkdd/metadata_nslkdd.json\n",
      "Data loaded: Nodes=125973, Edges=2\n",
      "Metadata: NodeFeatDim=125, NumClasses(binary)=2, PosWeight=1.1486098766326904\n",
      "Loading data from: ./processed_data_nslkdd/test_temporal_data_nslkdd.pt\n",
      "Loading metadata from: ./processed_data_nslkdd/metadata_nslkdd.json\n",
      "Data loaded: Nodes=11850, Edges=2\n",
      "Metadata: NodeFeatDim=125, NumClasses(binary)=2, PosWeight=1.1486098766326904\n",
      "\n",
      "--- TGAT Training Configuration ---\n",
      "Device: cuda:0, Epochs: 3, Batch Size: 256, LR: 0.0005\n",
      "Node Feat Dim: 125, Hidden Dim: 256, Time Emb: 64\n",
      "TGAT Layers: 2, Heads: 4, Dropout: 0.3\n",
      "TGAT Output Dim: 1, Mode: binary\n",
      "TGAT Loss: FocalLoss (FocalLoss used: True)\n",
      "----------------------------\n",
      "\n",
      "Train raw data file for loader's feature similarity FOUND at: /notebooks/TGAT_V2/data/KDDTrain+.txt\n",
      "Test raw data file for loader's feature similarity FOUND at: /notebooks/TGAT_V2/data/KDDTest-21.txt\n",
      "Loading 'service' from ./data/KDDTrain+.txt for similarity sampling...\n",
      "Successfully loaded 'service' for 125973 nodes.\n",
      "Loading 'service' from ./data/KDDTest-21.txt for similarity sampling...\n",
      "Successfully loaded 'service' for 11850 nodes.\n",
      "--- TGAT Epoch 1/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58ccf459beb4e3694502786494c38e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGAT Epoch 1/3 | Train TGAT Loss: 0.2040 | Acc: 0.4722 | Prec: 0.4659 | Rec: 0.9155 | F1: 0.6175 | AUC: 0.5008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fdf1d2bf7045ecad29fb2a51142007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation TGAT Phase:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGAT Epoch 1/3 | Validation TGAT Loss: 0.2145 | Acc: 0.8181 | Prec: 0.8184 | Rec: 0.9995 | F1: 0.8999 | AUC: 0.4979\n",
      "{'Normal (0)': {'precision': 0.16666666666666666, 'recall': 0.00046468401486988845, 'f1-score': 0.0009267840593141798, 'support': 2152.0}, 'Attack (1)': {'precision': 0.8183890577507599, 'recall': 0.9994844297793359, 'f1-score': 0.8999164422987652, 'support': 9698.0}, 'accuracy': 0.8180590717299578, 'macro avg': {'precision': 0.49252786220871325, 'recall': 0.4999745568971029, 'f1-score': 0.4504216131790397, 'support': 11850.0}, 'weighted avg': {'precision': 0.7000340716230832, 'recall': 0.8180590717299578, 'f1-score': 0.7366568857982336, 'support': 11850.0}}\n",
      "TGAT Current LR: 0.0005\n",
      "TGAT Epoch 1/3: New best TGAT model saved! Val F1 (Attack): 0.8999\n",
      "---------------------------------\n",
      "\n",
      "--- TGAT Epoch 2/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5226e0f6497843cca0304f9f4bdc3e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGAT Epoch 2/3 | Train TGAT Loss: 0.2037 | Acc: 0.4669 | Prec: 0.4656 | Rec: 0.9824 | F1: 0.6318 | AUC: 0.5003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266c99c8d57846bcab5840d7f1f6d763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation TGAT Phase:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGAT Epoch 2/3 | Validation TGAT Loss: 0.2038 | Acc: 0.8152 | Prec: 0.8183 | Rec: 0.9952 | F1: 0.8981 | AUC: 0.4999\n",
      "{'Normal (0)': {'precision': 0.16071428571428573, 'recall': 0.004182156133828996, 'f1-score': 0.008152173913043476, 'support': 2152.0}, 'Attack (1)': {'precision': 0.8182974393759539, 'recall': 0.9951536399257579, 'f1-score': 0.8981016192071469, 'support': 9698.0}, 'accuracy': 0.8151898734177215, 'macro avg': {'precision': 0.4895058625451198, 'recall': 0.49966789802979344, 'f1-score': 0.4531268965600952, 'support': 11850.0}, 'weighted avg': {'precision': 0.6988781189810247, 'recall': 0.8151898734177215, 'f1-score': 0.7364837958929773, 'support': 11850.0}}\n",
      "TGAT Current LR: 0.0005\n",
      "TGAT Epoch 2/3: No improvement count: 1 (Current F1 Attack: 0.8981, Best F1 Attack: 0.8999).\n",
      "---------------------------------\n",
      "\n",
      "--- TGAT Epoch 3/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990eddb3556c44cf82ae15841c9a00dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGAT Epoch 3/3 | Train TGAT Loss: 0.2036 | Acc: 0.4656 | Prec: 0.4654 | Rec: 0.9990 | F1: 0.6350 | AUC: 0.5001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002825a95ec24da6ba705fe9426304d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation TGAT Phase:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGAT Epoch 3/3 | Validation TGAT Loss: 0.2085 | Acc: 0.8160 | Prec: 0.8185 | Rec: 0.9961 | F1: 0.8986 | AUC: 0.4909\n",
      "{'Normal (0)': {'precision': 0.20833333333333334, 'recall': 0.004646840148698885, 'f1-score': 0.009090909090909092, 'support': 2152.0}, 'Attack (1)': {'precision': 0.8185053380782918, 'recall': 0.9960816663229531, 'f1-score': 0.8986046511627906, 'support': 9698.0}, 'accuracy': 0.8160337552742616, 'macro avg': {'precision': 0.5134193357058126, 'recall': 0.500364253235826, 'f1-score': 0.45384778012684984, 'support': 11850.0}, 'weighted avg': {'precision': 0.7076960423642706, 'recall': 0.8160337552742616, 'f1-score': 0.7370659530245046, 'support': 11850.0}}\n",
      "TGAT Current LR: 0.0005\n",
      "TGAT Epoch 3/3: No improvement count: 2 (Current F1 Attack: 0.8986, Best F1 Attack: 0.8999).\n",
      "---------------------------------\n",
      "\n",
      "--- TGAT Training (Stage 1) Finished in 211.09s. Best Val F1 (Attack): 0.8999 ---\n",
      "\n",
      "\n",
      "--- Stage 2: Sequence Aggregator Model Training ---\n",
      "Warning: 'feature_similarity_col_name' ('service') provided, but 'raw_data_file_path_for_ids' is None or empty. Disabling feature similarity sampling.\n",
      "Generating TGAT embeddings for all events...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ef6c1b5ef04b30bd2805080fe943b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 125973 event embeddings of dimension 256\n",
      "Warning: 'feature_similarity_col_name' ('service') provided, but 'raw_data_file_path_for_ids' is None or empty. Disabling feature similarity sampling.\n",
      "Generating TGAT embeddings for all events...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351a231604b84adeb3c937930808365e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 11850 event embeddings of dimension 256\n",
      "Creating training sequences (Length: 10, Step: 5, Label Mode: any_attack)...\n",
      "Created 25193 training sequences.\n",
      "Creating test sequences (Length: 10, Step: 5, Label Mode: any_attack)...\n",
      "Created 2369 test sequences.\n",
      "\n",
      "--- Training Sequence Aggregator Model (GRU) ---\n",
      "Input Embedding Dim: 256, Sequence Length: 10\n",
      "Seq Model: Hidden=128, Layers=1, Dropout=0.2\n",
      "Optimizer: Adam, LR=0.0001. Epochs: 20\n",
      "Sequence Model Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ed96964fbe4462a380a4423139e698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Sequence Epoch:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seq Train: Loss=0.0568, Acc=0.9964, F1=0.9982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4b95bb60154f33adc9d0477f167595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Seq Agg Phase:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Seq Agg Seq Aggregator | Val Seq Agg Seq Results Loss: 0.0047 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "  Seq Val:   Loss=0.0047, Acc=1.0000, F1=1.0000, AUC=N/A\n",
      "Sequence Model Epoch 2/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4658b0ea5f5d4c1e8994c6e8bf7f6cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Sequence Epoch:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seq Train: Loss=0.0140, Acc=0.9981, F1=0.9991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6250c6db398495ba5eaed187f46b1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Seq Agg Phase:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Seq Agg Seq Aggregator | Val Seq Agg Seq Results Loss: 0.0026 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "  Seq Val:   Loss=0.0026, Acc=1.0000, F1=1.0000, AUC=N/A\n",
      "Sequence Model Epoch 3/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339edd05e08d45c799e57e4e80e9dc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Sequence Epoch:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seq Train: Loss=0.0139, Acc=0.9981, F1=0.9991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db05d1f043e14e85822d65dcd8cc8041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Seq Agg Phase:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Seq Agg Seq Aggregator | Val Seq Agg Seq Results Loss: 0.0021 | Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "  Seq Val:   Loss=0.0021, Acc=1.0000, F1=1.0000, AUC=N/A\n",
      "Sequence Model Epoch 4/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b47c6980ddc44e886363a978b0733a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Sequence Epoch:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Starting Combined Training Pipeline (TGAT and Sequence Model) for NSL-KDD ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 修改以接收 train_pipeline 返回的額外數據\u001b[39;00m\n\u001b[1;32m     11\u001b[0m (combined_history_data,\n\u001b[1;32m     12\u001b[0m  final_trained_sequence_model,\n\u001b[1;32m     13\u001b[0m  returned_test_event_embeddings,\n\u001b[1;32m     14\u001b[0m  returned_test_event_labels_binary_squeezed,\n\u001b[1;32m     15\u001b[0m  returned_test_seq_labels_list,\n\u001b[1;32m     16\u001b[0m  returned_test_padded_sequences,\n\u001b[1;32m     17\u001b[0m  returned_test_seq_lengths\n\u001b[0;32m---> 18\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m combined_history_data:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining pipeline failed. Aborting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 301\u001b[0m, in \u001b[0;36mtrain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k_train_s \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]: history_sequence_model[k_train_s]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# [cite: 472]\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     train_loss_s, train_acc_s, _, _, train_f1_s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sequence_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequence_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequence_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_val\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [cite: 473]\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     history_sequence_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss_s) \u001b[38;5;66;03m# [cite: 473]\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     history_sequence_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc_s) \u001b[38;5;66;03m# [cite: 473]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 191\u001b[0m, in \u001b[0;36mtrain_sequence_epoch\u001b[0;34m(seq_model, loader, optimizer_seq, criterion_seq, device_seq)\u001b[0m\n\u001b[1;32m    188\u001b[0m optimizer_seq\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    189\u001b[0m output_logits_seq \u001b[38;5;241m=\u001b[39m seq_model(packed_sequences_batch)\n\u001b[0;32m--> 191\u001b[0m loss_seq \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_logits_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_batch_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_seq\u001b[38;5;241m.\u001b[39misnan() \u001b[38;5;129;01mor\u001b[39;00m loss_seq\u001b[38;5;241m.\u001b[39misinf():\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence Train: Loss NaN/Inf. Logits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_logits_seq\u001b[38;5;241m.\u001b[39mflatten()[:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels_batch_seq\u001b[38;5;241m.\u001b[39mflatten()[:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping batch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:3195\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell: Main execution block (MODIFIED to include Stage 2 Sequence Model Training)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 確保 RAW_DATA_DIR, TRAIN_FILE, TEST_FILE, PROCESSED_TRAIN_FILE 等全局變量已定義並指向 NSL-KDD 路徑\n",
    "    if not all(os.path.exists(f_path) for f_path in [PROCESSED_TRAIN_FILE, PROCESSED_TEST_FILE, METADATA_FILE]):\n",
    "        print(f\"Error: Preprocessed NSL-KDD data files not found in '{PROCESSED_DATA_DIR}'.\")\n",
    "        print(\"Please ensure the preprocessing notebook section was run successfully for NSL-KDD.\")\n",
    "    else:\n",
    "        print(\"--- Starting Combined Training Pipeline (TGAT and Sequence Model) for NSL-KDD ---\")\n",
    "        \n",
    "        # 修改以接收 train_pipeline 返回的額外數據\n",
    "        (combined_history_data,\n",
    "         final_trained_sequence_model,\n",
    "         # 以下是為最終序列評估返回的數據\n",
    "         final_eval_test_event_embeddings,\n",
    "         final_eval_test_event_labels_binary_squeezed,\n",
    "         final_eval_test_seq_labels_list,\n",
    "         final_eval_test_padded_sequences,\n",
    "         final_eval_test_seq_lengths\n",
    "        ) = train_pipeline()\n",
    "\n",
    "        if not combined_history_data:\n",
    "            print(\"Training pipeline failed. Aborting.\")\n",
    "        else:\n",
    "            history_tgat = combined_history_data.get('tgat')\n",
    "            history_sequence_model = combined_history_data.get('sequence_aggregator')\n",
    "\n",
    "            # --- 將訓練歷史記錄儲存到 CSV (可選) ---\n",
    "            if history_tgat:\n",
    "                try:\n",
    "                    df_tgat_history = pd.DataFrame(history_tgat)\n",
    "                    if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR) # MODEL_SAVE_DIR 應已更新為 _nslkdd\n",
    "                    tgat_csv_path = os.path.join(MODEL_SAVE_DIR, 'tgat_training_history_nslkdd.csv')\n",
    "                    df_tgat_history.to_csv(tgat_csv_path, index_label='epoch')\n",
    "                    print(f\"\\nTGAT model training history saved to: {tgat_csv_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError saving TGAT model training history to CSV: {e}\")\n",
    "\n",
    "            if history_sequence_model:\n",
    "                try:\n",
    "                    df_sequence_history = pd.DataFrame(history_sequence_model)\n",
    "                    if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR)\n",
    "                    sequence_csv_path = os.path.join(MODEL_SAVE_DIR, 'sequence_model_training_history_nslkdd.csv')\n",
    "                    df_sequence_history.to_csv(sequence_csv_path, index_label='epoch')\n",
    "                    print(f\"Sequence aggregator model training history saved to: {sequence_csv_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError saving sequence model training history to CSV: {e}\")\n",
    "            \n",
    "            # --- 繪製 TGAT 模型歷史 ---\n",
    "            if history_tgat:\n",
    "                fig_tgat, axs_tgat = plt.subplots(2, 2, figsize=(12, 10))\n",
    "                plot_metrics_map_tgat = [('Loss', 'loss'), ('Accuracy', 'acc'), ('F1-score', 'f1'), ('AUC', 'auc')]\n",
    "                for i, (title, key_metric) in enumerate(plot_metrics_map_tgat):\n",
    "                    ax_current_tgat = axs_tgat[i//2, i%2]\n",
    "                    # 過濾 None 和 NaN 值進行繪圖\n",
    "                    train_values_tgat = [v for v in history_tgat.get(f'train_{key_metric}', []) if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "                    val_values_tgat = [v for v in history_tgat.get(f'val_{key_metric}', []) if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "                    epochs_train_tgat = [j for j,v_ in enumerate(history_tgat.get(f'train_{key_metric}', [])) if v_ is not None and not (isinstance(v_, float) and np.isnan(v_))]\n",
    "                    epochs_val_tgat = [j for j,v_ in enumerate(history_tgat.get(f'val_{key_metric}', [])) if v_ is not None and not (isinstance(v_, float) and np.isnan(v_))]\n",
    "                    \n",
    "                    if epochs_train_tgat and train_values_tgat: ax_current_tgat.plot(epochs_train_tgat, train_values_tgat, label=f'TGAT Train {title}')\n",
    "                    if epochs_val_tgat and val_values_tgat: ax_current_tgat.plot(epochs_val_tgat, val_values_tgat, label=f'TGAT Validation {title}')\n",
    "                    ax_current_tgat.set_title(f'TGAT {title} over Epochs'); ax_current_tgat.set_xlabel('Epoch'); ax_current_tgat.set_ylabel(title); ax_current_tgat.legend()\n",
    "                plt.tight_layout(); plt.show()\n",
    "\n",
    "            # --- 繪製序列模型歷史 ---\n",
    "            if history_sequence_model:\n",
    "                # 調整繪圖邏輯以處理可能的子圖數量\n",
    "                num_seq_metrics_to_plot = 0\n",
    "                plot_metrics_seq_map = [('Loss', 'loss'), ('Accuracy', 'acc'), ('F1-score', 'f1')]\n",
    "                if any(val is not None and not (isinstance(val, float) and np.isnan(val)) for val in history_sequence_model.get('val_auc', [])): # 僅在 AUC 有效時繪製\n",
    "                    plot_metrics_seq_map.append(('AUC', 'auc'))\n",
    "                \n",
    "                num_seq_metrics_to_plot = len(plot_metrics_seq_map)\n",
    "                if num_seq_metrics_to_plot == 3:\n",
    "                    fig_seq, axs_seq_flat = plt.subplots(1, 3, figsize=(18, 5))\n",
    "                elif num_seq_metrics_to_plot == 4:\n",
    "                    fig_seq, axs_seq = plt.subplots(2, 2, figsize=(12, 10))\n",
    "                    axs_seq_flat = axs_seq.flatten()\n",
    "                else: # 如果沒有指標或只有1-2個指標，則相應調整\n",
    "                    if num_seq_metrics_to_plot > 0:\n",
    "                        fig_seq, axs_seq_flat = plt.subplots(1, num_seq_metrics_to_plot, figsize=(6 * num_seq_metrics_to_plot, 5))\n",
    "                        if num_seq_metrics_to_plot == 1: axs_seq_flat = [axs_seq_flat] # 確保是列表\n",
    "                    else:\n",
    "                        fig_seq, axs_seq_flat = None, [] # 不繪圖\n",
    "\n",
    "                if fig_seq: # 只有在有圖可繪時才繼續\n",
    "                    for i_seq, (title_s, key_metric_s) in enumerate(plot_metrics_seq_map):\n",
    "                        ax_s_current = axs_seq_flat[i_seq]\n",
    "                        train_values_s = [v for v in history_sequence_model.get(f'train_{key_metric_s}', []) if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "                        val_values_s = [v for v in history_sequence_model.get(f'val_{key_metric_s}', []) if v is not None and not (isinstance(v, float) and np.isnan(v))]\n",
    "                        epochs_train_s = [j for j,v_ in enumerate(history_sequence_model.get(f'train_{key_metric_s}', [])) if v_ is not None and not (isinstance(v_, float) and np.isnan(v_))]\n",
    "                        epochs_val_s = [j for j,v_ in enumerate(history_sequence_model.get(f'val_{key_metric_s}', [])) if v_ is not None and not (isinstance(v_, float) and np.isnan(v_))]\n",
    "\n",
    "                        if epochs_train_s and train_values_s: ax_s_current.plot(epochs_train_s, train_values_s, label=f'Seq.Agg. Train {title_s}')\n",
    "                        if epochs_val_s and val_values_s: ax_s_current.plot(epochs_val_s, val_values_s, label=f'Seq.Agg. Validation {title_s}')\n",
    "                        ax_s_current.set_title(f'Sequence Agg. {title_s}'); ax_s_current.set_xlabel('Epoch'); ax_s_current.set_ylabel(title_s); ax_s_current.legend()\n",
    "                    \n",
    "                    if num_seq_metrics_to_plot == 3 and len(axs_seq_flat) == 4 : # 如果是2x2但只畫了3個\n",
    "                        axs_seq_flat[3].set_visible(False)\n",
    "                    plt.tight_layout(); plt.show()\n",
    "\n",
    "            # --- Final Evaluation of Best TGAT Model (from Stage 1) ---\n",
    "            print(\"\\n--- Final Evaluation of Best TGAT Model (from Stage 1) on Test Set (NSL-KDD) ---\")\n",
    "            # 提示: 您的混淆矩陣顯示 TGAT 模型在 \"Normal\" 類別上表現很差。\n",
    "            # 請檢查類別權重 (pos_weight_binary)、FocalLoss 的 alpha 參數，或考慮數據重採樣/特徵工程。\n",
    "            best_tgat_model_path = os.path.join(MODEL_SAVE_DIR, BEST_MODEL_NAME)\n",
    "            if os.path.exists(best_tgat_model_path):\n",
    "                with open(METADATA_FILE, 'r') as f_meta: meta_final_tgat = json.load(f_meta)\n",
    "                tgat_output_dim_final = 1 if CLASSIFICATION_MODE == 'binary' else meta_final_tgat['NUM_CLASSES_MULTI']\n",
    "                tgat_display_classes_final = meta_final_tgat.get('NUM_CLASSES_BINARY', 2) if CLASSIFICATION_MODE == 'binary' else tgat_output_dim_final\n",
    "                tgat_node_feat_dim_final = meta_final_tgat['NODE_FEAT_DIM']\n",
    "                \n",
    "                tgat_model_for_final_eval = TGAT(\n",
    "                    node_feat_dim=tgat_node_feat_dim_final, time_emb_dim=TIME_DIM, n_head=N_HEADS, \n",
    "                    n_layers=N_LAYERS, hidden_dim_per_layer=HIDDEN_DIM, \n",
    "                    num_classes=tgat_output_dim_final, dropout=DROPOUT \n",
    "                ).to(DEVICE)\n",
    "                tgat_model_for_final_eval.load_state_dict(torch.load(best_tgat_model_path, map_location=DEVICE))\n",
    "                \n",
    "                tgat_final_test_data_cpu, _, _, tgat_final_pos_weight_cpu = load_processed_data(PROCESSED_TEST_FILE, METADATA_FILE, CLASSIFICATION_MODE)\n",
    "                \n",
    "                # 為最終 TGAT 評估的 loader 提供原始文件路徑\n",
    "                final_test_raw_path_for_loader = os.path.join(RAW_DATA_DIR, TEST_FILE) if FEATURE_SIMILARITY_COL_NAME else None\n",
    "\n",
    "                tgat_final_test_loader = TemporalNeighborLoader(\n",
    "                    tgat_final_test_data_cpu, BATCH_SIZE, NUM_NEIGHBORS, DEVICE, \n",
    "                    shuffle=False,\n",
    "                    recency_bias_factor=RECENCY_BIAS_FACTOR,\n",
    "                    feature_similarity_col_name=FEATURE_SIMILARITY_COL_NAME,\n",
    "                    feature_similarity_weight=FEATURE_SIMILARITY_WEIGHT,\n",
    "                    raw_data_file_path_for_ids=final_test_raw_path_for_loader, \n",
    "                    col_names_list=COL_NAMES\n",
    "                )\n",
    "                \n",
    "                tgat_final_criterion_args = {}\n",
    "                if CLASSIFICATION_MODE == 'binary':\n",
    "                    if USE_FOCAL_LOSS: \n",
    "                        tgat_final_criterion = FocalLoss(gamma=2.0, pos_weight_for_bce=tgat_final_pos_weight_cpu.to(DEVICE) if tgat_final_pos_weight_cpu is not None else None, alpha=None)\n",
    "                    else:\n",
    "                        if tgat_final_pos_weight_cpu is not None: tgat_final_criterion_args['pos_weight'] = tgat_final_pos_weight_cpu.to(DEVICE)\n",
    "                        tgat_final_criterion = nn.BCEWithLogitsLoss(**tgat_final_criterion_args)\n",
    "                else: \n",
    "                    tgat_final_criterion = nn.CrossEntropyLoss(**tgat_final_criterion_args)\n",
    "                \n",
    "                final_eval_results_tgat = evaluate_model(\n",
    "                    tgat_model_for_final_eval, tgat_final_test_loader, tgat_final_criterion, \n",
    "                    phase='Final TGAT Test (NSL-KDD)', return_embeddings_and_ids=False\n",
    "                )\n",
    "                loss_f_tgat, acc_f_tgat, prec_f_tgat, rec_f_tgat, f1_f_tgat, auc_f_tgat, y_true_f_tgat, y_pred_f_tgat, report_dict_f_tgat = final_eval_results_tgat\n",
    "                \n",
    "                print_metrics(\"Final TGAT Test\", loss_f_tgat, acc_f_tgat, prec_f_tgat, rec_f_tgat, f1_f_tgat, auc_f_tgat, phase='Final TGAT Test Results (NSL-KDD)', class_report=str(report_dict_f_tgat))\n",
    "                cm_class_names_final_tgat = ['Normal (0)', 'Attack (1)'] if CLASSIFICATION_MODE == 'binary' else [str(i) for i in range(tgat_display_classes_final)]\n",
    "                if y_true_f_tgat and y_pred_f_tgat: \n",
    "                    plot_confusion_matrix_custom(y_true_f_tgat, y_pred_f_tgat, class_names=cm_class_names_final_tgat, title='Final Best TGAT Test Confusion Matrix (NSL-KDD)')\n",
    "            else:\n",
    "                 print(f\"Best TGAT model file not found at: {best_tgat_model_path}. Skipping final TGAT evaluation.\")\n",
    "\n",
    "\n",
    "            # --- Final Evaluation of Sequence Aggregator Model ---\n",
    "            if final_trained_sequence_model and history_sequence_model:\n",
    "                print(\"\\n--- Final Evaluation of Sequence Aggregator Model on Test Sequences (NSL-KDD) ---\")\n",
    "                \n",
    "                # 使用從 train_pipeline 返回的測試集序列數據\n",
    "                if returned_test_padded_sequences is not None and returned_test_padded_sequences.numel() > 0 and \\\n",
    "                   returned_test_seq_labels_list is not None and returned_test_seq_lengths is not None:\n",
    "                    \n",
    "                    final_test_sequence_dataset = EmbeddingSequenceDataset(\n",
    "                        returned_test_padded_sequences,\n",
    "                        returned_test_seq_labels_list, \n",
    "                        returned_test_seq_lengths\n",
    "                    )\n",
    "                    # 確保 BATCH_SIZE_SEQ_MODEL, collate_fn_packed_fixed_length, DEVICE 已定義\n",
    "                    final_test_sequence_loader = DataLoader(\n",
    "                        final_test_sequence_dataset,\n",
    "                        batch_size=BATCH_SIZE_SEQ_MODEL, \n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn_packed_fixed_length \n",
    "                    )\n",
    "\n",
    "                    criterion_seq_final = nn.BCEWithLogitsLoss()\n",
    "                    _, _, _, _, _, _, final_seq_true, final_seq_pred = evaluate_sequence_model(\n",
    "                        final_trained_sequence_model, \n",
    "                        final_test_sequence_loader,\n",
    "                        criterion_seq_final,\n",
    "                        DEVICE,\n",
    "                        phase=\"Final Test Seq Agg (NSL-KDD)\"\n",
    "                    )\n",
    "                    if final_seq_true and final_seq_pred:\n",
    "                        plot_confusion_matrix_custom(final_seq_true, final_seq_pred,\n",
    "                                                     class_names=['Normal Seq', 'Attack Seq'],\n",
    "                                                     title='Final Sequence Aggregator Confusion Matrix (NSL-KDD)')\n",
    "                else:\n",
    "                    print(\"No valid test sequences (embeddings, labels, or lengths) were returned from the pipeline for final sequence model evaluation.\")\n",
    "            else:\n",
    "                print(\"Sequence model was not trained or history is unavailable. Skipping final sequence model evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974786e-9773-4ec4-9c25-5e2b7338c121",
   "metadata": {},
   "source": [
    "## 10. How to Run & Notes\n",
    "- **Ensure Preprocessing is Done**: Run `preprocess_kdd_large.ipynb` first.\n",
    "- **Adjust Configuration**: Check paths and hyperparameters in Cell 2.\n",
    "- **Run All Cells**: This will train, evaluate, save the best model, and plot metrics.\n",
    "- **Memory for `TemporalNeighborLoader`**:\n",
    "    - The current loader sends the *entire* graph's features (`self.temporal_data.x`) and timestamps (`self.temporal_data.ts`) to the specified `device` *for each batch*.\n",
    "    - **If `self.temporal_data.x` (all node features for the entire dataset) is too large to fit on the GPU, this will cause an Out-Of-Memory (OOM) error.**\n",
    "    - **For truly massive graphs**: The loader and model interaction need modification. For example, keep full data on CPU, loader passes indices, model fetches only necessary data slices to GPU. This is a more advanced optimization not implemented here but crucial for >GPU memory graphs. This script currently assumes the full feature/timestamp tensors fit on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11633d-ae85-419c-93bf-781961fca663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
