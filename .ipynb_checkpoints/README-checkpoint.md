# TGAT and Sequence Model for Network Intrusion Detection on NSL-KDD

## üåü Overview

This project implements a two-stage deep learning model for network intrusion detection using the NSL-KDD dataset. The primary goal is to leverage temporal dependencies and graph structures within network traffic data to accurately identify various types of network attacks.

The process involves:
1.  **Event-Level Feature Extraction with TGAT**: A Temporal Graph Attention Network (TGAT) is first trained on individual network events (connections) to learn rich temporal and relational features.
2.  **Sequence-Level Classification with RNN**: The embeddings generated by the trained TGAT model for each event are then used to form sequences. An RNN-based model (GRU or LSTM) is subsequently trained on these sequences to classify if a sequence of network events constitutes an attack.

The main implementation, including data preprocessing, TGAT training, embedding generation, and sequence model training, is detailed in the Jupyter Notebook: `TGAT_NSL-KDD.ipynb`.

## üìä Dataset: NSL-KDD

The NSL-KDD dataset is a refined version of the KDD Cup 1999 dataset, designed to address some of its inherent issues, making it a popular benchmark for intrusion detection systems.

Key characteristics:
* Contains various types of network intrusions as well as normal traffic.
* The project uses the `.txt` versions of the dataset (e.g., `KDDTrain+.txt`, `KDDTest+.txt`).
* **Attack Categories**: The dataset includes major attack categories such as:
    * DoS (Denial of Service)
    * Probe (Probing/Scanning)
    * R2L (Remote to Local)
    * U2R (User to Root)
    (A detailed mapping of specific attack types to these categories is available in `data/attack_types.txt` and used in the preprocessing script `TGAT_NSL-KDD.ipynb`).

For more details on the dataset, refer to `data/README.md`.

## ‚öôÔ∏è Preprocessing

The `TGAT_NSL-KDD.ipynb` notebook includes a comprehensive preprocessing pipeline (originally based on `preprocess_kdd_large.ipynb`) optimized for KDD-like datasets, using Dask for efficiency with larger files. The key steps are:
1.  **Data Loading**: Raw data (e.g., `KDDTrain+.txt`) is loaded. Column names are defined based on KDD documentation.
2.  **Label Processing**: Attack labels are mapped to binary (normal/attack) and multi-class categories (normal, DoS, Probe, R2L, U2R).
3.  **Feature Engineering**: New features are created, such as `time_since_last_event_same_service_proxy`, `avg_duration_recent_same_service_dst_host_srv_count_proxy`, and `count_recent_same_service_cum_proxy`.
4.  **Feature Transformation**:
    * Categorical features (e.g., `protocol_type`, `service`, `flag`) are one-hot encoded.
    * Numerical features are scaled using mean and standard deviation.
5.  **Temporal Graph Construction**: Node features (`x`), edge indices (`edge_index` representing sequential connections), and timestamps (`ts` based on event order) are constructed.
6.  **Output**: Processed data is saved as PyTorch tensor files (`train_temporal_data_nslkdd.pt`, `test_temporal_data_nslkdd.pt`) along with a metadata JSON file (`metadata_nslkdd.json`) containing information like feature dimensions and scaler parameters. The metadata file reports a node feature dimension of 125 after preprocessing.

## üß† Model Architecture

This project employs a two-stage modeling approach:

### 1. Event-Level Model: Temporal Graph Attention Network (TGAT)

* **Purpose**: Learns representations for individual network events by considering their temporal context and relationships with neighboring events.
* **Key Components** (`TGAT_NSL-KDD.ipynb`):
    * **FunctionalTimeEncoder**: Encodes the time difference between events.
    * **TemporalGraphAttentionLayer**: An attention mechanism that weighs the importance of temporal neighbors.
    * **TGAT**: Stacks multiple attention layers to build deep representations.
* **Custom `TemporalNeighborLoader`**: Used for efficient batching and sampling of temporal neighbors during training. It includes options for recency bias and feature similarity-based sampling to select more relevant neighbors.
* **Output**: For each network event, the TGAT model produces an embedding. These embeddings are then used as input to the sequence model.

### 2. Sequence-Level Model: Event Sequence Aggregator (RNN)

* **Purpose**: Classifies sequences of network events (represented by their TGAT embeddings) as normal or attack.
* **Model**: An RNN-based architecture (configurable as GRU or LSTM).
* **Input**: Sequences of event embeddings generated by the trained TGAT model.
* **Output**: A binary classification (normal sequence vs. attack sequence).

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ TGAT_NSL-KDD.ipynb          # Main Jupyter Notebook for preprocessing and two-stage training
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ KDDTrain+.txt             # NSL-KDD Training data (example)
‚îÇ   ‚îú‚îÄ‚îÄ KDDTest+.txt              # NSL-KDD Test data (example)
‚îÇ   ‚îú‚îÄ‚îÄ attack_types.txt          # Mapping of attack names to categories
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Readme for the data directory
‚îú‚îÄ‚îÄ processed_data_nslkdd/      # Stores data after preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ train_temporal_data_nslkdd.pt # Processed training data
‚îÇ   ‚îú‚îÄ‚îÄ test_temporal_data_nslkdd.pt  # Processed test data
‚îÇ   ‚îî‚îÄ‚îÄ metadata_nslkdd.json      # Metadata from preprocessing (feature dims, scaler info)
‚îú‚îÄ‚îÄ saved_models_nslkdd/        # Saved models and training history
‚îÇ   ‚îú‚îÄ‚îÄ best_tgat_model_nslkdd.pth  # Saved best TGAT event-level model
‚îÇ   ‚îú‚îÄ‚îÄ sequence_aggregator_model_nslkdd.pth # Saved sequence aggregator model
‚îÇ   ‚îú‚îÄ‚îÄ tgat_training_history_nslkdd.csv # Training history for TGAT model
‚îÇ   ‚îî‚îÄ‚îÄ sequence_model_training_history_nslkdd.csv # Training history for sequence model
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ TGAT.ipynb                    # Other related notebooks (e.g., earlier versions or different datasets)
‚îú‚îÄ‚îÄ TGAT_V2.ipynb
‚îú‚îÄ‚îÄ TGAT_V3.ipynb
‚îî‚îÄ‚îÄ README.md                     # This file
```

## üöÄ Setup and Installation

1.  **Clone the repository (if applicable):**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    The `requirements.txt` file lists the necessary packages. The notebook `TGAT_NSL-KDD.ipynb` also contains pip install commands for setting up the PyTorch Geometric environment correctly for a specific Torch version and CUDA availability.
    ```bash
    pip install -r requirements.txt
    ```
    Key dependencies include:
    * `torch==2.1.1+cu121` (or CPU version if CUDA is not available)
    * `torch_geometric==2.6.1`
    * `torch-geometric-temporal==0.56.0`
    * `pandas`, `numpy`, `scikit-learn`, `dask`

4.  **Dataset**:
    * Download the NSL-KDD dataset (e.g., from [UNB CIC Datasets](https://www.unb.ca/cic/datasets/nsl.html) or other sources mentioned in `TGAT_NSL-KDD.ipynb`).
    * Place the `KDDTrain+.txt` and `KDDTest+.txt` (or `KDDTest-21.txt` if used) files into the `./data/` directory. Ensure the filenames match those specified in the `TGAT_NSL-KDD.ipynb` configuration cells.

## üõ†Ô∏è How to Run

The project is primarily executed through the `TGAT_NSL-KDD.ipynb` Jupyter Notebook.

1.  **Run Preprocessing Cells**:
    * Execute the cells in the first part of the notebook (derived from `preprocess_kdd_large.ipynb`). This will:
        * Load the raw NSL-KDD data from `./data/`.
        * Perform feature engineering, encoding, and scaling.
        * Save the processed temporal data and metadata to `./processed_data_nslkdd/`.

2.  **Run Training and Evaluation Cells**:
    * Execute the cells in the second part of the notebook (derived from `train_tgat_from_processed.ipynb`). This involves a two-stage training process:
        * **Stage 1: TGAT Event-Level Model Training**:
            * Loads the preprocessed data.
            * Trains the TGAT model.
            * Saves the best performing TGAT model to `./saved_models_nslkdd/best_tgat_model_nslkdd.pth`.
            * Saves TGAT training history to `./saved_models_nslkdd/tgat_training_history_nslkdd.csv`.
        * **Stage 2: Sequence Aggregator Model Training**:
            * Loads the best trained TGAT model from Stage 1.
            * Generates event embeddings for the training and test sets using the TGAT model.
            * Creates sequences from these embeddings.
            * Trains the `EventSequenceAggregator` (RNN) model on these sequences.
            * Saves the trained sequence model to `./saved_models_nslkdd/sequence_aggregator_model_nslkdd.pth`.
            * Saves sequence model training history to `./saved_models_nslkdd/sequence_model_training_history_nslkdd.csv`.
    * The notebook will also plot training metrics and confusion matrices for both stages.

## üìà Results

The training progress and performance metrics for both the TGAT event-level model and the sequence aggregator model are saved in CSV files:
* `saved_models_nslkdd/tgat_training_history_nslkdd.csv`
* `saved_models_nslkdd/sequence_model_training_history_nslkdd.csv`

These files contain metrics such as loss, accuracy, F1-score, and AUC for training and validation sets over epochs. The notebook also plots these metrics and confusion matrices. The output from the notebook indicates a validation F1-score for the "Attack (1)" class around 0.7255 for the TGAT model after 3 epochs. The sequence model shows high performance on its validation set (accuracy and F1 close to 1.0), which might indicate its effectiveness or potential overfitting depending on the sequence construction and data split.

## üìù Notes
* **Configuration**: Hyperparameters for both models, paths, and other settings can be adjusted in the configuration cells of the `TGAT_NSL-KDD.ipynb` notebook.
* **Memory Usage**: The preprocessing steps using Dask are designed to handle large datasets. During training, the `TemporalNeighborLoader` is used. The notebook contains comments regarding potential memory issues if the entire graph's features are too large for GPU memory.
* **CUDA**: The setup and code are configured to use CUDA if available. Ensure your PyTorch and PyG installations are compatible with your CUDA version.